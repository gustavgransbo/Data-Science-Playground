{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "During my orchestra [LiTHe Bl√•s](http://litheblas.org)'s 45 year aniversary I will be doing a customary \"Skitsnack\" between two songs. \n",
    "A \"Skitsnack\" is where an orchestra member keeps the audience preoccupied by talking about anything between heaven and earth until we start playing the next song.\n",
    "I often find it hard to decide what I should talk about and usually end up telling really bad jokes or rambling about some algorithm I just read about. \n",
    "\n",
    "For this \"Skitsnack\" I will avoid coming up with my own material entierly by generating it with an n-grams model! \n",
    "The focus of this Notebook is simply to generate a short text which I think could entertain a group of 500 for about 30-60 seconds.\n",
    "\n",
    "There are some nice aspects of the task of generating a \"funny\" text from a predefined dictionary:\n",
    "* I can easily perform extrinsic testing. \"Is this text funny?\"\n",
    "* I don't have to worry about unkown words.\n",
    "\n",
    "As such, any quantitative evaluation will only happen in the spur of the moment.\n",
    "\n",
    "Inspiration:\n",
    "https://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "I haven't decided what data I should use to train my model, and will probably have to try some different sources when the model is finished. The final text will definitely be in Swedish to accomodate the audience, but to get started I will use the Dansih author [H.C. Andersen's Fairy Tales](http://www.gutenberg.org/ebooks/32572) translated to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('hcandersen_fairy_tales.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's remove all text which is not part of his stories, as I don't want to use this for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627 654\n",
      "4021 4048\n",
      "4140 4167\n",
      "375029 375056\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer(r\"HANS ANDERSEN'S FAIRY TALES\", text):\n",
    "    print(i.start(0), i.end(0))\n",
    "    # Hiding the output for future readability\n",
    "    #print(text[i.start(0): i.end(0) + 200])\n",
    "    #print(\"###########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[4167:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367113 367118\n",
      "NOTES\r\n",
      "\r\n",
      "\r\n",
      "THE STORKS\r\n",
      "\r\n",
      "          PAGE 29. On account of the ravages it makes among\r\n",
      "          noxious animals, the stork is a privileged bird\r\n",
      "          wherever it makes its home. In cities it is\r\n",
      "     \n",
      "###########################\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer(r\"NOTES\", text):\n",
    "    print(i.start(0), i.end(0))\n",
    "    print(text[i.start(0): i.end(0) + 200])\n",
    "    print(\"###########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[:367113 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all tabs and linebreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.sub(r'\\r', '', text)\n",
    "text = re.sub(r'\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Sentences\n",
    "First let's parse the text as sentences using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     THE FLAX   THE flax was in full bloom; it had pretty little blue flowers, as delicate as the wings of a moth.\n",
      "The sun shone on it and the showers watered it; and this was as good for the flax as it is for little children to be washed and then kissed by their mothers.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is not good, it includes the story title. I will noth bother with this at the moment though, as this is not the text I will be using for my final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(word_tokenize, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'shone', 'on', 'it', 'and', 'the', 'showers', 'watered', 'it', ';', 'and', 'this', 'was', 'as', 'good', 'for', 'the', 'flax', 'as', 'it', 'is', 'for', 'little', 'children', 'to', 'be', 'washed', 'and', 'then', 'kissed', 'by', 'their', 'mothers', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add beginning and end of sentence tags\n",
    "All my sentences need a root and a stop token. I will insert a `<BOS>` tag before the first word of each sentence and replace the last punctuation of each sentence with `<EOS>`, I do this to distinguish between in sentence punctuation and the actual end of sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(lambda x: ['<BOS>'] + x[:-1] + ['<EOS>'], sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'The', 'sun', 'shone', 'on', 'it', 'and', 'the', 'showers', 'watered', 'it', ';', 'and', 'this', 'was', 'as', 'good', 'for', 'the', 'flax', 'as', 'it', 'is', 'for', 'little', 'children', 'to', 'be', 'washed', 'and', 'then', 'kissed', 'by', 'their', 'mothers', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing n-grams from the setnences\n",
    "Let's try the nltk ngrams package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a generative model\n",
    "When generating my text I will not be considering context across sentences. Each `<EOS>` tag will be followed by a fresh `<BOS>`, interpreted as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams():\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def fit(self, sentences):\n",
    "        # Count all ngrams\n",
    "        self.grams = []\n",
    "        for i in range(1, self.n+1):\n",
    "            # Build a sorted list of all ngrams\n",
    "            grams = list(map(lambda x: list(ngrams(x, i)), sentences))\n",
    "            grams = np.array(reduce(lambda x, y: x + y, grams))\n",
    "            grams = grams[np.lexsort(grams[:,::-1].T,),:]\n",
    "            \n",
    "            # Build an array marking the first unique occurence of a n-gram\n",
    "            # Example unigrams, \n",
    "            # a = [('a'), ('a'), ('b'), ('c'), ('c')]\n",
    "            # -> [True, False, True, True, False]\n",
    "            first_occurence = np.append([True], np.array([np.any(grams[i-1] != grams[i]) for i in range(1, len(grams))]))\n",
    "            \n",
    "            # Assign each unique n-gram an index\n",
    "            # Example unigrams, \n",
    "            # a = [('a'), ('a'), ('b'), ('c'), ('c')]\n",
    "            # -> [0, 0, 1, 2, 2]\n",
    "            ids = np.append([0], first_occurence[1:].cumsum())\n",
    "            \n",
    "            # Build a mapping from n-gram to count\n",
    "            frequencies = dict(list(zip(map(tuple, grams[first_occurence,:]), np.bincount(ids))))\n",
    "            self.grams.append(frequencies)\n",
    "        \n",
    "        # Record the total number of unigrams\n",
    "        self.n_unigrams = sum(self.grams[0].values())         \n",
    "        \n",
    "    def log_prob(self, word, context):\n",
    "        n = len(context)\n",
    "        gram = context + (word,)\n",
    "        if n == 0:\n",
    "            return np.log(self.grams[n][gram] / self.n_unigrams)\n",
    "        else:\n",
    "            return np.log(self.grams[n][gram] / self.grams[n-1][context])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3902"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram = NGrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.99 s\n"
     ]
    }
   ],
   "source": [
    "%time trigram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('the', ('!', '--'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4657359027997265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('call', (\"'ll\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.7714179106038408"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('call', ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we can calculate the log probabilities of a word following an ngram. To avoid overfitting to the exact sentences in the text some smoothing should be done. Without smoothing we will assign zero probability to all ngrams that are not present in the text.\n",
    "\n",
    "However, I have encountered a problem: At the moment I don't feel creative enough to come up with a generative model that finds the next word in a sentence based on smoothed probabilities.\n",
    "\n",
    "I will start of with an unsmoothed model, and at the same time also switch to a tree structure to represent the model vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text generating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramsTree():\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def fit(self, sentences):\n",
    "         # Build a sorted list of all ngrams\n",
    "        grams = list(map(lambda x: list(ngrams(x, self.n)), sentences))\n",
    "        grams = np.array(reduce(lambda x, y: x + y, grams))\n",
    "        \n",
    "        # Construct a tree based on the ngrams\n",
    "        self.tree = {}\n",
    "        self.tree['count'] = len(grams)\n",
    "        self.tree['words'] = {}\n",
    "        for gram in grams:\n",
    "            reference = self.tree\n",
    "            for i, word in enumerate(gram):\n",
    "                if word in reference['words']: \n",
    "                    reference = reference['words'][word]\n",
    "                    reference['count'] += 1\n",
    "                else:\n",
    "                    reference['words'][word] = {}\n",
    "                    reference = reference['words'][word]\n",
    "                    reference['count'] = 1\n",
    "                    # Don't create maps for the leaves\n",
    "                    if i != self.n - 1:\n",
    "                        reference['words'] = {}\n",
    "                        \n",
    "        # Create integer bins for all words in each context based on their apperances\n",
    "        # These bins will be used to find a random successor to contexts with probabilities\n",
    "        # relative to number of appearences. \n",
    "        # Note: Keys in a dictionary are stored in a non-deterministic, but stable, order.\n",
    "        # Therefore associating the bin with index 0 with the key at index 0 is OK.\n",
    "        def build_bins(tree):\n",
    "            if 'words' not in tree:\n",
    "                return\n",
    "            else:\n",
    "                tree['bins'] = [0]\n",
    "                for values in tree['words'].values():\n",
    "                    tree['bins'].append(tree['bins'][-1] + values['count'])\n",
    "                    build_bins(values)\n",
    "        \n",
    "        build_bins(self.tree)\n",
    "\n",
    "        \n",
    "    def generate_sentence(self, sentence, max_length = 100):\n",
    "        n = min(len(sentence), self.n - 1)\n",
    "        reference = self.tree\n",
    "        #print(sentence, sentence[-n:])\n",
    "        for word in sentence[-n:]:\n",
    "            reference = reference['words'][word]\n",
    "        rand_i = random.randint(0, reference['count'] - 1)\n",
    "        word_index = bisect.bisect_right(reference['bins'], rand_i) - 1\n",
    "        word = list(reference['words'].keys())[word_index]\n",
    "        sentence.append(word)\n",
    "        if word == \"<EOS>\" or len(sentence) == max_length:\n",
    "            return sentence\n",
    "        else:\n",
    "            return self.generate_sentence(sentence)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treegram = NGramsTree(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    ['a', 'a', 'a'],\n",
    "    ['a', 'a', 'a'],\n",
    "    ['a', 'a', 'c'],\n",
    "    ['a', 'b', 'a'],\n",
    "    ['b', 'a', 'a'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 516 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%time treegram.fit(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of what the tree structure looks like for trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bins\": [\n",
      "        0,\n",
      "        4,\n",
      "        5\n",
      "    ],\n",
      "    \"count\": 5,\n",
      "    \"words\": {\n",
      "        \"a\": {\n",
      "            \"bins\": [\n",
      "                0,\n",
      "                3,\n",
      "                4\n",
      "            ],\n",
      "            \"count\": 4,\n",
      "            \"words\": {\n",
      "                \"a\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        2,\n",
      "                        3\n",
      "                    ],\n",
      "                    \"count\": 3,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 2\n",
      "                        },\n",
      "                        \"c\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"b\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ],\n",
      "                    \"count\": 1,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"b\": {\n",
      "            \"bins\": [\n",
      "                0,\n",
      "                1\n",
      "            ],\n",
      "            \"count\": 1,\n",
      "            \"words\": {\n",
      "                \"a\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ],\n",
      "                    \"count\": 1,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(treegram.tree, sort_keys=True,\n",
    "                      indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "treegram = NGramsTree(4)\n",
    "treegram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'He', 'divided', 'his', 'bread', 'and', 'water', 'with', 'me', 'and', 'gave', 'me', 'cheese', 'and', 'sausage', ',', 'and', 'I', 'told', 'her', 'I', 'felt', 'myself', 'quite', 'innocent', 'of', 'any', 'such', 'pretensions', '<EOS>']\n",
      "['<BOS>', '``', 'Yes', ',', 'but', 'with', 'something', 'in', 'it', 'to', 'laugh', 'at', ',', \"''\", 'said', 'he', ';', '``', 'there', 'is', 'no', 'help', 'for', 'it', ';', 'you', 'shall', 'have', 'your', 'way', ',', 'though', 'it', 'will', 'bring', 'you', 'to', 'sorrow', ',', 'my', 'pretty', 'princess', '<EOS>']\n",
      "['<BOS>', 'repeated', 'the', 'Portuguese', '<EOS>']\n",
      "['<BOS>', 'All', 'was', 'joy', 'and', 'gaiety', 'on', 'the', 'ship', 'until', 'long', 'after', 'midnight', '<EOS>']\n",
      "['<BOS>', 'And', 'so', 'they', 'did', ',', 'all', 'three', ',', 'while', 'the', 'fresh', 'stream', 'gushed', 'forth', 'from', 'its', 'mouth', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(treegram.generate_sentence(sentence = ['<BOS>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Seems like the sentences often follow closely with sentences from the book. This is expected as I do no smoothing at all. \n",
    "\n",
    "I would like the sentences to derail a little bit more. This could possibly be solved by increasing the size of my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "treegram = NGramsTree(5)\n",
    "treegram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', '``', 'You', 'bad', 'boy', '<EOS>']\n",
      "['<BOS>', 'The', 'metallic', 'groups', 'of', 'figures', ',', 'among', 'which', 'were', '``', 'Perseus', \"''\", 'and', '``', 'The', 'Rape', 'of', 'the', 'Sabines', ',', \"''\", 'looked', 'like', 'living', 'persons', ',', 'and', 'cries', 'of', 'terror', 'sounded', 'from', 'them', 'all', 'across', 'the', 'noble', 'square', '<EOS>']\n",
      "['<BOS>', '``', 'Why', ',', 'she', 'will', 'kiss', 'me', ',', 'and', 'say', ',', \"'What\", 'the', 'goodman', 'does', 'is', 'always', 'right', '.', '<EOS>']\n",
      "['<BOS>', 'The', 'ant-queen', 'remarked', 'that', 'their', 'conduct', 'that', 'day', 'showed', 'that', 'they', 'possessed', 'kind', 'hearts', 'and', 'good', 'understanding', '<EOS>']\n",
      "['<BOS>', '``', 'All', 'at', 'once', 'I', 'saw', 'the', 'most', 'charming', 'little', 'people', 'marching', 'towards', 'me', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(treegram.generate_sentence(sentence = ['<BOS>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5-grams the sentences seem to be stolen straight out of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
