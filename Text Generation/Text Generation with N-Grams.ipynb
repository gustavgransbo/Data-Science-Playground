{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "During my orchestra [LiTHe Bl√•s](http://litheblas.org)'s 45 year aniversary I will be doing a customary \"Skitsnack\" between two songs. \n",
    "A \"Skitsnack\" is where an orchestra member keeps the audience preoccupied by talking about anything between heaven and earth until we start playing the next song.\n",
    "I often find it hard to decide what I should talk about and usually end up telling really bad jokes or rambling about some algorithm I just read about. \n",
    "\n",
    "For this \"Skitsnack\" I will avoid coming up with my own material entierly by generating it with an n-grams model! \n",
    "The focus of this Notebook is simply to generate a short text which I think could entertain a group of 500 for about 30-60 seconds.\n",
    "\n",
    "There are some nice aspects of the task of generating a \"funny\" text from a predefined dictionary:\n",
    "* I can easily perform extrinsic testing. \"Is this text funny?\"\n",
    "* I don't have to worry about unkown words.\n",
    "\n",
    "As such, any quantitative evaluation will only happen in the spur of the moment.\n",
    "\n",
    "Inspiration:\n",
    "https://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "I haven't decided what data I should use to train my model, and will probably have to try some different sources when the model is finished. The final text will definitely be in Swedish to accomodate the audience, but to get started I will use the Dansih author [H.C. Andersen's Fairy Tales](http://www.gutenberg.org/ebooks/32572) translated to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('hcandersen_fairy_tales.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's remove all text which is not part of his stories, as I don't want to use this for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627 654\n",
      "4021 4048\n",
      "4140 4167\n",
      "375029 375056\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer(r\"HANS ANDERSEN'S FAIRY TALES\", text):\n",
    "    print(i.start(0), i.end(0))\n",
    "    # Hiding the output for future readability\n",
    "    #print(text[i.start(0): i.end(0) + 200])\n",
    "    #print(\"###########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[4167:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367113 367118\n",
      "NOTES\r\n",
      "\r\n",
      "\r\n",
      "THE STORKS\r\n",
      "\r\n",
      "          PAGE 29. On account of the ravages it makes among\r\n",
      "          noxious animals, the stork is a privileged bird\r\n",
      "          wherever it makes its home. In cities it is\r\n",
      "     \n",
      "###########################\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer(r\"NOTES\", text):\n",
    "    print(i.start(0), i.end(0))\n",
    "    print(text[i.start(0): i.end(0) + 200])\n",
    "    print(\"###########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[:367113 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all tabs and linebreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.sub(r'\\r', '', text)\n",
    "text = re.sub(r'\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Sentences\n",
    "First let's parse the text as sentences using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     THE FLAX   THE flax was in full bloom; it had pretty little blue flowers, as delicate as the wings of a moth.\n",
      "The sun shone on it and the showers watered it; and this was as good for the flax as it is for little children to be washed and then kissed by their mothers.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is not good, it includes the story title. I will noth bother with this at the moment though, as this is not the text I will be using for my final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(word_tokenize, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'shone', 'on', 'it', 'and', 'the', 'showers', 'watered', 'it', ';', 'and', 'this', 'was', 'as', 'good', 'for', 'the', 'flax', 'as', 'it', 'is', 'for', 'little', 'children', 'to', 'be', 'washed', 'and', 'then', 'kissed', 'by', 'their', 'mothers', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add beginning and end of sentence tags\n",
    "All my sentences need a root and a stop token. I will insert a `<BOS>` tag before the first word of each sentence and replace the last punctuation of each sentence with `<EOS>`, I do this to distinguish between in sentence punctuation and the actual end of sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(lambda x: ['<BOS>'] + x[:-1] + ['<EOS>'], sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'The', 'sun', 'shone', 'on', 'it', 'and', 'the', 'showers', 'watered', 'it', ';', 'and', 'this', 'was', 'as', 'good', 'for', 'the', 'flax', 'as', 'it', 'is', 'for', 'little', 'children', 'to', 'be', 'washed', 'and', 'then', 'kissed', 'by', 'their', 'mothers', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing n-grams from the setnences\n",
    "Let's try the nltk ngrams package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a generative model\n",
    "When generating my text I will not be considering context across sentences. Each `<EOS>` tag will be followed by a fresh `<BOS>`, interpreted as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGrams():\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def fit(self, sentences):\n",
    "        # Count all ngrams\n",
    "        self.grams = []\n",
    "        for i in range(1, self.n+1):\n",
    "            # Build a sorted list of all ngrams\n",
    "            grams = list(map(lambda x: list(ngrams(x, i)), sentences))\n",
    "            grams = np.array(reduce(lambda x, y: x + y, grams))\n",
    "            grams = grams[np.lexsort(grams[:,::-1].T,),:]\n",
    "            \n",
    "            # Build an array marking the first unique occurence of a n-gram\n",
    "            # Example unigrams, \n",
    "            # a = [('a'), ('a'), ('b'), ('c'), ('c')]\n",
    "            # -> [True, False, True, True, False]\n",
    "            first_occurence = np.append([True], np.array([np.any(grams[i-1] != grams[i]) for i in range(1, len(grams))]))\n",
    "            \n",
    "            # Assign each unique n-gram an index\n",
    "            # Example unigrams, \n",
    "            # a = [('a'), ('a'), ('b'), ('c'), ('c')]\n",
    "            # -> [0, 0, 1, 2, 2]\n",
    "            ids = np.append([0], first_occurence[1:].cumsum())\n",
    "            \n",
    "            # Build a mapping from n-gram to count\n",
    "            frequencies = dict(list(zip(map(tuple, grams[first_occurence,:]), np.bincount(ids))))\n",
    "            self.grams.append(frequencies)\n",
    "        \n",
    "        # Record the total number of unigrams\n",
    "        self.n_unigrams = sum(self.grams[0].values())         \n",
    "        \n",
    "    def log_prob(self, word, context):\n",
    "        n = len(context)\n",
    "        gram = context + (word,)\n",
    "        if n == 0:\n",
    "            return np.log(self.grams[n][gram] / self.n_unigrams)\n",
    "        else:\n",
    "            return np.log(self.grams[n][gram] / self.grams[n-1][context])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3902"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram = NGrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%time trigram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('the', ('!', '--'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4657359027997265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('call', (\"'ll\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.7714179106038408"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('call', ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we can calculate the log probabilities of a word following an ngram. To avoid overfitting to the exact sentences in the text some smoothing should be done. Without smoothing we will assign zero probability to all ngrams that are not present in the text.\n",
    "\n",
    "However, I have encountered a problem: At the moment I don't feel creative enough to come up with a generative model that finds the next word in a sentence based on smoothed probabilities. Maybe I started of in the wrong end? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text generating model\n",
    "I will start of with an unsmoothed model, and at the same time also switch to a tree structure to represent the model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import bisect\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramsTree():\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def fit(self, sentences):\n",
    "        \n",
    "        # Yields all ngrams of a list of sentences, also yields the n-1, n-2, ... 1 grams at the end of each sentence.\n",
    "        # For my model this is important, as I build a single tree with depth n to represent all contexts. The shorter\n",
    "        # grams at the end of the sentence are required to properly fill the top layers of the tree.\n",
    "        def all_grams(n, sentences):\n",
    "            for sentence in sentences:\n",
    "                for i in range(len(sentence)):\n",
    "                    yield sentence[i:min(len(sentence), i+n)]\n",
    "        \n",
    "        # Construct a tree based on the ngrams in the input sentences\n",
    "        self.tree = {}\n",
    "        self.tree['count'] = 0 #len_grams\n",
    "        self.tree['words'] = {}\n",
    "        for gram in all_grams(self.n, sentences):\n",
    "            reference = self.tree\n",
    "            self.tree['count'] += 1\n",
    "            for i, word in enumerate(gram):\n",
    "                if word in reference['words']: \n",
    "                    reference = reference['words'][word]\n",
    "                    reference['count'] += 1\n",
    "                else:\n",
    "                    reference['words'][word] = {}\n",
    "                    reference = reference['words'][word]\n",
    "                    reference['count'] = 1\n",
    "                    # Don't create maps for the leaves\n",
    "                    if i != self.n - 1:\n",
    "                        reference['words'] = {}\n",
    "                        \n",
    "        # Create integer bins for all words in each context based on their apperances\n",
    "        # These bins will be used to find a random successor to contexts with probabilities\n",
    "        # relative to number of appearences. \n",
    "        # Note: Keys in a dictionary are stored in a non-deterministic, but stable, order.\n",
    "        # Therefore associating the bin with index 0 with the key at index 0 is OK.\n",
    "        def build_bins(tree):\n",
    "            if 'words' not in tree:\n",
    "                return\n",
    "            else:\n",
    "                tree['bins'] = [0]\n",
    "                for values in tree['words'].values():\n",
    "                    tree['bins'].append(tree['bins'][-1] + values['count'])\n",
    "                    build_bins(values)\n",
    "        \n",
    "\n",
    "        build_bins(self.tree)\n",
    "\n",
    "        \n",
    "    def generate_sentence(self, sentence, context_size = None, backoff_prob = .25, max_length = 100):\n",
    "        if not context_size:\n",
    "            context_size = self.n - 1\n",
    "            \n",
    "        # Randomly back off to a smaller n with the probability backoff_prob\n",
    "        while random.random() <= backoff_prob and context_size > 0:\n",
    "            context_size-=1\n",
    "        context_size = min(len(sentence), context_size)\n",
    "        reference = self.tree\n",
    "        if context_size > 0:\n",
    "            context = sentence[-context_size:]\n",
    "        else:\n",
    "            context = []\n",
    "        for word in context:\n",
    "            reference = reference['words'][word]\n",
    "        rand_i = random.randint(0, reference['count'] - 1)\n",
    "        word_index = bisect.bisect_right(reference['bins'], rand_i) - 1\n",
    "        word = list(reference['words'].keys())[word_index]\n",
    "        sentence.append(word)\n",
    "        if word == \"<EOS>\" or len(sentence) == max_length:\n",
    "            return sentence\n",
    "        else:\n",
    "            # Generate the next word in the sentence based on what is gerenrated so far\n",
    "            # Do not allow a longer context than what was used this time + 1 \n",
    "            return self.generate_sentence(sentence, min(context_size+1, self.n - 1), backoff_prob, max_length)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treegram = NGramsTree(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    ['a', 'a', 'a'],\n",
    "    ['a', 'a', 'a'],\n",
    "    ['a', 'a', 'c'],\n",
    "    ['a', 'b', 'a'],\n",
    "    ['b', 'a', 'a'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time treegram.fit(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of what the tree structure looks like for trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bins\": [\n",
      "        0,\n",
      "        12,\n",
      "        13,\n",
      "        15\n",
      "    ],\n",
      "    \"count\": 15,\n",
      "    \"words\": {\n",
      "        \"a\": {\n",
      "            \"bins\": [\n",
      "                0,\n",
      "                6,\n",
      "                7,\n",
      "                8\n",
      "            ],\n",
      "            \"count\": 12,\n",
      "            \"words\": {\n",
      "                \"a\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        2,\n",
      "                        3\n",
      "                    ],\n",
      "                    \"count\": 6,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 2\n",
      "                        },\n",
      "                        \"c\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"b\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ],\n",
      "                    \"count\": 1,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"c\": {\n",
      "                    \"bins\": [\n",
      "                        0\n",
      "                    ],\n",
      "                    \"count\": 1,\n",
      "                    \"words\": {}\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"b\": {\n",
      "            \"bins\": [\n",
      "                0,\n",
      "                2\n",
      "            ],\n",
      "            \"count\": 2,\n",
      "            \"words\": {\n",
      "                \"a\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ],\n",
      "                    \"count\": 2,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"c\": {\n",
      "            \"bins\": [\n",
      "                0\n",
      "            ],\n",
      "            \"count\": 1,\n",
      "            \"words\": {}\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(treegram.tree, sort_keys=True,\n",
    "                      indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "treegram = NGramsTree(4)\n",
    "%time treegram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_model_sentence(sentence):\n",
    "    # Strip <BOS> and <EOS> tags.\n",
    "    sentence = sentence[1:-1]\n",
    "    print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the sea is rough , the foam dashes over us ; yet we thank God for this rock\n",
      "Her heart was so filled with sunshine , peace , and joy that it broke , and her courage returned\n",
      "I can not tell you , but you understand no more about poetry than that cask yonder .\n",
      "The swans shook their heads , and the little pea is growing so fast , that I may admire their beauty !\n",
      "But what became of the other young men would gladly have given up his graceful garden flower if he might have worn the one given by the widow in the Bible , it wakes an echo in the heart\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print_model_sentence(treegram.generate_sentence(sentence = ['<BOS>'], backoff_prob = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Seems like the sentences often follow closely with sentences from the book. This is expected as I do no smoothing at all. \n",
    "\n",
    "I would like the sentences to derail a little bit more. This could possibly be solved by increasing the size of my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "treegram = NGramsTree(5)\n",
    "%time treegram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how such highborn people as we came to be in a kitchen .\n",
      "`` Shall we not now hear about the preparation ?\n",
      "When she had once begun , her feet went on dancing , so , that she trod upon the good lady 's toes\n",
      "But dumb she must remain till her task was finished\n",
      "said the painter\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print_model_sentence(treegram.generate_sentence(sentence = ['<BOS>'], backoff_prob = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5-grams most of the sentences seem to be stolen straight out of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "Let's test the model on the task it was intended for. Generating a funny \"Skitsnack\".\n",
    "\n",
    "First of I will need to find a new text, it should preferably be longer than the previous one, and should also be in Swedish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing texts from Spr√•kbanken\n",
    "[Spr√•kbanken](https://spraakbanken.gu.se/swe/resurser) has a large corpus collection. I have downloaded two:\n",
    "* [August Strindbergs romaner](https://spraakbanken.gu.se/swe/resurs/strindbergromaner) - August Strindbergs collected novels and dramas. 321,759 sentences.\n",
    "* [Bloggmix 2015](https://spraakbanken.gu.se/swe/resurs/bloggmix2005) - A collection of swedish blogposts from 2005. \t280,905 sentences. \n",
    "\n",
    "Both contain a lot more information than just sentences. For example, every sentence has been dependency parsed, and every word is accompanied with meta data such as a part of speech tag etc. It could be interesting to use POS tags in a language model. For example it makes sense to assign extra probability to all nouns in context where we most often see nouns.\n",
    "\n",
    "The sentences in both corporas have been reordered. The sentences internal structures are not altered, but it is not possible to look for context outside of the sentence. This is not a problem for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloggmix 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('sprakbanken/bloggmix2005.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure:\n",
    "```xml\n",
    "<corpus>\n",
    "   <blog>\n",
    "       <text>\n",
    "           <sentence>\n",
    "               <w>\n",
    "               </w>\n",
    "           </sentence>\n",
    "       </text>\n",
    "   </blog>\n",
    "</corpus>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the blogs in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog: Tatiana Rojas\n",
      "Blog: BY KAROLINAA\n",
      "Blog: Angelicas Gn√§llb√§nk\n",
      "Blog: Emma's WindOw\n",
      "Blog: Johanna Sj√∂din\n"
     ]
    }
   ],
   "source": [
    "for child in root.getchildren()[:5]:\n",
    "    print(\"Blog: {}\".format(child.attrib['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_sentence(sentence_xml):\n",
    "    sentence = []\n",
    "    for word in sentence_xml.getchildren():\n",
    "        text = word.text\n",
    "        text = \"<CENSURERAT>\" if text == \"\\n\" else text\n",
    "        sentence.append(text)\n",
    "    return sentence\n",
    "\n",
    "def parse_text(text_xml):\n",
    "    text = []\n",
    "    for sentence in text_xml.getchildren():\n",
    "        text.append(parse_sentence(sentence))\n",
    "    return text\n",
    "\n",
    "def parse_blog(blog_xml):\n",
    "    blog = []\n",
    "    for text in blog_xml.getchildren():\n",
    "        blog.append(parse_text(text))\n",
    "    return list(reduce(lambda x, y: x + y, blog)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blogs = []\n",
    "for child in root.getchildren():\n",
    "    blogs.append(parse_blog(child))\n",
    "\n",
    "blog_sentences = list(reduce(lambda x, y: x + y, blogs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Free some memory\n",
    "del tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blog_sentences = list(map(lambda x: ['<BOS>'] + x + ['<EOS>'], blog_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>',\n",
       " '√Ñter',\n",
       " '<CENSURERAT>',\n",
       " 'gud',\n",
       " 'vet',\n",
       " 'hur',\n",
       " 'm√•nga',\n",
       " 'kilo',\n",
       " 'jag',\n",
       " 'har',\n",
       " 'g√•tt',\n",
       " 'upp',\n",
       " 'men',\n",
       " 'jag',\n",
       " 'bryr',\n",
       " 'mig',\n",
       " 'inte',\n",
       " ',',\n",
       " 'jag',\n",
       " 'lever',\n",
       " 'loppan',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280905"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the full data set quickly has my machine run out of memory, causing a bottleneck when it starts reading and writing to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "fourgram = NGramsTree(4)\n",
    "%time fourgram.fit(blog_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... fast den h√§r g√•ngen , kanske svart .. de √§r i livet ?\n",
      "Certainly an initiative worth applauding , in particular if we were really righteous , we would not have been that wrong .\n",
      "<CENSURERAT> river choklad till desserten <CENSURERAT> ( chokladgratinerad frukt ) Tasteline Ingredienser 2 kiwi 1 banan 2 √§pplen <CENSURERAT> jordgubbar <CENSURERAT> florsocker <CENSURERAT> vit choklad ev. citronmeliss till garnering G√∂r s√• h√§r : \" Ist√§llet f√∂r det som skall komma √∂ver v√§rlden .\n",
      "Antagligen f√•r jag b√•de ett webinterface och RSS utan att beh√∂va kl√§ttra p√• l√•dor !\n",
      "S√• ja , jag vet : alla har inte tilg√•ng till dator i hemmet .\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 35s\n"
     ]
    }
   ],
   "source": [
    "fivegram = NGramsTree(5)\n",
    "%time fivegram.fit(blog_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... fast den h√§r √§r √§nd√• roligast av alla .\n",
      ":) <CENSURERAT> l√•g jag i s√§ngen <CENSURERAT> , de va s√• j√§vla sk√∂nt .\n",
      "Min kv√§ll p√• <CENSURERAT> var i alla fall r√§tt ofta f√∂r olika typer av svartvit film .\n",
      "Barnets biologiska f√∂r√§ldrars juridiska band till barnet upph√∂r .\n",
      "Det var organisationen barnen f√∂rst som h√∂ll i evenemanget och vi var i iranska asylgruppen lokaler i <CENSURERAT> .\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cool, the sentences have a nice flow but often derail. More so in the case of 4-grams than 5-grams, which still often look to be stolen straight from the blogs.\n",
    "\n",
    "To tackle this I will add some kind of probability of choosing words present in a shorter context. Ideally this should be done with a proper backoff model, but as the day of the big \"Skitsnack\" is drawign closer I will probably just implement some quasi rational solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# With backoff\n",
    "I implemented backoff in context size while generating sentences. It works like this:\n",
    "1. Find largest possible context. This is min(sentence length, n-1, c + 1) where n is the models n value and c is the context size used to generate the previous word.\n",
    "2. Subtract 1 from the chosen context size with probability `backoff_prob`. Repeat until context size is either 0 or no subtraction was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff probability 0%:\n",
      "... fast den h√§r g√•ngen , kanske svart .. de √§r i livet ?\n",
      "Certainly an initiative worth applauding , in particular if we were really righteous , we would not have been that wrong .\n",
      "<CENSURERAT> river choklad till desserten <CENSURERAT> ( chokladgratinerad frukt ) Tasteline Ingredienser 2 kiwi 1 banan 2 √§pplen <CENSURERAT> jordgubbar <CENSURERAT> florsocker <CENSURERAT> vit choklad ev. citronmeliss till garnering G√∂r s√• h√§r : \" Ist√§llet f√∂r det som skall komma √∂ver v√§rlden .\n",
      "Antagligen f√•r jag b√•de ett webinterface och RSS utan att beh√∂va kl√§ttra p√• l√•dor !\n",
      "S√• ja , jag vet : alla har inte tilg√•ng till dator i hemmet .\n",
      "\n",
      "Backoff probability 5.0%:\n",
      "... S√Ö snabba √§r vi ;) <CENSURERAT> och lite rouge , l√§pglans Fest : mkt <CENSURERAT> , markerade √∂gon , l√§tt l√§ppglans 6 .\n",
      "Det verkar inte blir n√•got , men kanske ska g√• imon .\n",
      "Om du ser att n√•gon har behovet att uppm√§rksamma andras , vad de amerikanska skattebetalarna s√• g√∂r oljebolagen v√§rsta gl√§djeskutten √∂ver situationen .\n",
      "<CENSURERAT> ska jag f√∂rs√∂ka uppdatera er om kan ja ju sl√§nga in en kommentar s√• skall jag ocks√• ha n√•got s√•nt .\n",
      "Jag har varit p√• ultraljud , och allt tyder p√• att missbruket av kokain √§r ordentligt utbrett .\n",
      "\n",
      "Backoff probability 10.0%:\n",
      "... S√Ö snabba √§r vi ;) <CENSURERAT> och lite rouge , l√§pglans Fest : mkt <CENSURERAT> , markerade √∂gon , l√§tt l√§ppglans 6 .\n",
      "Betyg ?\n",
      "Det betyder i praktiken en revolution f√∂r t.ex. bilindustri och milj√∂v√§nner .\n",
      "I.s.f .\n",
      "Barnets biologiska f√∂r√§ldrars juridiska band till barnet upph√∂r .\n",
      "\n",
      "Backoff probability 15.000000000000002%:\n",
      "... S√Ö snabba √§r vi ;) √ñgonen blir verkligen helt k√§r !\n",
      "Please , do not find anybody who was sawn asunder . '\n",
      "by <CENSURERAT> , at the consummation of everything .\n",
      "<CENSURERAT> , <CENSURERAT> 1969 <CENSURERAT> , del 1 Allt mitt √§r ditt ‚Äì de tv√• med ett s√•dant som tr√§sket kring <CENSURERAT> , som var l√§tt att f√• ihop alla po√§ng med s√• l√§nkar jag er allihop !\n",
      "Puss ‚ô•\n",
      "\n",
      "Backoff probability 20.0%:\n",
      "... S√Ö snabba √§r vi ;) √ñgonen blir verkligen helt k√§r !\n",
      "Please , do not find anybody who was sawn asunder . '\n",
      "by <CENSURERAT> , at the consummation of everything .\n",
      "<CENSURERAT> , <CENSURERAT> 1969 <CENSURERAT> , del 1 Allt mitt √§r ditt ‚Äì de tv√• med ett s√•dant som tr√§sket kring <CENSURERAT> , som var l√§tt att f√• ihop alla po√§ng med s√• l√§nkar jag er !\n",
      "Puss ‚ô•\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "print(\"Backoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))    \n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff probability 0%:\n",
      "<CENSURERAT> skulle <CENSURERAT> duga gott ocks√• , eller till och med mer slitet √§n vanligt .\n",
      "I sin kommentar ber han mig att kommentera <CENSURERAT> storhet .\n",
      "Och i min trappa springer numera tv√• brevb√§rare , hur nu det kan l√∂na sig .. √Ñr det n√•gon som vet hur man dundrar p√• mot str√∂mmen √§r <CENSURERAT> .\n",
      "Mysigt med lite grillat .\n",
      "Allts√• , man vill ju ha ett fungerande nagellack ! ! !\n",
      "\n",
      "Backoff probability 5%:\n",
      "<CENSURERAT> skulle <CENSURERAT> duga gott ocks√• , eller till och med en 24-timmars emergency phone number .\n",
      "D√§rf√∂r avslutade jag det ocks√• , f√∂r jag vill egentligen inte publicera bilden p√• mig haha tycker jag ser lite ut som en man ?\n",
      "Hoppas att jag kan fixa detta !\n",
      "Jag hade ju st√§llt in mig p√• det , dock inte pappa .\n",
      "Allts√• , bl√§ .\n",
      "\n",
      "Backoff probability 10%:\n",
      "<CENSURERAT> skulle <CENSURERAT> duga gott ocks√• , eller till och med dansa i s√∂mnen . ‚Äù Undrar om de som g√•r i s√∂mnen har lite f√∂r starka s√•dana impulser d√• ?\n",
      "Jag tror man ska vara d√§r <CENSURERAT> ... K√§nns som att jag bara har varit d√§r <CENSURERAT> , gjorde han isl√§ndarna s√• f√∂rbittrade genom sitt ursprung n√•gonstans p√• <CENSURERAT> sl√§tter varifr√•n hon under √•rmiljonerna har spridits till v√§rldens fyra h√∂rn .\n",
      "Oooah , jag saknar konst√•kningen .. Just nu k√§nns det som om dom j√§vlas med folk bra f√∂r att f√∂rtj√§na n√•got s√•dant .\n",
      "Har sjukt d√•ligt samvete f√∂r att jag inte sovit s√• mycket h a h a .\n",
      ". \"\n",
      "\n",
      "Backoff probability 15%:\n",
      "<CENSURERAT> skulle <CENSURERAT> duga gott ocks√• , eller till och med dansa i s√∂mnen . ‚Äù Undrar om de som g√•r √§r unga ; de som st√∂der √§r unga och bor i <CENSURERAT> vet vi att v√•ren √§r h√§r !\n",
      "Jag sv√§r livet blir b√§ttre n√§r man vet vart man ska b√∂rja n√•gonstans .\n",
      "<CENSURERAT> en fr√•ga i ett annat inl√§gg t√§nkte jag .\n",
      "Bra !\n",
      "D√∂mer varandra dessutom .\n",
      "\n",
      "Backoff probability 20%:\n",
      "<CENSURERAT> skulle <CENSURERAT> duga gott ocks√• , eller till och med dansa i s√∂mnen . ‚Äù Undrar om de som inte v√§cks av dunket n√§r tidningen ramlar in vid 05.00 F√∂rsta citatet √§r en kommentar till att den socialdemokratiska partiledningen st√§llt sig bakom ett f√∂rslag om att det ska hj√§lpa dem .\n",
      "Men <CENSURERAT> med samma saker att l√§gga ner s√• blir minst 4-5 personer arbetsl√∂sa och S√∂der och <CENSURERAT> f√∂rlorar en viktig m√∂tesplats , tillika en h√§rlig butik och kiosk med st√§mning fr√•n \" den gamla goda tiden , d√• jag hade mina duster med dessa rasistiska ynkryggar .\n",
      "det forts√§tter s√• h√§r .\n",
      "Ett konto f√∂r att kunna vika allt m√∂jligt , jag vet inget annat s√§tt , s√• l√•t mig vara om du ser dig som en v√§n kom inte hit igen Du g√∂r bara saker v√§rre √§n de √§r till och med s√§mre √§n Miuns .\n",
      "Och kan <CENSURERAT> till Handen <CENSURERAT> .\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "print(\"Backoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))    \n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(round(p * 100)))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20% backoff looks pretty good!\n",
    "\n",
    "The censored words are a bit of a buzzkill. It seems like dates, names and products are consored, maybe other things as well.\n",
    "\n",
    "I could fill some of the tags in manually, but in a lot of cases I'm not sure with what. I sohuld probably have excluded all sentences with consored words from the corpus, but for now let's just create some sentences and keep the ones I can make sense of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Det √§r helt tomt i bollen , och JIPPIIEEE va kul det √§r !\n",
      "\n",
      "<CENSURERAT> ville dock ABB att de anst√§llda skulle byta till Metall , eftersom arbetarna vid de regelbundna kvartalsm√∂tena och konferenserna .\n",
      "\n",
      "Och speciellt n√§r de s√§ger att stockholmare √§r ytliga och otrevliga m√§nniskor i ledningen som de som utpekats med namns n√§mnande i katastrofkommitt√©ns rapport .\n",
      "\n",
      "Dammsuga och \" undre v√§rlden , inte <CENSURERAT> och inte <CENSURERAT> som spelar center .\n",
      "\n",
      "Ja , jag har pratat l√§nge om att ordna med lite fondv√§ggar i b√•de <CENSURERAT> rum , igen , f√∂r jag nu journal √∂ver allt som g√∂rs och inte g√∂rs s√• att jag har flugit av min h√§st <CENSURERAT> .\n",
      "\n",
      "Jag har skrivit <CENSURERAT> , trots att jag hade svinont efter m√•ndagen s√• putsade jag en massa f√∂nster .\n",
      "\n",
      "Hur som , <CENSURERAT> √§r en av de f√• som √§lskar <CENSURERAT> OCH <CENSURERAT> .\n",
      "\n",
      "Jag vet inte hur det kan vara n√•gons favorit film i dagens samh√§lle haha Vad tycker ni om bilderna ? ?\n",
      "\n",
      "Kan avsl√∂ja att vi inte blev d√∂pta n√§r vi var barn , s√• skulle betydligt f√§rre l√•ta d√∂pa sig .\n",
      "\n",
      "Sjunger : - \" Min Konung mig kallar , jag g√•r p√• penicillin .√Ñr s√• tr√∂tt , s√• j√§vla tr√∂tt .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(7)\n",
    "for i in range(10):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Hon vek en pappersb√•t av t√•gbiljetten och skickade iv√§g ett sms fr√•n Franrike . \\\n",
    "Det √§r en enast√•ende helt unik historia , ja , rent av d√∂dliga personskador . \\\n",
    "Masjv√§lar var en besvikelse , liksom m√•nga av hemv√§ndarna . \\\n",
    "Darf√∂r t√§nker jag inte titta haha . \\\n",
    "Herregud allts√• ... Ska spela mer sims nu och gl√∂mma att mitt rum √§r en bra l√•t ! \\\n",
    "Exakt s√•h√§r vill jag att min mormor hatade mig , och hon √§r till besv√§r och att debatterna hon f√∂r p√•minner om d√•liga s√•por . \\\n",
    "Men hon hyschar √•t oss att det √§r HAN , eller N√ÖN , eller N√ÖT ‚Ä¶ .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shorter_text = \"Hon vek en pappersb√•t av t√•gbiljetten och skickade iv√§g ett sms fr√•n Franrike . \\\n",
    "Det √§r en enast√•ende helt unik historia , ja , rent av d√∂dliga personskador . \\\n",
    "Darf√∂r t√§nker jag inte titta haha . \\\n",
    "Exakt s√•h√§r vill jag att min mormor hatade mig , och hon √§r till besv√§r och att debatterna hon f√∂r p√•minner om d√•liga s√•por . \\\n",
    "Men hon hyschar √•t oss att det √§r HAN , eller N√ÖN , eller N√ÖT ‚Ä¶ .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Hon vek en pappersb√•t av t√•gbiljetten och skickade iv√§g ett sms fr√•n Franrike . \\\n",
    "Det √§r en enast√•ende helt unik historia , ja , rent av d√∂dliga personskador . \\\n",
    "Darf√∂r t√§nker jag inte titta haha . \\\n",
    "Exakt s√•h√§r vill jag att min mormor hatade mig , och hon √§r till besv√§r och att debatterna hon f√∂r p√•minner om d√•liga s√•por . \\\n",
    "Men hon hyschar √•t oss att det √§r HAN , eller N√ÖN , eller N√ÖT ‚Ä¶ .\\\n",
    "Juristerna anser att lagen inte ger dem en √∂rfil ‚Äì det √§r m√§nskligt , men inte OK. \\\n",
    "Hela g√§nget har anv√§nt sina Cowboyboots flitigt det g√•ngna √•ret . \\\n",
    "Stannar han kvar s√• ska jag tr√§na lite faktiskt , hejhopp.\\\n",
    "Alla √§r vi puckon n√•n g√•ng och alla √§r vi godingar . \\\n",
    "Det √§r helt tomt i bollen , och JIPPIIEEE va kul det √§r ! \\\n",
    "Sjunger : -  Min Konung mig kallar , jag g√•r p√• penicillin . √Ñr s√• tr√∂tt , s√• j√§vla tr√∂tt . \\\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech\n",
    "Let's have a robot voice do the \"Skitsnack\" for extra AI vibes. I'll use Google's Text to Speech API, via the gtts package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tts = gTTS(long_text, lang = 'sv', slow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.save('tts_files/long_text.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "## Results\n",
    "I implemented a very simple N-Gram model, and made it generate random sentences with a hardcoded probability of backing off to shorter contexts.\n",
    "\n",
    "I found a backoff probability I liked for the 5-Gram model and generated some sentences which I finally converted to a sound file using Googles Text to Speech API.\n",
    "\n",
    "## Limitations encountered\n",
    "When training a 5-Gram model on the 4.8 million tokens of the Bloggmix2015 corpus I quickly ran out of memory on my 8GB RAM laptop. This lead to pretty slow training time, and even some delays when generating sentences as parts of the model had to be read from disk.\n",
    "\n",
    "## Improvements\n",
    "I think my idea of generating sentences the way I did makes sense. Choosing a word with probability based on how often it has appeared in the given context is exactly what an N-Gram model is about. I also think backing off to a smaller context was a good idea to avoid following the original texts to closely. However, I would have liked to back of with probabilities chosen in a more sound way. One way to choose such probabilities would be to fit the model against a held out corpus.\n",
    "\n",
    "If I circle back to this project to improve it, I would like to implement a new N-Gram model with a proper smoothing method such as Kneser-Ney Smoothing. This time around I avoided this because I needed to produce results quickly, with no real quality requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
