{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "During my orchestra [LiTHe Bl√•s](http://litheblas.org)'s 45 year aniversary I will be doing a customary \"Skitsnack\" between two songs. \n",
    "A \"Skitsnack\" is where an orchestra member keeps the audience preoccupied by talking about anything between heaven and earth until we start playing the next song.\n",
    "I often find it hard to decide what I should talk about and usually end up telling really bad jokes or rambling about some algorithm I just read about. \n",
    "\n",
    "For this \"Skitsnack\" I will avoid coming up with my own material entierly by generating it with an n-grams model! \n",
    "The focus of this Notebook is simply to generate a short text which I think could entertain a group of 500 for about 30-60 seconds.\n",
    "\n",
    "There are some nice aspects of the task of generating a \"funny\" text from a predefined dictionary:\n",
    "* I can easily perform extrinsic testing. \"Is this text funny?\"\n",
    "* I don't have to worry about unkown words.\n",
    "\n",
    "As such, any quantitative evaluation will only happen in the spur of the moment.\n",
    "\n",
    "Inspiration:\n",
    "https://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "I haven't decided what data I should use to train my model, and will probably have to try some different sources when the model is finished. The final text will definitely be in Swedish to accomodate the audience, but to get started I will use the Dansih author [H.C. Andersen's Fairy Tales](http://www.gutenberg.org/ebooks/32572) translated to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('hcandersen_fairy_tales.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's remove all text which is not part of his stories, as I don't want to use this for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627 654\n",
      "4021 4048\n",
      "4140 4167\n",
      "375029 375056\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer(r\"HANS ANDERSEN'S FAIRY TALES\", text):\n",
    "    print(i.start(0), i.end(0))\n",
    "    # Hiding the output for future readability\n",
    "    #print(text[i.start(0): i.end(0) + 200])\n",
    "    #print(\"###########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[4167:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367113 367118\n",
      "NOTES\r\n",
      "\r\n",
      "\r\n",
      "THE STORKS\r\n",
      "\r\n",
      "          PAGE 29. On account of the ravages it makes among\r\n",
      "          noxious animals, the stork is a privileged bird\r\n",
      "          wherever it makes its home. In cities it is\r\n",
      "     \n",
      "###########################\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer(r\"NOTES\", text):\n",
    "    print(i.start(0), i.end(0))\n",
    "    print(text[i.start(0): i.end(0) + 200])\n",
    "    print(\"###########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = text[:367113 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all tabs and linebreaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.sub(r'\\r', '', text)\n",
    "text = re.sub(r'\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Sentences\n",
    "First let's parse the text as sentences using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     THE FLAX   THE flax was in full bloom; it had pretty little blue flowers, as delicate as the wings of a moth.\n",
      "The sun shone on it and the showers watered it; and this was as good for the flax as it is for little children to be washed and then kissed by their mothers.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is not good, it includes the story title. I will noth bother with this at the moment though, as this is not the text I will be using for my final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(word_tokenize, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'shone', 'on', 'it', 'and', 'the', 'showers', 'watered', 'it', ';', 'and', 'this', 'was', 'as', 'good', 'for', 'the', 'flax', 'as', 'it', 'is', 'for', 'little', 'children', 'to', 'be', 'washed', 'and', 'then', 'kissed', 'by', 'their', 'mothers', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add beginning and end of sentence tags\n",
    "All my sentences need a root and a stop token. I will insert a `<BOS>` tag before the first word of each sentence and replace the last punctuation of each sentence with `<EOS>`, I do this to distinguish between in sentence punctuation and the actual end of sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(lambda x: ['<BOS>'] + x[:-1] + ['<EOS>'], sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'The', 'sun', 'shone', 'on', 'it', 'and', 'the', 'showers', 'watered', 'it', ';', 'and', 'this', 'was', 'as', 'good', 'for', 'the', 'flax', 'as', 'it', 'is', 'for', 'little', 'children', 'to', 'be', 'washed', 'and', 'then', 'kissed', 'by', 'their', 'mothers', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing n-grams from the setnences\n",
    "Let's try the nltk ngrams package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a generative model\n",
    "When generating my text I will not be considering context across sentences. Each `<EOS>` tag will be followed by a fresh `<BOS>`, interpreted as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGrams():\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def fit(self, sentences):\n",
    "        # Count all ngrams\n",
    "        self.grams = []\n",
    "        for i in range(1, self.n+1):\n",
    "            # Build a sorted list of all ngrams\n",
    "            grams = list(map(lambda x: list(ngrams(x, i)), sentences))\n",
    "            grams = np.array(reduce(lambda x, y: x + y, grams))\n",
    "            grams = grams[np.lexsort(grams[:,::-1].T,),:]\n",
    "            \n",
    "            # Build an array marking the first unique occurence of a n-gram\n",
    "            # Example unigrams, \n",
    "            # a = [('a'), ('a'), ('b'), ('c'), ('c')]\n",
    "            # -> [True, False, True, True, False]\n",
    "            first_occurence = np.append([True], np.array([np.any(grams[i-1] != grams[i]) for i in range(1, len(grams))]))\n",
    "            \n",
    "            # Assign each unique n-gram an index\n",
    "            # Example unigrams, \n",
    "            # a = [('a'), ('a'), ('b'), ('c'), ('c')]\n",
    "            # -> [0, 0, 1, 2, 2]\n",
    "            ids = np.append([0], first_occurence[1:].cumsum())\n",
    "            \n",
    "            # Build a mapping from n-gram to count\n",
    "            frequencies = dict(list(zip(map(tuple, grams[first_occurence,:]), np.bincount(ids))))\n",
    "            self.grams.append(frequencies)\n",
    "        \n",
    "        # Record the total number of unigrams\n",
    "        self.n_unigrams = sum(self.grams[0].values())         \n",
    "        \n",
    "    def log_prob(self, word, context):\n",
    "        n = len(context)\n",
    "        gram = context + (word,)\n",
    "        if n == 0:\n",
    "            return np.log(self.grams[n][gram] / self.n_unigrams)\n",
    "        else:\n",
    "            return np.log(self.grams[n][gram] / self.grams[n-1][context])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3902"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram = NGrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%time trigram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('the', ('!', '--'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4657359027997265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('call', (\"'ll\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.7714179106038408"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.log_prob('call', ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we can calculate the log probabilities of a word following an ngram. To avoid overfitting to the exact sentences in the text some smoothing should be done. Without smoothing we will assign zero probability to all ngrams that are not present in the text.\n",
    "\n",
    "However, I have encountered a problem: At the moment I don't feel creative enough to come up with a generative model that finds the next word in a sentence based on smoothed probabilities. Maybe I started of in the wrong end? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text generating model\n",
    "I will start of with an unsmoothed model, and at the same time also switch to a tree structure to represent the model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import bisect\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramsTree():\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def fit(self, sentences):\n",
    "        \n",
    "        # Yields all ngrams of a list of sentences, also yields the n-1, n-2, ... 1 grams at the end of each sentence.\n",
    "        # For my model this is important, as I build a single tree with depth n to represent all contexts. The shorter\n",
    "        # grams at the end of the sentence are required to properly fill the top layers of the tree.\n",
    "        def all_grams(n, sentences):\n",
    "            for sentence in sentences:\n",
    "                for i in range(len(sentence)):\n",
    "                    yield sentence[i:min(len(sentence), i+n)]\n",
    "        \n",
    "        # Construct a tree based on the ngrams in the input sentences\n",
    "        self.tree = {}\n",
    "        self.tree['count'] = 0 #len_grams\n",
    "        self.tree['words'] = {}\n",
    "        for gram in all_grams(self.n, sentences):\n",
    "            reference = self.tree\n",
    "            self.tree['count'] += 1\n",
    "            for i, word in enumerate(gram):\n",
    "                if word in reference['words']: \n",
    "                    reference = reference['words'][word]\n",
    "                    reference['count'] += 1\n",
    "                else:\n",
    "                    reference['words'][word] = {}\n",
    "                    reference = reference['words'][word]\n",
    "                    reference['count'] = 1\n",
    "                    # Don't create maps for the leaves\n",
    "                    if i != self.n - 1:\n",
    "                        reference['words'] = {}\n",
    "                        \n",
    "        # Create integer bins for all words in each context based on their apperances\n",
    "        # These bins will be used to find a random successor to contexts with probabilities\n",
    "        # relative to number of appearences. \n",
    "        # Note: Keys in a dictionary are stored in a non-deterministic, but stable, order.\n",
    "        # Therefore associating the bin with index 0 with the key at index 0 is OK.\n",
    "        def build_bins(tree):\n",
    "            if 'words' not in tree:\n",
    "                return\n",
    "            else:\n",
    "                tree['bins'] = [0]\n",
    "                for values in tree['words'].values():\n",
    "                    tree['bins'].append(tree['bins'][-1] + values['count'])\n",
    "                    build_bins(values)\n",
    "        \n",
    "\n",
    "        build_bins(self.tree)\n",
    "\n",
    "        \n",
    "    def generate_sentence(self, sentence, n = None, backoff_prob = .25, max_length = 100):\n",
    "        if not n:\n",
    "            n = self.n\n",
    "        #print(\"Initial n: {}\".format(n))\n",
    "        # Randomly back off to a smaller n with the probability backoff_prob\n",
    "        while random.random() <= backoff_prob and n > 1:\n",
    "            n-=1\n",
    "        #print(\"Backoff n: {}\".format(n))\n",
    "        n = min(len(sentence), n - 1)\n",
    "        #print(\"Final n: {}\".format(n))\n",
    "        reference = self.tree\n",
    "        #print(\"Context: {}\".format(sentence[-n:]))\n",
    "        if n > 0:\n",
    "            context = sentence[-n:]\n",
    "        else:\n",
    "            context = []\n",
    "        for word in context:\n",
    "            reference = reference['words'][word]\n",
    "        rand_i = random.randint(0, reference['count'] - 1)\n",
    "        word_index = bisect.bisect_right(reference['bins'], rand_i) - 1\n",
    "        word = list(reference['words'].keys())[word_index]\n",
    "        sentence.append(word)\n",
    "        if word == \"<EOS>\" or len(sentence) == max_length:\n",
    "            return sentence\n",
    "        else:\n",
    "            # Generate the next word in the sentence based on what is gerenrated so far\n",
    "            # Do not allow a longer context than what was used this time + 2 \n",
    "            return self.generate_sentence(sentence, min(n+2, self.n), backoff_prob, max_length)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treegram = NGramsTree(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentences = [\n",
    "    ['a', 'a', 'a'],\n",
    "    ['a', 'a', 'a'],\n",
    "    ['a', 'a', 'c'],\n",
    "    ['a', 'b', 'a'],\n",
    "    ['b', 'a', 'a'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time treegram.fit(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of what the tree structure looks like for trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bins\": [\n",
      "        0,\n",
      "        12,\n",
      "        13,\n",
      "        15\n",
      "    ],\n",
      "    \"count\": 15,\n",
      "    \"words\": {\n",
      "        \"a\": {\n",
      "            \"bins\": [\n",
      "                0,\n",
      "                6,\n",
      "                7,\n",
      "                8\n",
      "            ],\n",
      "            \"count\": 12,\n",
      "            \"words\": {\n",
      "                \"a\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        2,\n",
      "                        3\n",
      "                    ],\n",
      "                    \"count\": 6,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 2\n",
      "                        },\n",
      "                        \"c\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"b\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ],\n",
      "                    \"count\": 1,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"c\": {\n",
      "                    \"bins\": [\n",
      "                        0\n",
      "                    ],\n",
      "                    \"count\": 1,\n",
      "                    \"words\": {}\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"b\": {\n",
      "            \"bins\": [\n",
      "                0,\n",
      "                2\n",
      "            ],\n",
      "            \"count\": 2,\n",
      "            \"words\": {\n",
      "                \"a\": {\n",
      "                    \"bins\": [\n",
      "                        0,\n",
      "                        1\n",
      "                    ],\n",
      "                    \"count\": 2,\n",
      "                    \"words\": {\n",
      "                        \"a\": {\n",
      "                            \"count\": 1\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"c\": {\n",
      "            \"bins\": [\n",
      "                0\n",
      "            ],\n",
      "            \"count\": 1,\n",
      "            \"words\": {}\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(treegram.tree, sort_keys=True,\n",
    "                      indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 956 ms\n"
     ]
    }
   ],
   "source": [
    "treegram = NGramsTree(4)\n",
    "%time treegram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_model_sentence(sentence):\n",
    "    # Strip <BOS> and <EOS> tags.\n",
    "    sentence = sentence[1:-1]\n",
    "    print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the sound came also a strong , rushing wind , its stormy breath clearly uttering the words , `` Everything in its right place , '' said the little princess , dressed in silk and velvet , rejoicing in the balmy breezes laden with the fragrance from the fresh verdure , and the mermaid saw that the old people were very active and industrious ; they were at rest and had much better things to do\n",
      "In a moment there came into the garden\n",
      "The king and queen are present .\n",
      "Not a bird was to be married\n",
      "Suddenly he fancied he heard feet outside going pitapat\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print_model_sentence(treegram.generate_sentence(sentence = ['<BOS>'], backoff_prob = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Seems like the sentences often follow closely with sentences from the book. This is expected as I do no smoothing at all. \n",
    "\n",
    "I would like the sentences to derail a little bit more. This could possibly be solved by increasing the size of my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "treegram = NGramsTree(5)\n",
    "%time treegram.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` We are glad of that , '' said Great Claus\n",
      "Attend to me ; that 's far more important\n",
      "`` I will not fly , '' said one of the plants in the room , who caught him and stuck him on a pin in a box of curiosities\n",
      "The lamp before the picture of the Madonna threw a strong light on the pale , delicate face of the child\n",
      "The color has been changed by age to dark green , but clear , fresh water pours from the snout , which shines as if it had been a beautiful human being\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print_model_sentence(treegram.generate_sentence(sentence = ['<BOS>'], backoff_prob = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5-grams most of the sentences seem to be stolen straight out of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "Let's test the model on the task it was intended for. Generating a funny \"Skitsnack\".\n",
    "\n",
    "First of I will need to find a new text, it should preferably be longer than the previous one, and should also be in Swedish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing texts from Spr√•kbanken\n",
    "[Spr√•kbanken](https://spraakbanken.gu.se/swe/resurser) has a large corpus collection. I have downloaded two:\n",
    "* [August Strindbergs romaner](https://spraakbanken.gu.se/swe/resurs/strindbergromaner) - August Strindbergs collected novels and dramas. 321,759 sentences.\n",
    "* [Bloggmix 2015](https://spraakbanken.gu.se/swe/resurs/bloggmix2005) - A collection of swedish blogposts from 2005. \t280,905 sentences. \n",
    "\n",
    "Both contain a lot more information than just sentences. For example, every sentence has been dependency parsed, and every word is accompanied with meta data such as a part of speech tag etc. It could be interesting to use POS tags in a language model. For example it makes sense to assign extra probability to all nouns in context where we most often see nouns.\n",
    "\n",
    "The sentences in both corporas have been reordered. The sentences internal structures are not altered, but it is not possible to look for context outside of the sentence. This is not a problem for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloggmix 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('sprakbanken/bloggmix2005.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure:\n",
    "```xml\n",
    "<corpus>\n",
    "   <blog>\n",
    "       <text>\n",
    "           <sentence>\n",
    "               <w>\n",
    "               </w>\n",
    "           </sentence>\n",
    "       </text>\n",
    "   </blog>\n",
    "</corpus>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the blogs in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog: Tatiana Rojas\n",
      "Blog: BY KAROLINAA\n",
      "Blog: Angelicas Gn√§llb√§nk\n",
      "Blog: Emma's WindOw\n",
      "Blog: Johanna Sj√∂din\n"
     ]
    }
   ],
   "source": [
    "for child in root.getchildren()[:5]:\n",
    "    print(\"Blog: {}\".format(child.attrib['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_sentence(sentence_xml):\n",
    "    sentence = []\n",
    "    for word in sentence_xml.getchildren():\n",
    "        text = word.text\n",
    "        text = \"<CENSURERAT>\" if text == \"\\n\" else text\n",
    "        sentence.append(text)\n",
    "    return sentence\n",
    "\n",
    "def parse_text(text_xml):\n",
    "    text = []\n",
    "    for sentence in text_xml.getchildren():\n",
    "        text.append(parse_sentence(sentence))\n",
    "    return text\n",
    "\n",
    "def parse_blog(blog_xml):\n",
    "    blog = []\n",
    "    for text in blog_xml.getchildren():\n",
    "        blog.append(parse_text(text))\n",
    "    return list(reduce(lambda x, y: x + y, blog)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blogs = []\n",
    "for child in root.getchildren():\n",
    "    blogs.append(parse_blog(child))\n",
    "\n",
    "sentences = list(reduce(lambda x, y: x + y, blogs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(map(lambda x: ['<BOS>'] + x + ['<EOS>'], sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>',\n",
       " '√Ñter',\n",
       " '<CENSURERAT>',\n",
       " 'gud',\n",
       " 'vet',\n",
       " 'hur',\n",
       " 'm√•nga',\n",
       " 'kilo',\n",
       " 'jag',\n",
       " 'har',\n",
       " 'g√•tt',\n",
       " 'upp',\n",
       " 'men',\n",
       " 'jag',\n",
       " 'bryr',\n",
       " 'mig',\n",
       " 'inte',\n",
       " ',',\n",
       " 'jag',\n",
       " 'lever',\n",
       " 'loppan',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280905"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the full data set takes quite a while. Let's train on a subset of the data initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "fourgram = NGramsTree(4)\n",
    "%time fourgram.fit(sentences[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff probability 0%:\n",
      "Kurslitteratur inom IT √§r en lurig grej .\n",
      ": S , hmm isf har de bara f√•tt mig att m√• bra .\n",
      "<CENSURERAT> fastslog <CENSURERAT> som sagt i studion och fotade <CENSURERAT> och <CENSURERAT> .\n",
      "_Karolina Skande 8 Kommentarer\n",
      "Med nytt ledarskap √§r det h√∂g tid f√∂r nytt ledarskap .\n",
      "\n",
      "Backoff probability 5.0%:\n",
      "Kurslitteratur inom IT √§r en lurig grej .\n",
      ": S , hmm isf har de bara f√•tt mig att m√• bra .\n",
      "<CENSURERAT> fastslog <CENSURERAT> som sagt i studion och fotade <CENSURERAT> och <CENSURERAT> .\n",
      "Men det √§r f√∂r m√•nga och det slutade med en liten ridbana eller n√•got s√• g√∂r det ! \"\n",
      "OBS ! !\n",
      "\n",
      "Backoff probability 10.0%:\n",
      "Kurslitteratur inom IT √§r en lurig grej .\n",
      ": S , hmm isf har de bara f√•tt mig n√•n s√∂mn s√• jag l√§r ju √§ndra det Aja , <CENSURERAT> ska h√§r bloggas f√∂r alla mina 500 facebookv√§nner som ocks√• haft sina tal √§n och vill inte splittras fr√•n min √§lskade <CENSURERAT> och k√§kade tacos !\n",
      "Den h√§r , eller rent ut sagt , inte blev v√§rre .\n",
      "Eller fer sure , l√•ter coolare .\n",
      "Jag tror Malm√∂s backar √§r i starkt behov av en bild :o <CENSURERAT> har jag spenderat <CENSURERAT> att ringa √§r min v√§n <CENSURERAT> blogginl√§gg om helgen ocks√• .\n",
      "\n",
      "Backoff probability 15.000000000000002%:\n",
      "Kurslitteratur inom IT √§r en lurig grej : <CENSURERAT> f√∂ljer mig p√• instagram ( littlemisshaidi ) kanske s√•g att jag hade ingen kraft i norsk press vid tillf√§llet .\n",
      "Det √§r tur att det √§r skamligt att vi \" gl√∂mmer \" <CENSURERAT> s√• √§r det <CENSURERAT> att vi st√•tt vid min sida ( som inte √§r s√• m√§rkv√§rdiga det jag vill jag s√•klart h√•lla den s√• tills den laddat klart , helt sjukt gott .\n",
      "<CENSURERAT> ska jag √• Brat ge oss av en <CENSURERAT> EOS 350D , inte alls .\n",
      "Bloggen √§r snygg och hemtrevlig .\n",
      "Ja .\n",
      "\n",
      "Backoff probability 20.0%:\n",
      "Kurslitteratur inom IT √§r en lurig grej : <CENSURERAT> f√∂ljer mig p√• instagram ( littlemisshaidi ) kanske s√•g att jag hade <CENSURERAT> , ni ska bara ha de bra .\n",
      "Det √§r tur att det √§r skamligt att vi \" gl√∂mmer \" <CENSURERAT> s√• √§r det <CENSURERAT> att vi st√•tt vid min sida ( som inte √§r s√• m√§rkv√§rdiga det jag har l√§st enormt mycket om vad andra tycker om mig , gjorde frukost och pappa ska inte shoppa , s√• kan ni kolla in den h√§r s√∂tnosen fr√•n Kooba : Tumblr\n",
      "Bloggen √§r snygg och hemtrevlig .\n",
      "Ja .\n",
      "HEEEJ f√∂rl√•t gud .\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "print(\"Backoff probability {}%:\".format(p * 100))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))    \n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))\n",
    "random.seed(0)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fourgram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "fivegram = NGramsTree(5)\n",
    "%time fivegram.fit(sentences[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff probability 0%:\n",
      "<CENSURERAT> ringde min klocka vid tio och jag h√∂ll p√• att sm√§lla av .... fick kramp i magen typ .. <CENSURERAT> h√§mtade oss och vi √•kte hem till mig och satt bredvid varandra med varnsin dator och var allm√§nt n√∂rdiga .\n",
      "B√§st att forts√§tta plugga <CENSURERAT> , fika ing√•r s√•klart !\n",
      "Jag beh√∂ver nog inte oroa mig f√∂r att de inte √§r allt ‚Ä¶ Men attans s√• mycket enklare mitt liv skulle bli !\n",
      "Nej , avskaffa monarkin .\n",
      "Vill du anm√§la dig till dagens blogg s√• g√∂r du det H√Ñ√Ñ√ÑR .\n",
      "\n",
      "Backoff probability 5.0%:\n",
      "<CENSURERAT> ringde min klocka vid tio och jag h√∂ll p√• att somna rakt upp och ned kan jag f√• rysningar .\n",
      "√Ñntligen platt mage !\n",
      "Nej jag vet inte men antar att jag f√•r skylla mig sj√§lv , jag beh√∂ver aldrig g√∂ra mig till och l√•tsas vara n√•gon annan f√∂r att du ska n√• ditt m√• l .\n",
      "<CENSURERAT> <CENSURERAT> har jag mejlat \" aff√§ren \" och fr√•gat om jag f√•r l√§gga undan , inte f√∂r att utbildas utan f√∂r att de p√• ena intervjun s√§ger en l√∂n och p√• den andra intervjun s√§ger en annan , BETYDLIGT l√§gre l√∂n .\n",
      "N A J S .\n",
      "\n",
      "Backoff probability 10.0%:\n",
      "<CENSURERAT> ringde min klocka och min ascoola v√•g , wiho !\n",
      "det funkar verkligen !\n",
      "En mer nylig <CENSURERAT> pixar film , som jag gillar √§r att man kan sova gott om natten .\n",
      "Jag saknar stallet och h√§starna\n",
      "B√§st att forts√§tta plugga <CENSURERAT> , fika ing√•r s√•klart !\n",
      "\n",
      "Backoff probability 15.000000000000002%:\n",
      "<CENSURERAT> ringde min klocka och min ascoola v√•g , wiho !\n",
      "det funkar verkligen !\n",
      "En mer nylig <CENSURERAT> pixar film , som jag gillar √§r modig .\n",
      "Spana under de d√§r timmarna p√• tr√§b√§nken , f√∂rutom n√§r jag fick rusa till toaletten f√∂r jag √§tit f√∂r mycket onyttigt och f√∂r lite designer innan jag l√§gger mig ner och skriva en lista p√• vad som komma skall under den riktiga karnevalen som g√•r av .\n",
      "Har inte l√§st dem f√∂rut , men tycker att det k√§nns b√§ttre nu ! ! !\n",
      "\n",
      "Backoff probability 20.0%:\n",
      "<CENSURERAT> ringde min klocka och min ascoola v√•g , wiho !\n",
      "det funkar verkligen !\n",
      "En mer nylig <CENSURERAT> pixar film , som jag gillar √§r modig .\n",
      "Spana under de d√§r timmarna p√• tr√§b√§nken , f√∂rutom n√§r jag fick rusa till toaletten f√∂r jag √§tit f√∂r mycket onyttigt och f√∂r lite designer innan jag g√•r till jobbet och sen s√• blir det matl√•da <CENSURERAT> n√§r vi var och cyklade p√• <CENSURERAT> 5 kommentarer\n",
      "Vill ha n√•got litet fint & betydelsefullt .\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "print(\"Backoff probability {}%:\".format(p * 100))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "    \n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))    \n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))\n",
    "\n",
    "p+= .05\n",
    "print(\"\\nBackoff probability {}%:\".format(p * 100))\n",
    "random.seed(1)\n",
    "for i in range(5):\n",
    "    print_model_sentence(fivegram.generate_sentence(sentence = ['<BOS>'], backoff_prob=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cool, the sentences have a nice flow but often derail. More so in the case of 4-grams than 5-grams, which still often look to be stolen straight from the blogs.\n",
    "\n",
    "To tackle this I will add some kind of probability of choosing words present in a shorter context. Ideally this should be done with a proper backoff model, but as the day of the big \"Skitsnack\" is drawign closer I will probably just implement some quasi rational solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
