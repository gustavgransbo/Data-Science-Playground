{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I am reading up on Recurrent Neural Networks (RNN) and their ability to handle sequential inout and outputs.\n",
    "[This article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) starts of with a small toy problem that I want to implement. \n",
    "It teaches a small RNN to generate text character by character.\n",
    "Characters are one-hot encoded, with a vocabulary limited to `helo` and uses only a single training example: `hello`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "I would like to extend the problem to model three different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['hello', 'bench', 'aloha']\n",
    "vocab = list(set(reduce(lambda x, y: x+y, words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'bench', 'aloha']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['l', 'o', 'b', 'a', 'h', 'c', 'n', 'e'], 8)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary of my toy problem is twice the size, and I am training on words with some ammount of overlap.\n",
    "I expect `bench` to be the easiest word to learn, and possibly `hello` and `aloha` a bit harder as they overlap more.\n",
    "\n",
    "I keep one impartant restriction, all examples are 5 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_encoding = {}\n",
    "for i, c in enumerate(vocab):\n",
    "    char_encoding[c] = np.zeros(len(vocab))\n",
    "    char_encoding[c][i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
       " 'b': array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'c': array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]),\n",
       " 'e': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
       " 'h': array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
       " 'l': array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'n': array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
       " 'o': array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_char(char):\n",
    "    assert(char in vocab), \"{} not in vocabulary\".format(char)\n",
    "    return char_encoding[char]\n",
    "def decode_char(arr):\n",
    "    assert(len(arr) == len(vocab)), \"Array of shape {} does not match vocab of length {}\".format(arr.shape, len(vocab))\n",
    "    return vocab[np.argmax(arr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "As input I use the first four characters of each word, and as targets I use the four last characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[encode_char(c) for c in word[:-1]] for word in words])\n",
    "y = np.array([[encode_char(c) for c in word[1:]] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 8)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 8)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Model, Input, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=100, input_shape=(4, len(vocab)), return_sequences=True))\n",
    "model.add(Dense(units=8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit(X[0].reshape(1, 4, 8), y[0].reshape(1, 4, 8), epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2271a2aa6a0>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell -> ello\n",
      "benc -> ench\n",
      "aloh -> loha\n"
     ]
    }
   ],
   "source": [
    "for word, pred in zip(words, predictions):\n",
    "    print(word[:-1], \"->\",  \"\".join(list(map(decode_char, pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, 100% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a super simpel RNN to predict the next character in a sequence of 4 characters, in a very small vocabulary of 8 words. I got perfect accuracy on my three simpel trianing examples.\n",
    "\n",
    "My RNN is very limited:\n",
    "* It cannot handle variable length input\n",
    "* It takes four characters of a five character word as input, so the only real contribution of the network is predicting the last character."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
