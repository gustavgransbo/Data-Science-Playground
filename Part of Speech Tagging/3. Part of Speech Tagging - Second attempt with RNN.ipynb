{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In `2. Part of Speech Tagging - LSTM` I attempted to use a RNN for Part of Speech (POS) tagging based on word embeddings created with Google's word2vec pretrained on their news data set, and some extra features.\n",
    "\n",
    "In this notebook I will have another go at it with some more experience with RNNs. Also, this time I will be sure to ignore padded data when calculating loss functions by using the `sample_weight` parameter of `fit`. I think not suing sample weight might have severly harmed my previous model.\n",
    "\n",
    "This time I will include a frozen embedding layer in my model instead of pre processing the data into word vectors.\n",
    "I'll try word embeddings trained by [Glove](https://nlp.stanford.edu/projects/glove/) this time. Glove  which is Stanfords embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "I will be using the same training data for my tagger as in `1. Part of Speech Tagging - First Atempt`:\n",
    "[Universal Dependencies - English Web Treebank](http://universaldependencies.org/treebanks/en_ewt/index.html), a CoNLL-U formart corpus with 254 830 words and 16 622 sentences in english *taken from various web media including weblogs, newsgroups, emails, reviews, and Yahoo! answers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "First lets load the training data and convert it to a python dictionary and a pandas data frame.\n",
    "I use the [conllu](https://github.com/EmilStenstrom/conllu) python package to parse the CoNLL-U files to dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-train.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-dev.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    dev_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-test.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    test_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dict = conllu.parse(train_text)\n",
    "dev_dict = conllu.parse(dev_text)\n",
    "test_dict = conllu.parse(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count sentences and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 12543 sentences and 204607 tokens\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "n_train_sentences = len(train_dict)\n",
    "n_train_tokens = reduce(lambda x, y: x + len(y), train_dict, 0)\n",
    "\n",
    "print(\"The training set contains {} sentences and {} tokens\".format(n_train_sentences, n_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[token['form'] for token in sentence] for sentence in train_dict]\n",
    "train_labels = [[token['upostag'] for token in sentence] for sentence in train_dict]\n",
    "\n",
    "dev_sentences = [[token['form'] for token in sentence] for sentence in dev_dict]\n",
    "dev_labels = [[token['upostag'] for token in sentence] for sentence in dev_dict]\n",
    "\n",
    "test_sentences = [[token['form'] for token in sentence] for sentence in test_dict]\n",
    "test_labels = [[token['upostag'] for token in sentence] for sentence in test_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags = list(set(reduce(lambda x, y: x + y, train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels = len(pos_tags)\n",
    "n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_idx = dict(zip(pos_tags, np.arange(len(pos_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_encoding = {}\n",
    "for pos, i in pos_idx.items():\n",
    "    pos_encoding[pos] = np.zeros(len(pos_tags))\n",
    "    pos_encoding[pos][i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "[Wang et al.](https://arxiv.org/pdf/1510.06168.pdf) showed that a bidirectional LSTM network could achieve state of the art performance without using any morphological features, they only used these features:\n",
    "* Word embedding of the word (cast to lower case)\n",
    "* Suffix of length two, one-hot encoded\n",
    "* Wether the word is all caps, lower case, or has an initial capital letter. One-hot encoded.\n",
    "\n",
    "I am not using a bidirectional LSTM, but at least I am using a RNN. \n",
    "I'll opt to only use word embeddings for starters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "I will opt to use the 100 dimensional Glove 6B data. My vocabulary will be exactly the words inside the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_path = 'glove/glove.6B/glove.6B.100d.txt'\n",
    "embeddings = {}\n",
    "token_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tok, *vec = line.split()\n",
    "        embeddings[tok] = np.array(vec, dtype='float32')\n",
    "        # Reserve index 0 for unknown words\n",
    "        token_index[tok] = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have any words that are out of vocabulary OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for sentence in train_sentences:\n",
    "    for tok in sentence:\n",
    "        if tok not in embeddings:\n",
    "            oov.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, 31 000 tokens out of 204 000 tokens are OOV. Most likely it's caused by what preprocessing was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Al',\n",
       " 'Zaman',\n",
       " 'American',\n",
       " 'Shaikh',\n",
       " 'Abdullah',\n",
       " 'Ani',\n",
       " 'Qaim',\n",
       " 'Syrian',\n",
       " 'This',\n",
       " 'DPA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Glove is only lowe case, as I should convert my data to lower case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[token.lower() for token in sentence] for sentence in train_sentences]\n",
    "dev_sentences = [[token.lower() for token in sentence] for sentence in dev_sentences]\n",
    "test_sentences = [[token.lower() for token in sentence] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for sentence in train_sentences:\n",
    "    for tok in sentence:\n",
    "        if tok not in embeddings:\n",
    "            oov.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2442"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'akkab\",\n",
       " 'jubur',\n",
       " 'batawi',\n",
       " 'sarhid',\n",
       " 'batawi',\n",
       " 'clientelage',\n",
       " \"47's\",\n",
       " 'fallujan',\n",
       " 'saddamites',\n",
       " 'fallujan',\n",
       " 'sweared',\n",
       " 'unscear',\n",
       " 'conseguences',\n",
       " 'emercom',\n",
       " 'wi940',\n",
       " 'http://www.ibiblio.org/expo/soviet.exhibit/chernobyl.html',\n",
       " 'http://www.ibrae.ac.ru/ibrae/eng/chernobyl/nat_rep/nat_repe.htm#24',\n",
       " 'http://www.nsrl.ttu.edu/chernobyl/wildlifepreserve.htm',\n",
       " 'http://www.environmentalchemistry.com/yogi/hazmat/articles/chernobyl1.html',\n",
       " 'http://digon_va.tripod.com/chernobyl.htm',\n",
       " 'http://www.oneworld.org/index_oc/issue196/byckau.html',\n",
       " 'http://www.collectinghistory.net/chernobyl/',\n",
       " 'http://www.ukrainianweb.com/chernobyl_ukraine.htm',\n",
       " 'http://www.bullatomsci.org/issues/1993/s93/s93marples.html',\n",
       " 'http://www.calguard.ca.gov/ia/chernobyl-15%20years.htm',\n",
       " 'http://www.infoukes.com/history/chornobyl/gregorovich/index.html',\n",
       " 'http://www.un.org/ha/chernobyl/',\n",
       " 'http://www.tecsoc.org/pubs/history/2002/apr26.htm',\n",
       " 'http://www.chernobyl.org.uk/page2.htm',\n",
       " 'http://www.time.com/time/daily/chernobyl/860901.accident.html',\n",
       " 'http://www.infoukes.com/history/chornobyl/elg/',\n",
       " 'http://www.world-nuclear.org/info/chernobyl/inf07.htm',\n",
       " 'http://www.nea.fr/html/rp/chernobyl/conclusions5.html',\n",
       " 'http://www.nea.fr/html/rp/chernobyl/c01.html',\n",
       " 'http://www.nea.fr/html/rp/chernobyl/c05.html',\n",
       " 'http://www.physics.isu.edu/radinf/chern.htm',\n",
       " 'http://www.chernobyl.info/en',\n",
       " 'http://www.arps.org.au/chernobyl.htm',\n",
       " 'http://www-formal.stanford.edu/jmc/progress/chernobyl.html',\n",
       " 'http://www.21stcenturysciencetech.com/articles/chernobyl.html',\n",
       " \"50's\",\n",
       " 'hirsohima',\n",
       " 'nagaski',\n",
       " '.......',\n",
       " 'http://www.adiccp.org/home/default.asp',\n",
       " 'wearies',\n",
       " 'post-saddam',\n",
       " 'www.juancole.com',\n",
       " 'jawaharal',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " 'sithamparanathan',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " '16/11/2004',\n",
       " 'trifurcation',\n",
       " '28/10/2004',\n",
       " 'campenni',\n",
       " 'campenni',\n",
       " '’72',\n",
       " '’73',\n",
       " 'swifties',\n",
       " 'abbudi',\n",
       " 'abbudi',\n",
       " 'emminence',\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"khaza'il\",\n",
       " 'husseiniyas',\n",
       " 'majoos',\n",
       " 'f*ed',\n",
       " 'fallujan',\n",
       " 'inad',\n",
       " \"gu'ud\",\n",
       " 'dulaym',\n",
       " \"gu'ud\",\n",
       " \"gu'ud\",\n",
       " 'madhlum',\n",
       " 'lihabi',\n",
       " 'lihaib',\n",
       " 'lihaibi',\n",
       " 'post-chavez',\n",
       " 'http://www.amazon.ca/exec/obidos/asin/0915765381/701-3377456-8181939',\n",
       " 'cannistaro',\n",
       " 'quaddaffi',\n",
       " 'http://www.disinfo.com/archive/pages/dossier/id334/pg1/',\n",
       " 'http://www.bigeye.com/111003.htm',\n",
       " 'http://www.thekcrachannel.com/news/4503872/detail.html',\n",
       " 'louisianna',\n",
       " '..',\n",
       " 'tagesthemen',\n",
       " 'ã³l',\n",
       " 'yuor',\n",
       " '.........']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot's of Arabic sounding words, urls and misspelled words. Let's see if the gensim data has an url token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'url' in embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's replace all urls with that one. Also, let's check what POS tags are expected for urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Match words starting with www., http:// or https://\n",
    "    if re.match(r'^(?:https{0,1}\\:\\/\\/.*|www\\.*)', word):\n",
    "        return \"url\"\n",
    "    else:\n",
    "        return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[process_word(word) for word in sentence] for sentence in train_sentences]\n",
    "dev_sentences = [[process_word(word) for word in sentence] for sentence in dev_sentences]\n",
    "test_sentences = [[process_word(word) for word in sentence] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for sentence in train_sentences:\n",
    "    for tok in sentence:\n",
    "        if tok not in embeddings:\n",
    "            oov.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2310"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_tags = []\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    for j, tok in enumerate(sentence):\n",
    "        if tok == 'url':\n",
    "            url_tags.append(train_labels[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NOUN', 'PROPN', 'X'],\n",
       "       dtype='<U5'), array([  2,   1, 132], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(url_tags),return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are classified as `X`. I think casting them all to `url` is sound!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to correct the spelling of all misspelled words. One method could be to calculate the [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance) of all OOV words to words inside the vocabulary, and casting them to the closest match (if it is withing some maxmimum distance).\n",
    "\n",
    "[Norvig](http://norvig.com/spell-correct.html) shows how to find all words within 2 Damerau-Levenshtein distance. Neat!\n",
    "THis method is easily extended to find all words within k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find all characters present in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_vocab = set()\n",
    "\n",
    "for word in embeddings:\n",
    "    for c in word:\n",
    "        character_vocab.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(character_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that's a lot of characters. Lets ignore all characters that are not ascii. (I think these characters are the most common in misspellings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only keep characters in the ascii range\n",
    "ascii_vocab = set([c for c in character_vocab if 32 <= ord(c) <= 126])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ascii_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbour_words(word):\n",
    "    # Tuples with all possible splits of word\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # All words generated by deleting one character\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    # All words generated by swapping two characters in word\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    # All words generated by inserting a character in word\n",
    "    insertions = [L + c + R for L, R in splits for c in ascii_vocab]\n",
    "    # All words generated by replacing a character in word\n",
    "    replace = [L + c +R[1:] for L, R, in splits for c in ascii_vocab if R]\n",
    "    \n",
    "    return set(deletes+transposes+insertions+replace)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_spelling(word, vocab, max_distance=1):\n",
    "    candidates = set([word])\n",
    "    \n",
    "    for i in range(max_distance+1):\n",
    "        for candidate in candidates:\n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "        new_candidates = set()\n",
    "        for candidate in candidates:\n",
    "            new_candidates = new_candidates.union(neighbour_words(candidate))\n",
    "        candidates = new_candidates\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, the amount of possibilities explode for longer words and this large vocabulary.\n",
    "I'll only search for matches withing just one edit, and if there are many possibilities I don't care which one.\n",
    "\n",
    "Here is a version of the above function that will just look at distance one, and return the first match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_one_neighbour(word, vocab):\n",
    "    # Tuples with all possible splits of word\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # All words generated by deleting one character\n",
    "    for L, R in splits:\n",
    "        candidate = L + R[1:] if R else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by swapping two characters in word\n",
    "    for L, R in splits:\n",
    "        candidate = L + R[1:] if R else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by swapping two characters in word\n",
    "    for L, R in splits:    \n",
    "        candidate = L + R[1] + R[0] + R[2:] if len(R) > 1 else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by inserting a character in word\n",
    "    for L, R in splits:\n",
    "        for c in ascii_vocab:    \n",
    "            candidate = L + c + R \n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "    # All words generated by replacing a character in word\n",
    "    for L, R, in splits:\n",
    "        for c in ascii_vocab:    \n",
    "            candidate = L + c +R[1:] if R else None\n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "        \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_one_neighbour(\"mispeled\", [\"misspelled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_spelling = [find_one_neighbour(word, embeddings) for word in oov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_spelling = dict(zip(oov, fixed_spelling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'akkab\", None),\n",
       " ('jubur', 'bubur'),\n",
       " ('batawi', 'tatawi'),\n",
       " ('sarhid', 'sahid'),\n",
       " ('clientelage', None),\n",
       " (\"47's\", '47s'),\n",
       " ('fallujan', 'falluja'),\n",
       " ('saddamites', None),\n",
       " ('sweared', 'seared'),\n",
       " ('unscear', None),\n",
       " ('conseguences', 'consequences'),\n",
       " ('emercom', None),\n",
       " ('wi940', None),\n",
       " (\"50's\", '50s'),\n",
       " ('hirsohima', 'hiroshima'),\n",
       " ('nagaski', 'nagasaki'),\n",
       " ('.......', None),\n",
       " ('wearies', 'wearied'),\n",
       " ('post-saddam', None),\n",
       " ('jawaharal', 'jawaharlal')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(new_spelling.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fixed = len(oov) - np.asarray(fixed_spelling, dtype='bool').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1066"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might not always be perfect corrections, but I think this is better than keeping the words as oov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_words(word, fix_dict):\n",
    "    if word in fix_dict:\n",
    "        return fix_dict[word]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_oov(sentences, vocab):\n",
    "    oov = []\n",
    "    for sentence in train_sentences:\n",
    "        for tok in sentence:\n",
    "            if tok not in embeddings:\n",
    "                oov.append(tok)\n",
    "    return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_train = find_oov(train_sentences, embeddings)\n",
    "fixed_spelling_train = [find_one_neighbour(word, embeddings) for word in oov_train]\n",
    "fixed_spelling_train = dict(zip(oov_train, fixed_spelling_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_dev = find_oov(dev_sentences, embeddings)\n",
    "fixed_spelling_dev = [find_one_neighbour(word, embeddings) for word in oov_dev]\n",
    "fixed_spelling_dev = dict(zip(oov_dev, fixed_spelling_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_test = find_oov(test_sentences, embeddings)\n",
    "fixed_spelling_test = [find_one_neighbour(word, embeddings) for word in oov_test]\n",
    "fixed_spelling_test = dict(zip(oov_test, fixed_spelling_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[replace_words(word, fixed_spelling_train) for word in sentence] for sentence in train_sentences]\n",
    "dev_sentences = [[replace_words(word, fixed_spelling_dev) for word in sentence] for sentence in dev_sentences]\n",
    "test_sentences = [[replace_words(word, fixed_spelling_test) for word in sentence] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data\n",
    "I will encode the targets and the tokens as integers.\n",
    "Later I will encode targets with one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "First build a map from pos tag to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_index = {}\n",
    "for i, pos in enumerate(pos_tags):\n",
    "    # Reserve 0 for padded labels\n",
    "    pos_tag_index[pos] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in train_labels])\n",
    "Y_dev = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in dev_labels])\n",
    "Y_test = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "I already built the mapping from token to index.\n",
    "Encode all OOV tokens as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray([np.asarray([token_index[tok] if tok in token_index else 0 for tok in sentence]) for sentence in train_sentences])\n",
    "X_dev = np.asarray([np.asarray([token_index[tok] if tok in token_index else 0 for tok in sentence]) for sentence in dev_sentences])\n",
    "X_test = np.asarray([np.asarray([token_index[tok] if tok in token_index else 0 for tok in sentence]) for sentence in test_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = max([len(sentence) for sentence in train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_sentence_length, padding='post')\n",
    "Y_train = pad_sequences(Y_train, maxlen=max_sentence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = pad_sequences(X_dev, maxlen=max_sentence_length, padding='post')\n",
    "Y_dev = pad_sequences(Y_dev, maxlen=max_sentence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, maxlen=max_sentence_length, padding='post')\n",
    "Y_test = pad_sequences(Y_test, maxlen=max_sentence_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_train = (X_train != 0).reshape(X_train.shape[0], X_train.shape[1])\n",
    "sample_weights_train = sample_weights_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_dev = (X_dev != 0).reshape(X_dev.shape[0], X_dev.shape[1])\n",
    "sample_weights_dev = sample_weights_dev.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_test = (X_test != 0).reshape(X_test.shape[0], X_test.shape[1])\n",
    "sample_weights_test = sample_weights_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    arr = np.zeros(n)\n",
    "    arr[i] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  7, 14,  7,  9,  4, 15, 14, 14, 14,  7, 14,  7,  1,  4,  2,  1,\n",
       "        4,  2,  1,  4,  2, 14,  7,  2,  1,  9,  4,  7,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.asarray([np.asarray([one_hot(i, n_labels+1) for i in sentence]) for sentence in Y_train])\n",
    "\n",
    "Y_dev = np.asarray([np.asarray([one_hot(i, n_labels+1) for i in sentence]) for sentence in Y_dev])\n",
    "\n",
    "Y_test = np.asarray([np.asarray([one_hot(i, n_labels+1) for i in sentence]) for sentence in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12543, 159, 18)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "Create a frozen embedding layer from the Glove data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.array(list(embeddings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words, embedding_dims = embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(n_words,\n",
    "                            embedding_dims,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sentence_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = GRU(latent_dim, return_sequences=True)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12543 samples, validate on 2002 samples\n",
      "Epoch 1/1\n",
      "12543/12543 [==============================] - 333s 27ms/step - loss: 0.9386 - acc: 0.9048 - val_loss: 0.6287 - val_acc: 0.9649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc924c9c50>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=(X_dev, Y_dev, sample_weights_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, after just one epoch the model scores 96% accuracy on the dev set. Seems almost to good to be true.\n",
    "I will have to verify this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One small note first. I tried to train my model without a BatchNormalization layer, and it was not able to make progress at all. Goes to show how important normalised data is!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
