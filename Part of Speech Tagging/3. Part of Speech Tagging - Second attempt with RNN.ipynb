{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In `2. Part of Speech Tagging - LSTM` I attempted to use a RNN for Part of Speech (POS) tagging based on word embeddings created with Google's word2vec pretrained on their news data set, and some extra features.\n",
    "\n",
    "In this notebook I will have another go at it with some more experience with RNNs. Also, this time I will be sure to ignore padded data when calculating loss functions by using the `sample_weight` parameter of `fit`. I think not suing sample weight might have severly harmed my previous model.\n",
    "\n",
    "This time I will include a frozen embedding layer in my model instead of pre processing the data into word vectors.\n",
    "I'll try word embeddings trained by [Glove](https://nlp.stanford.edu/projects/glove/) this time. Glove  which is Stanfords embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "I will be using the same training data for my tagger as in `1. Part of Speech Tagging - First Atempt`:\n",
    "[Universal Dependencies - English Web Treebank](http://universaldependencies.org/treebanks/en_ewt/index.html), a CoNLL-U formart corpus with 254 830 words and 16 622 sentences in english *taken from various web media including weblogs, newsgroups, emails, reviews, and Yahoo! answers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "First lets load the training data and convert it to a python dictionary and a pandas data frame.\n",
    "I use the [conllu](https://github.com/EmilStenstrom/conllu) python package to parse the CoNLL-U files to dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-train.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-dev.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    dev_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-test.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    test_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dict = conllu.parse(train_text)\n",
    "dev_dict = conllu.parse(dev_text)\n",
    "test_dict = conllu.parse(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count sentences and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 12543 sentences and 204607 tokens\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "n_train_sentences = len(train_dict)\n",
    "n_train_tokens = reduce(lambda x, y: x + len(y), train_dict, 0)\n",
    "\n",
    "print(\"The training set contains {} sentences and {} tokens\".format(n_train_sentences, n_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[token['form'] for token in sentence] for sentence in train_dict]\n",
    "train_labels = [[token['upostag'] for token in sentence] for sentence in train_dict]\n",
    "\n",
    "dev_sentences = [[token['form'] for token in sentence] for sentence in dev_dict]\n",
    "dev_labels = [[token['upostag'] for token in sentence] for sentence in dev_dict]\n",
    "\n",
    "test_sentences = [[token['form'] for token in sentence] for sentence in test_dict]\n",
    "test_labels = [[token['upostag'] for token in sentence] for sentence in test_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags = list(set(reduce(lambda x, y: x + y, train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels = len(pos_tags)\n",
    "n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_idx = dict(zip(pos_tags, np.arange(len(pos_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_encoding = {}\n",
    "for pos, i in pos_idx.items():\n",
    "    pos_encoding[pos] = np.zeros(len(pos_tags))\n",
    "    pos_encoding[pos][i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "[Wang et al.](https://arxiv.org/pdf/1510.06168.pdf) showed that a bidirectional LSTM network could achieve state of the art performance without using any morphological features, they only used these features:\n",
    "* Word embedding of the word (cast to lower case)\n",
    "* Suffix of length two, one-hot encoded\n",
    "* Wether the word is all caps, lower case, or has an initial capital letter. One-hot encoded.\n",
    "\n",
    "I am not using a bidirectional LSTM, but at least I am using a RNN. \n",
    "I'll opt to only use word embeddings for starters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "I will opt to use the 100 dimensional Glove 6B data. My vocabulary will be exactly the words inside the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_path = 'glove/glove.6B/glove.6B.100d.txt'\n",
    "embeddings = {}\n",
    "token_index = {}\n",
    "index_token = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tok, *vec = line.split()\n",
    "        embeddings[tok] = np.array(vec, dtype='float32')\n",
    "        # Reserve index 0 for padding\n",
    "        token_index[tok] = i + 1\n",
    "        index_token[i+1] = tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have any words that are out of vocabulary OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for sentence in train_sentences:\n",
    "    for tok in sentence:\n",
    "        if tok not in embeddings:\n",
    "            oov.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, 31 000 tokens out of 204 000 tokens are OOV. Most likely it's caused by what preprocessing was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Al',\n",
       " 'Zaman',\n",
       " 'American',\n",
       " 'Shaikh',\n",
       " 'Abdullah',\n",
       " 'Ani',\n",
       " 'Qaim',\n",
       " 'Syrian',\n",
       " 'This',\n",
       " 'DPA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Glove is only lower case, I should convert my data to lower case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[token.lower() for token in sentence] for sentence in train_sentences]\n",
    "dev_sentences = [[token.lower() for token in sentence] for sentence in dev_sentences]\n",
    "test_sentences = [[token.lower() for token in sentence] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for sentence in train_sentences:\n",
    "    for tok in sentence:\n",
    "        if tok not in embeddings:\n",
    "            oov.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2442"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down to 2 442 OOV tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'akkab\",\n",
       " 'jubur',\n",
       " 'batawi',\n",
       " 'sarhid',\n",
       " 'batawi',\n",
       " 'clientelage',\n",
       " \"47's\",\n",
       " 'fallujan',\n",
       " 'saddamites',\n",
       " 'fallujan',\n",
       " 'sweared',\n",
       " 'unscear',\n",
       " 'conseguences',\n",
       " 'emercom',\n",
       " 'wi940',\n",
       " 'http://www.ibiblio.org/expo/soviet.exhibit/chernobyl.html',\n",
       " 'http://www.ibrae.ac.ru/ibrae/eng/chernobyl/nat_rep/nat_repe.htm#24',\n",
       " 'http://www.nsrl.ttu.edu/chernobyl/wildlifepreserve.htm',\n",
       " 'http://www.environmentalchemistry.com/yogi/hazmat/articles/chernobyl1.html',\n",
       " 'http://digon_va.tripod.com/chernobyl.htm',\n",
       " 'http://www.oneworld.org/index_oc/issue196/byckau.html',\n",
       " 'http://www.collectinghistory.net/chernobyl/',\n",
       " 'http://www.ukrainianweb.com/chernobyl_ukraine.htm',\n",
       " 'http://www.bullatomsci.org/issues/1993/s93/s93marples.html',\n",
       " 'http://www.calguard.ca.gov/ia/chernobyl-15%20years.htm',\n",
       " 'http://www.infoukes.com/history/chornobyl/gregorovich/index.html',\n",
       " 'http://www.un.org/ha/chernobyl/',\n",
       " 'http://www.tecsoc.org/pubs/history/2002/apr26.htm',\n",
       " 'http://www.chernobyl.org.uk/page2.htm',\n",
       " 'http://www.time.com/time/daily/chernobyl/860901.accident.html',\n",
       " 'http://www.infoukes.com/history/chornobyl/elg/',\n",
       " 'http://www.world-nuclear.org/info/chernobyl/inf07.htm',\n",
       " 'http://www.nea.fr/html/rp/chernobyl/conclusions5.html',\n",
       " 'http://www.nea.fr/html/rp/chernobyl/c01.html',\n",
       " 'http://www.nea.fr/html/rp/chernobyl/c05.html',\n",
       " 'http://www.physics.isu.edu/radinf/chern.htm',\n",
       " 'http://www.chernobyl.info/en',\n",
       " 'http://www.arps.org.au/chernobyl.htm',\n",
       " 'http://www-formal.stanford.edu/jmc/progress/chernobyl.html',\n",
       " 'http://www.21stcenturysciencetech.com/articles/chernobyl.html',\n",
       " \"50's\",\n",
       " 'hirsohima',\n",
       " 'nagaski',\n",
       " '.......',\n",
       " 'http://www.adiccp.org/home/default.asp',\n",
       " 'wearies',\n",
       " 'post-saddam',\n",
       " 'www.juancole.com',\n",
       " 'jawaharal',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " 'sithamparanathan',\n",
       " 'kalkat',\n",
       " 'kalkat',\n",
       " '16/11/2004',\n",
       " 'trifurcation',\n",
       " '28/10/2004',\n",
       " 'campenni',\n",
       " 'campenni',\n",
       " '’72',\n",
       " '’73',\n",
       " 'swifties',\n",
       " 'abbudi',\n",
       " 'abbudi',\n",
       " 'emminence',\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"sha'lan\",\n",
       " \"khaza'il\",\n",
       " 'husseiniyas',\n",
       " 'majoos',\n",
       " 'f*ed',\n",
       " 'fallujan',\n",
       " 'inad',\n",
       " \"gu'ud\",\n",
       " 'dulaym',\n",
       " \"gu'ud\",\n",
       " \"gu'ud\",\n",
       " 'madhlum',\n",
       " 'lihabi',\n",
       " 'lihaib',\n",
       " 'lihaibi',\n",
       " 'post-chavez',\n",
       " 'http://www.amazon.ca/exec/obidos/asin/0915765381/701-3377456-8181939',\n",
       " 'cannistaro',\n",
       " 'quaddaffi',\n",
       " 'http://www.disinfo.com/archive/pages/dossier/id334/pg1/',\n",
       " 'http://www.bigeye.com/111003.htm',\n",
       " 'http://www.thekcrachannel.com/news/4503872/detail.html',\n",
       " 'louisianna',\n",
       " '..',\n",
       " 'tagesthemen',\n",
       " 'ã³l',\n",
       " 'yuor',\n",
       " '.........']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot's of Arabic sounding words, urls and misspelled words. Let's see if the gensim data has an url token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'url' in embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's replace all urls with that one. Also, let's check what POS tags are expected for urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Match words starting with www., http:// or https://\n",
    "    if re.match(r'^(?:https{0,1}\\:\\/\\/.*|www\\.*)', word):\n",
    "        return \"url\"\n",
    "    else:\n",
    "        return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[process_word(word) for word in sentence] for sentence in train_sentences]\n",
    "dev_sentences = [[process_word(word) for word in sentence] for sentence in dev_sentences]\n",
    "test_sentences = [[process_word(word) for word in sentence] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for sentence in train_sentences:\n",
    "    for tok in sentence:\n",
    "        if tok not in embeddings:\n",
    "            oov.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2310"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_tags = []\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    for j, tok in enumerate(sentence):\n",
    "        if tok == 'url':\n",
    "            url_tags.append(train_labels[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NOUN', 'PROPN', 'X'],\n",
       "       dtype='<U5'), array([  2,   1, 132], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(url_tags),return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most are classified as `X`. I think casting them all to `url` is sound!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to correct the spelling of all misspelled words. One method could be to calculate the [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance) of all OOV words to words inside the vocabulary, and casting them to the closest match (if it is withing some maxmimum distance).\n",
    "\n",
    "[Norvig](http://norvig.com/spell-correct.html) shows how to find all words within 2 Damerau-Levenshtein distance. Neat!\n",
    "THis method is easily extended to find all words within k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find all characters present in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_vocab = set()\n",
    "\n",
    "for word in embeddings:\n",
    "    for c in word:\n",
    "        character_vocab.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(character_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that's a lot of characters. Lets ignore all characters that are not ascii. (I think these characters are the most common in misspellings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only keep characters in the ascii range\n",
    "ascii_vocab = set([c for c in character_vocab if 32 <= ord(c) <= 126])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ascii_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbour_words(word):\n",
    "    # Tuples with all possible splits of word\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # All words generated by deleting one character\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    # All words generated by swapping two characters in word\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    # All words generated by inserting a character in word\n",
    "    insertions = [L + c + R for L, R in splits for c in ascii_vocab]\n",
    "    # All words generated by replacing a character in word\n",
    "    replace = [L + c +R[1:] for L, R, in splits for c in ascii_vocab if R]\n",
    "    \n",
    "    return set(deletes+transposes+insertions+replace)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_spelling(word, vocab, max_distance=1):\n",
    "    candidates = set([word])\n",
    "    \n",
    "    for i in range(max_distance+1):\n",
    "        for candidate in candidates:\n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "        new_candidates = set()\n",
    "        for candidate in candidates:\n",
    "            new_candidates = new_candidates.union(neighbour_words(candidate))\n",
    "        candidates = new_candidates\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, the amount of possibilities explode for longer words and this large vocabulary.\n",
    "I'll only search for matches withing just one edit, and if there are many possibilities I don't care which one.\n",
    "\n",
    "Here is a version of the above function that will just look at distance one, and return the first match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_one_neighbour(word, vocab):\n",
    "    # Tuples with all possible splits of word\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # All words generated by deleting one character\n",
    "    for L, R in splits:\n",
    "        candidate = L + R[1:] if R else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by swapping two characters in word\n",
    "    for L, R in splits:\n",
    "        candidate = L + R[1:] if R else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by swapping two characters in word\n",
    "    for L, R in splits:    \n",
    "        candidate = L + R[1] + R[0] + R[2:] if len(R) > 1 else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by inserting a character in word\n",
    "    for L, R in splits:\n",
    "        for c in ascii_vocab:    \n",
    "            candidate = L + c + R \n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "    # All words generated by replacing a character in word\n",
    "    for L, R, in splits:\n",
    "        for c in ascii_vocab:    \n",
    "            candidate = L + c +R[1:] if R else None\n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "        \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_one_neighbour(\"mispeled\", [\"misspelled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_spelling = [find_one_neighbour(word, embeddings) for word in oov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_spelling = dict(zip(oov, fixed_spelling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'akkab\", None),\n",
       " ('jubur', 'subur'),\n",
       " ('batawi', 'tatawi'),\n",
       " ('sarhid', 'sahid'),\n",
       " ('clientelage', None),\n",
       " (\"47's\", '47s'),\n",
       " ('fallujan', 'falluja'),\n",
       " ('saddamites', None),\n",
       " ('sweared', 'seared'),\n",
       " ('unscear', None),\n",
       " ('conseguences', 'consequences'),\n",
       " ('emercom', None),\n",
       " ('wi940', None),\n",
       " (\"50's\", '50s'),\n",
       " ('hirsohima', 'hiroshima'),\n",
       " ('nagaski', 'nagasaki'),\n",
       " ('.......', None),\n",
       " ('wearies', 'wearier'),\n",
       " ('post-saddam', None),\n",
       " ('jawaharal', 'jawaharlal')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(new_spelling.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fixed = len(oov) - np.asarray(fixed_spelling, dtype='bool').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1066"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might not always be perfect corrections, but I think this is better than keeping the words as oov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_words(word, fix_dict):\n",
    "    if word in fix_dict:\n",
    "        return fix_dict[word]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_oov(sentences, vocab):\n",
    "    oov = []\n",
    "    for sentence in train_sentences:\n",
    "        for tok in sentence:\n",
    "            if tok not in embeddings:\n",
    "                oov.append(tok)\n",
    "    return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_train = find_oov(train_sentences, embeddings)\n",
    "fixed_spelling_train = [find_one_neighbour(word, embeddings) for word in oov_train]\n",
    "fixed_spelling_train = dict(zip(oov_train, fixed_spelling_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_dev = find_oov(dev_sentences, embeddings)\n",
    "fixed_spelling_dev = [find_one_neighbour(word, embeddings) for word in oov_dev]\n",
    "fixed_spelling_dev = dict(zip(oov_dev, fixed_spelling_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_test = find_oov(test_sentences, embeddings)\n",
    "fixed_spelling_test = [find_one_neighbour(word, embeddings) for word in oov_test]\n",
    "fixed_spelling_test = dict(zip(oov_test, fixed_spelling_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[replace_words(word, fixed_spelling_train) for word in sentence] for sentence in train_sentences]\n",
    "dev_sentences = [[replace_words(word, fixed_spelling_dev) for word in sentence] for sentence in dev_sentences]\n",
    "test_sentences = [[replace_words(word, fixed_spelling_test) for word in sentence] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data\n",
    "I will encode the targets and the tokens as integers.\n",
    "Later I will encode targets with one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "First build a map from pos tag to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tag_index = {}\n",
    "for i, pos in enumerate(pos_tags):\n",
    "    # Reserve 0 for padded labels\n",
    "    pos_tag_index[pos] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in train_labels])\n",
    "Y_dev = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in dev_labels])\n",
    "Y_test = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "I already built the mapping from token to index.\n",
    "Encode all OOV tokens as \"unk\". See https://stackoverflow.com/questions/49239941/what-is-unk-in-glove-6b-50d-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray([np.asarray([token_index[tok if tok in token_index else \"unk\"] for tok in sentence]) for sentence in train_sentences])\n",
    "X_dev = np.asarray([np.asarray([token_index[tok if tok in token_index else \"unk\"]  for tok in sentence]) for sentence in dev_sentences])\n",
    "X_test = np.asarray([np.asarray([token_index[tok if tok in token_index else \"unk\"] for tok in sentence]) for sentence in test_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = max([len(sentence) for sentence in train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_sentence_length, padding='post')\n",
    "Y_train = pad_sequences(Y_train, maxlen=max_sentence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = pad_sequences(X_dev, maxlen=max_sentence_length, padding='post')\n",
    "Y_dev = pad_sequences(Y_dev, maxlen=max_sentence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, maxlen=max_sentence_length, padding='post')\n",
    "Y_test = pad_sequences(Y_test, maxlen=max_sentence_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set sample weights\n",
    "We should not considered training instances with word index 0 when calculating our loss function, as these are padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_train = (X_train != 0).reshape(X_train.shape[0], X_train.shape[1])\n",
    "sample_weights_train = sample_weights_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_dev = (X_dev != 0).reshape(X_dev.shape[0], X_dev.shape[1])\n",
    "sample_weights_dev = sample_weights_dev.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_test = (X_test != 0).reshape(X_test.shape[0], X_test.shape[1])\n",
    "sample_weights_test = sample_weights_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(i, n):\n",
    "    arr = np.zeros(n)\n",
    "    arr[i] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 12, 10, 12,  3,  4, 13, 10, 10, 10, 12, 10, 12,  5,  4, 16,  5,\n",
       "        4, 16,  5,  4, 16, 10, 12, 16,  5,  3,  4, 12,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.asarray([np.asarray([one_hot(i, n_labels+1) for i in sentence]) for sentence in Y_train])\n",
    "\n",
    "Y_dev = np.asarray([np.asarray([one_hot(i, n_labels+1) for i in sentence]) for sentence in Y_dev])\n",
    "\n",
    "Y_test = np.asarray([np.asarray([one_hot(i, n_labels+1) for i in sentence]) for sentence in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12543, 159, 18)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "Create a frozen embedding layer from the Glove data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 100\n",
    "\n",
    "pad = np.zeros(embedding_dims).reshape(1, embedding_dims)\n",
    "\n",
    "embedding_matrix = np.concatenate([pad, np.array(list(embeddings.values()))])\n",
    "\n",
    "n_words = len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(n_words,\n",
    "                            embedding_dims,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sentence_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = GRU(latent_dim, return_sequences=True)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the weighted_metrics argument instead of metrics because I don't want to include padding when calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal', weighted_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12543 samples, validate on 2002 samples\n",
      "Epoch 1/5\n",
      "12543/12543 [==============================] - 330s 26ms/step - loss: 0.6036 - weighted_acc: 0.8208 - val_loss: 0.3917 - val_weighted_acc: 0.8839\n",
      "Epoch 2/5\n",
      "12543/12543 [==============================] - 338s 27ms/step - loss: 0.2848 - weighted_acc: 0.9125 - val_loss: 0.3273 - val_weighted_acc: 0.8984\n",
      "Epoch 3/5\n",
      "12543/12543 [==============================] - 336s 27ms/step - loss: 0.2234 - weighted_acc: 0.9301 - val_loss: 0.3063 - val_weighted_acc: 0.9052\n",
      "Epoch 4/5\n",
      "12543/12543 [==============================] - 340s 27ms/step - loss: 0.1873 - weighted_acc: 0.9409 - val_loss: 0.3065 - val_weighted_acc: 0.9067\n",
      "Epoch 5/5\n",
      "12543/12543 [==============================] - 359s 29ms/step - loss: 0.1592 - weighted_acc: 0.9497 - val_loss: 0.3021 - val_weighted_acc: 0.9091\n"
     ]
    }
   ],
   "source": [
    "callback = model.fit(X_train, Y_train,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=(X_dev, Y_dev, sample_weights_dev),\n",
    "          callbacks = [History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(callback.history)\n",
    "history_df.index = np.arange(1, history_df.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_weighted_acc</th>\n",
       "      <th>weighted_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.603642</td>\n",
       "      <td>0.391734</td>\n",
       "      <td>0.883905</td>\n",
       "      <td>0.820780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.284763</td>\n",
       "      <td>0.327256</td>\n",
       "      <td>0.898351</td>\n",
       "      <td>0.912548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.223450</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.905157</td>\n",
       "      <td>0.930096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187292</td>\n",
       "      <td>0.306545</td>\n",
       "      <td>0.906701</td>\n",
       "      <td>0.940925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.159196</td>\n",
       "      <td>0.302063</td>\n",
       "      <td>0.909146</td>\n",
       "      <td>0.949655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss  val_weighted_acc  weighted_acc\n",
       "1  0.603642  0.391734          0.883905      0.820780\n",
       "2  0.284763  0.327256          0.898351      0.912548\n",
       "3  0.223450  0.306300          0.905157      0.930096\n",
       "4  0.187292  0.306545          0.906701      0.940925\n",
       "5  0.159196  0.302063          0.909146      0.949655"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a139686eb8>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7oAAAIjCAYAAAAtGmdSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLWIwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a139671940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "history_df[['val_weighted_acc', 'weighted_acc']].plot(ax=ax)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_title('Tagging Accuracy on the training and validation sets during training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy almost reaches 91% in 5 epochs.\n",
    "My previous best model was a DNN that reached 93% validation accuracy, maybe I can surpas this with enough training.\n",
    "\n",
    "Unfortunately training accuracy is improving much quicker than validation accuracy, meaning the model is showing signs of overfitting.\n",
    "Let's regularize it with Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('keras_models/gru1_5_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Two, Dropout\n",
    "Let's add dropout to the layers of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GRU(latent_dim, return_sequences=True)(x)\n",
    "x = Dropout(rate=.5)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model2 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the weighted_metrics argument instead of metrics because I don't want to include padding when calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal', weighted_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12543 samples, validate on 2002 samples\n",
      "Epoch 1/5\n",
      "12543/12543 [==============================] - 291s 23ms/step - loss: 1.2682 - weighted_acc: 0.6028 - val_loss: 0.6200 - val_weighted_acc: 0.8071\n",
      "Epoch 2/5\n",
      "12543/12543 [==============================] - 326s 26ms/step - loss: 0.8398 - weighted_acc: 0.7259 - val_loss: 0.5331 - val_weighted_acc: 0.8357\n",
      "Epoch 3/5\n",
      "12543/12543 [==============================] - 352s 28ms/step - loss: 0.7481 - weighted_acc: 0.7544 - val_loss: 0.4978 - val_weighted_acc: 0.8453\n",
      "Epoch 4/5\n",
      "12543/12543 [==============================] - 359s 29ms/step - loss: 0.6981 - weighted_acc: 0.7707 - val_loss: 0.4697 - val_weighted_acc: 0.8547\n",
      "Epoch 5/5\n",
      "12543/12543 [==============================] - 363s 29ms/step - loss: 0.6623 - weighted_acc: 0.7827 - val_loss: 0.4498 - val_weighted_acc: 0.8625\n"
     ]
    }
   ],
   "source": [
    "callback = model2.fit(X_train, Y_train,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=(X_dev, Y_dev, sample_weights_dev),\n",
    "          callbacks = [History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(callback.history)\n",
    "history_df.index = np.arange(1, history_df.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_weighted_acc</th>\n",
       "      <th>weighted_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.268206</td>\n",
       "      <td>0.619999</td>\n",
       "      <td>0.807134</td>\n",
       "      <td>0.602827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.839784</td>\n",
       "      <td>0.533110</td>\n",
       "      <td>0.835683</td>\n",
       "      <td>0.725871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.748131</td>\n",
       "      <td>0.497780</td>\n",
       "      <td>0.845260</td>\n",
       "      <td>0.754424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.698109</td>\n",
       "      <td>0.469708</td>\n",
       "      <td>0.854695</td>\n",
       "      <td>0.770725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.662290</td>\n",
       "      <td>0.449781</td>\n",
       "      <td>0.862550</td>\n",
       "      <td>0.782689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss  val_weighted_acc  weighted_acc\n",
       "1  1.268206  0.619999          0.807134      0.602827\n",
       "2  0.839784  0.533110          0.835683      0.725871\n",
       "3  0.748131  0.497780          0.845260      0.754424\n",
       "4  0.698109  0.469708          0.854695      0.770725\n",
       "5  0.662290  0.449781          0.862550      0.782689"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a1859d5e10>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7oAAAIjCAYAAAAtGmdSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a185ca32e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "history_df[['val_weighted_acc', 'weighted_acc']].plot(ax=ax)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_title('Tagging Accuracy on the training and validation sets during training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here training accuracy is evaluated with dropout applied, leading to it always being subpar to the validation accuracy which is evaluated with the full RNN.\n",
    "\n",
    "Validation accuracy is improving significantly with each step, though it starts much lower compared to the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543/12543 [==============================] - 133s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38856157256087021, 0.87552848792119831]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_train, Y_train, sample_weight=sample_weights_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy after 5 epochs is a little bit higher than the validation accuracy 87.5% vs. 86.2%.\n",
    "\n",
    "Anyway, let's give the model another couple of epochs and see where it gets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_path = 'keras_models/gru2_best.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(best_model_path, monitor='val_weighted_acc', verbose=0, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12543 samples, validate on 2002 samples\n",
      "Epoch 1/30\n",
      "12543/12543 [==============================] - 351s 28ms/step - loss: 0.5859 - weighted_acc: 0.8066 - val_loss: 0.4084 - val_weighted_acc: 0.8737\n",
      "Epoch 2/30\n",
      "12543/12543 [==============================] - 360s 29ms/step - loss: 0.5739 - weighted_acc: 0.8110 - val_loss: 0.4095 - val_weighted_acc: 0.8742\n",
      "Epoch 3/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.5659 - weighted_acc: 0.8135 - val_loss: 0.4021 - val_weighted_acc: 0.8788\n",
      "Epoch 4/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.5614 - weighted_acc: 0.8143 - val_loss: 0.3955 - val_weighted_acc: 0.8769\n",
      "Epoch 5/30\n",
      "12543/12543 [==============================] - 359s 29ms/step - loss: 0.5562 - weighted_acc: 0.8160 - val_loss: 0.3926 - val_weighted_acc: 0.8771\n",
      "Epoch 6/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.5504 - weighted_acc: 0.8183 - val_loss: 0.3921 - val_weighted_acc: 0.8794\n",
      "Epoch 7/30\n",
      "12543/12543 [==============================] - 360s 29ms/step - loss: 0.5452 - weighted_acc: 0.8195 - val_loss: 0.3879 - val_weighted_acc: 0.8816\n",
      "Epoch 8/30\n",
      "12543/12543 [==============================] - 363s 29ms/step - loss: 0.5430 - weighted_acc: 0.8201 - val_loss: 0.3933 - val_weighted_acc: 0.8795\n",
      "Epoch 9/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.5377 - weighted_acc: 0.8221 - val_loss: 0.3860 - val_weighted_acc: 0.8817\n",
      "Epoch 10/30\n",
      "12543/12543 [==============================] - 359s 29ms/step - loss: 0.5302 - weighted_acc: 0.8251 - val_loss: 0.3818 - val_weighted_acc: 0.8830\n",
      "Epoch 11/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.5306 - weighted_acc: 0.8243 - val_loss: 0.3837 - val_weighted_acc: 0.8823\n",
      "Epoch 12/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.5246 - weighted_acc: 0.8267 - val_loss: 0.3770 - val_weighted_acc: 0.8836\n",
      "Epoch 13/30\n",
      "12543/12543 [==============================] - 360s 29ms/step - loss: 0.5205 - weighted_acc: 0.8272 - val_loss: 0.3768 - val_weighted_acc: 0.8829\n",
      "Epoch 14/30\n",
      "12543/12543 [==============================] - 360s 29ms/step - loss: 0.5176 - weighted_acc: 0.8283 - val_loss: 0.3756 - val_weighted_acc: 0.8829\n",
      "Epoch 15/30\n",
      "12543/12543 [==============================] - 359s 29ms/step - loss: 0.5144 - weighted_acc: 0.8298 - val_loss: 0.3779 - val_weighted_acc: 0.8838\n",
      "Epoch 16/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.5093 - weighted_acc: 0.8313 - val_loss: 0.3767 - val_weighted_acc: 0.8847\n",
      "Epoch 17/30\n",
      "12543/12543 [==============================] - 360s 29ms/step - loss: 0.5077 - weighted_acc: 0.8326 - val_loss: 0.3742 - val_weighted_acc: 0.8832\n",
      "Epoch 18/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.5053 - weighted_acc: 0.8336 - val_loss: 0.3725 - val_weighted_acc: 0.8851\n",
      "Epoch 19/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.5026 - weighted_acc: 0.8345 - val_loss: 0.3752 - val_weighted_acc: 0.8859\n",
      "Epoch 20/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.4990 - weighted_acc: 0.8349 - val_loss: 0.3702 - val_weighted_acc: 0.8868\n",
      "Epoch 21/30\n",
      "12543/12543 [==============================] - 360s 29ms/step - loss: 0.4985 - weighted_acc: 0.8350 - val_loss: 0.3672 - val_weighted_acc: 0.8881\n",
      "Epoch 22/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.4924 - weighted_acc: 0.8370 - val_loss: 0.3708 - val_weighted_acc: 0.8863\n",
      "Epoch 23/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.4887 - weighted_acc: 0.8385 - val_loss: 0.3691 - val_weighted_acc: 0.8881\n",
      "Epoch 24/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.4902 - weighted_acc: 0.8375 - val_loss: 0.3673 - val_weighted_acc: 0.8868\n",
      "Epoch 25/30\n",
      "12543/12543 [==============================] - 365s 29ms/step - loss: 0.4873 - weighted_acc: 0.8393 - val_loss: 0.3630 - val_weighted_acc: 0.8889\n",
      "Epoch 26/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.4854 - weighted_acc: 0.8391 - val_loss: 0.3640 - val_weighted_acc: 0.8878\n",
      "Epoch 27/30\n",
      "12543/12543 [==============================] - 362s 29ms/step - loss: 0.4849 - weighted_acc: 0.8401 - val_loss: 0.3670 - val_weighted_acc: 0.8864\n",
      "Epoch 28/30\n",
      "12543/12543 [==============================] - 363s 29ms/step - loss: 0.4810 - weighted_acc: 0.8409 - val_loss: 0.3661 - val_weighted_acc: 0.8890\n",
      "Epoch 29/30\n",
      "12543/12543 [==============================] - 364s 29ms/step - loss: 0.4787 - weighted_acc: 0.8421 - val_loss: 0.3684 - val_weighted_acc: 0.8888\n",
      "Epoch 30/30\n",
      "12543/12543 [==============================] - 361s 29ms/step - loss: 0.4797 - weighted_acc: 0.8410 - val_loss: 0.3625 - val_weighted_acc: 0.8892\n"
     ]
    }
   ],
   "source": [
    "callback2 = model2.fit(X_train, Y_train,\n",
    "          batch_size=64,\n",
    "          epochs=30,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=(X_dev, Y_dev, sample_weights_dev),\n",
    "          callbacks = [History(), checkpoint, EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=5, verbose=0, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543/12543 [==============================] - 111s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25727052977474207, 0.91790999171456911]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_train, Y_train, sample_weight=sample_weights_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, the model is slowly improving and training accuracy is not that far ahead of validation accuracy. I feel like the model is learning way to slowsly though...\n",
    "\n",
    "Let's try a little less dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Three\n",
    "I will keep 50% dropout for the embeddings to not overfit specific features, but I will decrease the dropout of the GRU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GRU(latent_dim, return_sequences=True)(x)\n",
    "x = Dropout(rate=.2)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model3 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the weighted_metrics argument instead of metrics because I don't want to include padding when calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal', weighted_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12543 samples, validate on 2002 samples\n",
      "Epoch 1/5\n",
      "12543/12543 [==============================] - 357s 28ms/step - loss: 1.0899 - weighted_acc: 0.6553 - val_loss: 0.5668 - val_weighted_acc: 0.8278\n",
      "Epoch 2/5\n",
      "12543/12543 [==============================] - 367s 29ms/step - loss: 0.7164 - weighted_acc: 0.7663 - val_loss: 0.4823 - val_weighted_acc: 0.8529\n",
      "Epoch 3/5\n",
      "12543/12543 [==============================] - 383s 31ms/step - loss: 0.6302 - weighted_acc: 0.7942 - val_loss: 0.4442 - val_weighted_acc: 0.8632\n",
      "Epoch 4/5\n",
      "12543/12543 [==============================] - 367s 29ms/step - loss: 0.5806 - weighted_acc: 0.8082 - val_loss: 0.4240 - val_weighted_acc: 0.8702\n",
      "Epoch 5/5\n",
      "12543/12543 [==============================] - 394s 31ms/step - loss: 0.5543 - weighted_acc: 0.8172 - val_loss: 0.4051 - val_weighted_acc: 0.8777\n"
     ]
    }
   ],
   "source": [
    "callback = model3.fit(X_train, Y_train,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=(X_dev, Y_dev, sample_weights_dev),\n",
    "          callbacks = [History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543/12543 [==============================] - 164s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3338926083674929, 0.89495196775282659]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_train, Y_train, sample_weight=sample_weights_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is learning a little bit faster. But training time is still very long for little results. Let's see if I can improve training time by removing unecessary padding in the batches. There is no use in padding all sequences in one batch to be longer than the actual longest sequence in that batch.\n",
    "\n",
    "I will sort the sentences by length and apply padding per batch of 64. \n",
    "Maybe sorting the sentences has unknown effects on training, but let's ignore that possibility for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in train_labels])\n",
    "Y_dev = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in dev_labels])\n",
    "Y_test = np.asarray([np.asarray([pos_tag_index[pos] for pos in sentence]) for sentence in test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray([np.asarray([token_index[tok if tok in token_index else \"unk\"] for tok in sentence]) for sentence in train_sentences])\n",
    "X_dev = np.asarray([np.asarray([token_index[tok if tok in token_index else \"unk\"]  for tok in sentence]) for sentence in dev_sentences])\n",
    "X_test = np.asarray([np.asarray([token_index[tok if tok in token_index else \"unk\"] for tok in sentence]) for sentence in test_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_idx_train = np.argsort(list(map(lambda x: len(x), X_train)))\n",
    "sorted_idx_dev = np.argsort(list(map(lambda x: len(x), X_dev)))\n",
    "sorted_idx_test = np.argsort(list(map(lambda x: len(x), X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[sorted_idx_train]\n",
    "Y_train = Y_train[sorted_idx_train]\n",
    "\n",
    "X_dev = X_dev[sorted_idx_dev]\n",
    "Y_dev = Y_dev[sorted_idx_dev]\n",
    "\n",
    "X_test = X_test[sorted_idx_test]\n",
    "Y_test = Y_test[sorted_idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "class ShortPaddingGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, return_sample_weights=True):\n",
    "        self.return_sample_weights = return_sample_weights\n",
    "        self.batch_size = batch_size\n",
    "        self.x, self.y, self.sample_weights = [], [], []\n",
    "        # Pad Data so all batches have sequences of equal length\n",
    "        for idx in range(int(np.ceil(len(x_set) / float(self.batch_size)))):\n",
    "            batch_x = x_set[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_y = y_set[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            \n",
    "            # The input is sorted so the last sequence is the longest\n",
    "            max_len = len(batch_x[-1])\n",
    "            batch_x = np.array(pad_sequences(batch_x, maxlen=max_len, padding='post'))\n",
    "            batch_y = pad_sequences(batch_y, maxlen=max_len, padding='post')\n",
    "            \n",
    "            # One-hot encode labels\n",
    "            batch_y = np.array([np.array([one_hot(i, n_labels+1) for i in sentence]) for sentence in batch_y])\n",
    "            \n",
    "            self.x.append(batch_x)\n",
    "            self.y.append(batch_y)\n",
    "            self.sample_weights.append(batch_x != 0)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the batches\n",
    "        if self.return_sample_weights:\n",
    "            return self.x[idx], self.y[idx], self.sample_weights[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def set_return_sample_weights(self, flag):\n",
    "        self.return_sample_weights = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = ShortPaddingGenerator(X_train, Y_train, batch_size=64)\n",
    "dev_gen = ShortPaddingGenerator(X_dev, Y_dev, batch_size=64)\n",
    "test_gen = ShortPaddingGenerator(X_test, Y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in  range(len(train_gen)):\n",
    "    x, _, _ = train_gen.__getitem__(i)\n",
    "    lengths.append(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.897959183673468"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, mean sequence length of the training data is now 17 instead of 159."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Three Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First re-create the embedding with unspecified input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(n_words,\n",
    "                            embedding_dims,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GRU(latent_dim, return_sequences=True)(x)\n",
    "x = Dropout(rate=.2)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model3 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the weighted_metrics argument instead of metrics because I don't want to include padding when calculating accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal', weighted_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 36s 184ms/step - loss: 1.3100 - weighted_acc: 0.6079 - val_loss: 0.6966 - val_weighted_acc: 0.7989\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 34s 172ms/step - loss: 0.8791 - weighted_acc: 0.7251 - val_loss: 0.5906 - val_weighted_acc: 0.8251\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 36s 183ms/step - loss: 0.7708 - weighted_acc: 0.7548 - val_loss: 0.5390 - val_weighted_acc: 0.8366\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 37s 186ms/step - loss: 0.6952 - weighted_acc: 0.7763 - val_loss: 0.5019 - val_weighted_acc: 0.8435\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 36s 186ms/step - loss: 0.6452 - weighted_acc: 0.7911 - val_loss: 0.4707 - val_weighted_acc: 0.8571\n"
     ]
    }
   ],
   "source": [
    "callback = model3.fit_generator(train_gen,\n",
    "          epochs=5,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38717058553542094, 0.87904721677640707]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10x training speed! Slightly worse results though, but I can compensate with more epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 34s 172ms/step - loss: 0.6077 - weighted_acc: 0.8041 - val_loss: 0.4565 - val_weighted_acc: 0.8619\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 38s 196ms/step - loss: 0.5833 - weighted_acc: 0.8120 - val_loss: 0.4486 - val_weighted_acc: 0.8610\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 38s 196ms/step - loss: 0.5605 - weighted_acc: 0.8186 - val_loss: 0.4444 - val_weighted_acc: 0.8625\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 39s 200ms/step - loss: 0.5493 - weighted_acc: 0.8216 - val_loss: 0.4259 - val_weighted_acc: 0.8688\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 41s 209ms/step - loss: 0.5356 - weighted_acc: 0.8274 - val_loss: 0.4435 - val_weighted_acc: 0.8664\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 42s 214ms/step - loss: 0.5252 - weighted_acc: 0.8286 - val_loss: 0.4176 - val_weighted_acc: 0.8709\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 43s 217ms/step - loss: 0.5133 - weighted_acc: 0.8327 - val_loss: 0.4051 - val_weighted_acc: 0.8792\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 44s 225ms/step - loss: 0.5084 - weighted_acc: 0.8337 - val_loss: 0.4123 - val_weighted_acc: 0.8703\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 46s 236ms/step - loss: 0.4961 - weighted_acc: 0.8378 - val_loss: 0.3990 - val_weighted_acc: 0.8793\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 48s 246ms/step - loss: 0.4949 - weighted_acc: 0.8396 - val_loss: 0.4029 - val_weighted_acc: 0.8806\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 47s 239ms/step - loss: 0.4860 - weighted_acc: 0.8397 - val_loss: 0.3976 - val_weighted_acc: 0.8825\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4818 - weighted_acc: 0.8410 - val_loss: 0.4005 - val_weighted_acc: 0.8826\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4814 - weighted_acc: 0.8419 - val_loss: 0.4011 - val_weighted_acc: 0.8749\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 45s 232ms/step - loss: 0.4733 - weighted_acc: 0.8442 - val_loss: 0.3907 - val_weighted_acc: 0.8816\n",
      "Epoch 15/50\n",
      "196/196 [==============================] - 46s 232ms/step - loss: 0.4679 - weighted_acc: 0.8471 - val_loss: 0.3931 - val_weighted_acc: 0.8801\n",
      "Epoch 16/50\n",
      "196/196 [==============================] - 46s 234ms/step - loss: 0.4677 - weighted_acc: 0.8467 - val_loss: 0.3848 - val_weighted_acc: 0.8804\n",
      "Epoch 17/50\n",
      "196/196 [==============================] - 48s 244ms/step - loss: 0.4630 - weighted_acc: 0.8478 - val_loss: 0.3865 - val_weighted_acc: 0.8805\n"
     ]
    }
   ],
   "source": [
    "callback2 = model3.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=5, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25376246424655752, 0.91815803803335405]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, the model quickly stops improving. Let's try even lower dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mistakes = 0\n",
    "for i in  range(len(dev_gen)):\n",
    "    x, y, sw = dev_gen.__getitem__(i)\n",
    "    mistakes += ((model3.predict(x).argmax(2) != y.argmax(2)) & sw).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = np.sum(list(map( lambda x: (x != 0).sum(), X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88333996023856853"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(instances -mistakes) / instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Four\n",
    "Reduce dropout to 20% for the embedding layer as well. Also, use masking in the embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(n_words,\n",
    "                            embedding_dims,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False,\n",
    "                           mask_zero = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GRU(latent_dim, return_sequences=True)(x)\n",
    "x = Dropout(rate=.2)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model4 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I use mask_zero in the Embedding layer I no longer need weighted_metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 58s 295ms/step - loss: 1.0665 - acc: 0.6957 - val_loss: 0.5904 - val_acc: 0.8306\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 48s 245ms/step - loss: 0.6073 - acc: 0.8157 - val_loss: 0.4774 - val_acc: 0.8582\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 49s 249ms/step - loss: 0.5025 - acc: 0.8474 - val_loss: 0.4361 - val_acc: 0.8656\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 48s 243ms/step - loss: 0.4355 - acc: 0.8652 - val_loss: 0.3907 - val_acc: 0.8792\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 49s 248ms/step - loss: 0.3903 - acc: 0.8793 - val_loss: 0.3840 - val_acc: 0.8827\n"
     ]
    }
   ],
   "source": [
    "callback = model4.fit_generator(train_gen,\n",
    "          epochs=5,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.41647787628831207, 0.88269489509361487]"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate_generator(dev_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pretty quickly climbs to 88% dev set accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 62s 317ms/step - loss: 0.2345 - acc: 0.9246 - val_loss: 0.3848 - val_acc: 0.8966\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 68s 346ms/step - loss: 0.2303 - acc: 0.9253 - val_loss: 0.3849 - val_acc: 0.8943\n"
     ]
    }
   ],
   "source": [
    "callback2 = model4.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=1, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13354741472771395, 0.95754417615255027]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It quickly stagnates around 89-90% validation accuracy though, with a traing accuracy that keeps increasing. Overfitting!\n",
    "\n",
    "So, our model with higher dropout ended up underfitting the data and this model overfits. Let's try to change something else, lets go for LSTM cells instead of GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LSTM(latent_dim, return_sequences=True)(x)\n",
    "x = Dropout(rate=.2)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model5 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we use `mask_zero` we no longer need sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen.set_return_sample_weights(False)\n",
    "dev_gen.set_return_sample_weights(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 84s 431ms/step - loss: 1.4165 - acc: 0.5899 - val_loss: 0.7189 - val_acc: 0.7993\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 82s 419ms/step - loss: 0.8519 - acc: 0.7343 - val_loss: 0.5680 - val_acc: 0.8321\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 86s 438ms/step - loss: 0.7164 - acc: 0.7739 - val_loss: 0.5066 - val_acc: 0.8476\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 87s 441ms/step - loss: 0.6337 - acc: 0.7968 - val_loss: 0.4542 - val_acc: 0.8610\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 93s 474ms/step - loss: 0.5852 - acc: 0.8113 - val_loss: 0.4308 - val_acc: 0.8648\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 84s 427ms/step - loss: 0.5496 - acc: 0.8222 - val_loss: 0.4131 - val_acc: 0.8705\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 81s 415ms/step - loss: 0.5232 - acc: 0.8305 - val_loss: 0.3962 - val_acc: 0.8723\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 82s 419ms/step - loss: 0.5023 - acc: 0.8377 - val_loss: 0.3824 - val_acc: 0.8783\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 82s 416ms/step - loss: 0.4886 - acc: 0.8402 - val_loss: 0.3819 - val_acc: 0.8805\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 92s 472ms/step - loss: 0.4777 - acc: 0.8436 - val_loss: 0.3739 - val_acc: 0.8848\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 80s 409ms/step - loss: 0.4612 - acc: 0.8495 - val_loss: 0.3803 - val_acc: 0.8840\n"
     ]
    }
   ],
   "source": [
    "callback = model5.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=1, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2731159959722913, 0.91332545114040642]"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also quickly stops improving, and never reaches past 88.5% dev set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Six, Bidirectional LSTM\n",
    "Let's try the BLSTM architecture used by [Wang et al., 2015](https://arxiv.org/pdf/1510.06168.pdf), originally from [Graves, 2002](https://www.cs.toronto.edu/~graves/preprint.pdf):\n",
    "Two LSTM layers with 93 hidden units, one doing a forward pass of the input and the other a backward pass. \n",
    "\n",
    "Graves used no regularization except early stopping, input noise and weight noise.\n",
    "I will also use some dropout and batchnormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Bidirectional(LSTM(93, return_sequences=True), merge_mode='concat')(x)\n",
    "x = Dropout(rate=.2)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model6 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 55s 281ms/step - loss: 1.5010 - acc: 0.5629 - val_loss: 0.6992 - val_acc: 0.7947\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 44s 225ms/step - loss: 0.8124 - acc: 0.7489 - val_loss: 0.5509 - val_acc: 0.8316\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 43s 222ms/step - loss: 0.6797 - acc: 0.7865 - val_loss: 0.4835 - val_acc: 0.8496\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6059 - acc: 0.8072 - val_loss: 0.4379 - val_acc: 0.8634\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 45s 229ms/step - loss: 0.5586 - acc: 0.8209 - val_loss: 0.4205 - val_acc: 0.8679\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.5157 - acc: 0.8360 - val_loss: 0.3989 - val_acc: 0.8738\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.4956 - acc: 0.8406 - val_loss: 0.3863 - val_acc: 0.8794\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 45s 230ms/step - loss: 0.4750 - acc: 0.8458 - val_loss: 0.3673 - val_acc: 0.8861\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.4555 - acc: 0.8525 - val_loss: 0.3595 - val_acc: 0.8891\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 44s 225ms/step - loss: 0.4417 - acc: 0.8564 - val_loss: 0.3666 - val_acc: 0.8840\n"
     ]
    }
   ],
   "source": [
    "callback = model6.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=1, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25492338224251115, 0.91910281682579009]"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 32s 165ms/step - loss: 0.4289 - acc: 0.8602 - val_loss: 0.3469 - val_acc: 0.8894\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 31s 161ms/step - loss: 0.4233 - acc: 0.8629 - val_loss: 0.3436 - val_acc: 0.8911\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 31s 160ms/step - loss: 0.4132 - acc: 0.8650 - val_loss: 0.3405 - val_acc: 0.8954\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 31s 159ms/step - loss: 0.4067 - acc: 0.8666 - val_loss: 0.3353 - val_acc: 0.8966\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 32s 163ms/step - loss: 0.3994 - acc: 0.8695 - val_loss: 0.3325 - val_acc: 0.8988\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.3925 - acc: 0.8721 - val_loss: 0.3330 - val_acc: 0.8943\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 31s 161ms/step - loss: 0.3871 - acc: 0.8726 - val_loss: 0.3289 - val_acc: 0.8964\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 32s 161ms/step - loss: 0.3817 - acc: 0.8747 - val_loss: 0.3219 - val_acc: 0.8972\n"
     ]
    }
   ],
   "source": [
    "callback = model6.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=3, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2039173866305769, 0.93562637685180272]"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still overfitting... Anyway, let's see if we can push validation accuracy a bit higher with more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 31s 160ms/step - loss: 0.3780 - acc: 0.8764 - val_loss: 0.3227 - val_acc: 0.9035\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 31s 160ms/step - loss: 0.3718 - acc: 0.8794 - val_loss: 0.3276 - val_acc: 0.8971\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.3709 - acc: 0.8783 - val_loss: 0.3240 - val_acc: 0.8987\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 36s 184ms/step - loss: 0.3621 - acc: 0.8817 - val_loss: 0.3237 - val_acc: 0.8992\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 37s 189ms/step - loss: 0.3633 - acc: 0.8795 - val_loss: 0.3153 - val_acc: 0.9021\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 39s 201ms/step - loss: 0.3583 - acc: 0.8831 - val_loss: 0.3186 - val_acc: 0.9005\n"
     ]
    }
   ],
   "source": [
    "callback = model6.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=5, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18992041873131643, 0.94025394896670966]"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy is now lagging by 4% units compared to the training accuracy. I either need more regularization or fewer  hidden nodes. I think truning up dropout is a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Seven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None,))\n",
    "x = embedding_layer(model_input)\n",
    "x = Dropout(rate=.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Bidirectional(LSTM(93, return_sequences=True), merge_mode='concat')(x)\n",
    "x = Dropout(rate=.5)(x)\n",
    "model_output = Dense(n_labels + 1, activation='softmax')(x)\n",
    "model7 = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 53s 269ms/step - loss: 1.6010 - acc: 0.5255 - val_loss: 0.7375 - val_acc: 0.7786\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 45s 232ms/step - loss: 0.9241 - acc: 0.7133 - val_loss: 0.5936 - val_acc: 0.8189\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 47s 240ms/step - loss: 0.7833 - acc: 0.7544 - val_loss: 0.5194 - val_acc: 0.8393\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 46s 235ms/step - loss: 0.7140 - acc: 0.7747 - val_loss: 0.4878 - val_acc: 0.8501\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 45s 230ms/step - loss: 0.6636 - acc: 0.7891 - val_loss: 0.4547 - val_acc: 0.8567\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 47s 240ms/step - loss: 0.6296 - acc: 0.7998 - val_loss: 0.4414 - val_acc: 0.8628\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 48s 245ms/step - loss: 0.5997 - acc: 0.8071 - val_loss: 0.4232 - val_acc: 0.8679\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 47s 242ms/step - loss: 0.5815 - acc: 0.8134 - val_loss: 0.4069 - val_acc: 0.8715\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.5659 - acc: 0.8171 - val_loss: 0.4011 - val_acc: 0.8756\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 45s 227ms/step - loss: 0.5500 - acc: 0.8214 - val_loss: 0.3876 - val_acc: 0.8803\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 46s 234ms/step - loss: 0.5421 - acc: 0.8256 - val_loss: 0.3804 - val_acc: 0.8825\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.5296 - acc: 0.8271 - val_loss: 0.3714 - val_acc: 0.8865\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.5226 - acc: 0.8302 - val_loss: 0.3704 - val_acc: 0.8881\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.5138 - acc: 0.8329 - val_loss: 0.3717 - val_acc: 0.8878\n"
     ]
    }
   ],
   "source": [
    "callback = model7.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=1, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27408962487404609, 0.91330931672903182]"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy is about 1.3% units ahead. Let's give the model some more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "196/196 [==============================] - 45s 227ms/step - loss: 0.5094 - acc: 0.8341 - val_loss: 0.3602 - val_acc: 0.8903\n",
      "Epoch 2/50\n",
      "196/196 [==============================] - 46s 236ms/step - loss: 0.5015 - acc: 0.8371 - val_loss: 0.3611 - val_acc: 0.8907\n",
      "Epoch 3/50\n",
      "196/196 [==============================] - 45s 232ms/step - loss: 0.4953 - acc: 0.8386 - val_loss: 0.3560 - val_acc: 0.8949\n",
      "Epoch 4/50\n",
      "196/196 [==============================] - 46s 235ms/step - loss: 0.4895 - acc: 0.8408 - val_loss: 0.3517 - val_acc: 0.8928\n",
      "Epoch 5/50\n",
      "196/196 [==============================] - 46s 232ms/step - loss: 0.4855 - acc: 0.8413 - val_loss: 0.3582 - val_acc: 0.8927\n",
      "Epoch 6/50\n",
      "196/196 [==============================] - 45s 232ms/step - loss: 0.4808 - acc: 0.8426 - val_loss: 0.3455 - val_acc: 0.8958\n",
      "Epoch 7/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4756 - acc: 0.8448 - val_loss: 0.3425 - val_acc: 0.8960\n",
      "Epoch 8/50\n",
      "196/196 [==============================] - 46s 235ms/step - loss: 0.4746 - acc: 0.8453 - val_loss: 0.3424 - val_acc: 0.8962\n",
      "Epoch 9/50\n",
      "196/196 [==============================] - 46s 232ms/step - loss: 0.4703 - acc: 0.8464 - val_loss: 0.3366 - val_acc: 0.8963\n",
      "Epoch 10/50\n",
      "196/196 [==============================] - 48s 243ms/step - loss: 0.4656 - acc: 0.8475 - val_loss: 0.3426 - val_acc: 0.8951\n",
      "Epoch 11/50\n",
      "196/196 [==============================] - 49s 249ms/step - loss: 0.4635 - acc: 0.8472 - val_loss: 0.3428 - val_acc: 0.8957\n",
      "Epoch 12/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4630 - acc: 0.8506 - val_loss: 0.3440 - val_acc: 0.8924\n",
      "Epoch 13/50\n",
      "196/196 [==============================] - 47s 240ms/step - loss: 0.4581 - acc: 0.8507 - val_loss: 0.3376 - val_acc: 0.8970\n",
      "Epoch 14/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4593 - acc: 0.8496 - val_loss: 0.3404 - val_acc: 0.8928\n",
      "Epoch 15/50\n",
      "196/196 [==============================] - 45s 230ms/step - loss: 0.4544 - acc: 0.8498 - val_loss: 0.3312 - val_acc: 0.8984\n",
      "Epoch 16/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4526 - acc: 0.8523 - val_loss: 0.3343 - val_acc: 0.8996\n",
      "Epoch 17/50\n",
      "196/196 [==============================] - 47s 241ms/step - loss: 0.4493 - acc: 0.8524 - val_loss: 0.3323 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      "196/196 [==============================] - 46s 234ms/step - loss: 0.4503 - acc: 0.8530 - val_loss: 0.3292 - val_acc: 0.8959\n",
      "Epoch 19/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4475 - acc: 0.8551 - val_loss: 0.3251 - val_acc: 0.9016\n",
      "Epoch 20/50\n",
      "196/196 [==============================] - 45s 230ms/step - loss: 0.4451 - acc: 0.8532 - val_loss: 0.3302 - val_acc: 0.8979\n",
      "Epoch 21/50\n",
      "196/196 [==============================] - 46s 232ms/step - loss: 0.4410 - acc: 0.8549 - val_loss: 0.3287 - val_acc: 0.9002\n",
      "Epoch 22/50\n",
      "196/196 [==============================] - 44s 227ms/step - loss: 0.4415 - acc: 0.8562 - val_loss: 0.3237 - val_acc: 0.9007\n",
      "Epoch 23/50\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.4415 - acc: 0.8564 - val_loss: 0.3288 - val_acc: 0.8952\n",
      "Epoch 24/50\n",
      "196/196 [==============================] - 46s 237ms/step - loss: 0.4357 - acc: 0.8567 - val_loss: 0.3329 - val_acc: 0.8968\n"
     ]
    }
   ],
   "source": [
    "callback = model7.fit_generator(train_gen,\n",
    "          epochs=50,\n",
    "          validation_data=dev_gen,\n",
    "          callbacks = [History(), EarlyStopping(monitor='val_acc', min_delta=0, patience=5, mode='auto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21661268590569241, 0.93231694311804059]"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.evaluate_generator(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.save('keras_models/bstlm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves 90% dev set accuracy and 93% training accuracy. So it's not overfitting that badly! Maybe it could be allowed some more complexity and some more training time.\n",
    "\n",
    "Regardless, I have now tried many similar RNN approaches without surpassing my previous best model which achieved 93% dev set accuracy. My previous model was a DNN with somewhat different features:\n",
    "* It used word2vec embeddings of the two adjacent words and the word it self\n",
    "* It used an appriori probabilit estimate for each POS tag based on the training set\n",
    "\n",
    "The RNN should arguably be better of than the DNN when it comes to information about its surroundings. It has access to all previous words in the case of the ordinary RNN and all other words in case of the bidirectional RNN. I don't know how switching word2vec for Glove affected the model though.\n",
    "\n",
    "Missing out on the appriori estimates might actually be a big deal though.\n",
    "\n",
    "Well, let's evaulate the model and see what it gets right and what it struggles with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Let's have a look at what tokens we are getting wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's verify that our predictions actually get us the accuracy reported by evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "inputs = []\n",
    "batch_size = 64\n",
    "batches = int(np.ceil(len(X_dev)/batch_size))\n",
    "\n",
    "for i in range(batches):\n",
    "    batch_x = X_dev[i*batch_size:(i+1)*batch_size]\n",
    "    batch_y = Y_dev[i*batch_size:(i+1)*batch_size]\n",
    "    batch_x = pad_sequences(batch_x, padding='post')\n",
    "    batch_y = pad_sequences(batch_y, padding='post')\n",
    "    \n",
    "    predictions.append(model7.predict(batch_x).argmax(2).flatten())\n",
    "    labels.append(batch_y.flatten())\n",
    "    inputs.append(batch_x.flatten())\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions)\n",
    "labels = np.concatenate(labels)\n",
    "inputs = np.concatenate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90644135188866803"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions, labels, sample_weight= labels != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(predictions, labels, sample_weight=labels != 0), columns = ['pad']+pos_tags, index=['pad']+pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pad</th>\n",
       "      <th>PRON</th>\n",
       "      <th>X</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>PART</th>\n",
       "      <th>ADV</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>SYM</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AUX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pad</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1503</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>118</td>\n",
       "      <td>3825</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>418</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1864</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>616</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1009</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1232</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3069</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2459</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>316</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pad  PRON   X   ADJ  NOUN   DET  CCONJ  PART   ADV  INTJ  PROPN  SYM  \\\n",
       "pad      0     0   0     0     0     0      0     0     0     0      0    0   \n",
       "PRON     0  2174   1     7     5    20      0     0     9     3      7    0   \n",
       "X        0     0  71     2     9     0      0     0     0     2     54    7   \n",
       "ADJ      0     0   3  1503   120     0      0     0    47     0    100    0   \n",
       "NOUN     0     2  24   118  3825     1      2     0    28     5    418    5   \n",
       "DET      0    12   7    10     5  1864      4     0    24     4      1    2   \n",
       "CCONJ    0     0   0     0     0     3    759     0     0     0      0    1   \n",
       "PART     0     3   0     0     0     0      1   616    14     0      0    0   \n",
       "ADV      0     0   1    53    29     0      5     1  1009     5      1    0   \n",
       "INTJ     0     1   0     3     3     0      0     0     0    74      1    0   \n",
       "PROPN    0     1  18    12    66     1      1     0     5     7   1232    2   \n",
       "SYM      0     0   0     0     2     0      0     0     0     0      0   29   \n",
       "PUNCT    0     3   9     1     9     0      0     2     1     5      3   18   \n",
       "VERB     0     2   2    69    83     1      3     1    22     5     37    0   \n",
       "SCONJ    0    18   0     1     1     3      2     0     8     1      0    0   \n",
       "NUM      0     1  13     3    31     1      1     0     2     0     14    5   \n",
       "ADP      0     0   5     6     6     0      2     7    93     4      7    1   \n",
       "AUX      0     1   1     0     3     0      0     3     4     0      2    0   \n",
       "\n",
       "       PUNCT  VERB  SCONJ  NUM   ADP   AUX  \n",
       "pad        0     0      0    0     0     0  \n",
       "PRON       0     1      7    1     0     0  \n",
       "X          7     0      0    4     2     0  \n",
       "ADJ        0    55      0    0     0     0  \n",
       "NOUN       2    97      2   10     3     0  \n",
       "DET        0     1      1    0     1     0  \n",
       "CCONJ      0     0      0    0     2     0  \n",
       "PART       0     0      5    0    18     4  \n",
       "ADV        0    21      9    0    20     1  \n",
       "INTJ       0     2      0    0     0     0  \n",
       "PROPN      1    13      1    3     2     2  \n",
       "SYM        0     0      0    0     0     0  \n",
       "PUNCT   3069     1      0    0     1     0  \n",
       "VERB       0  2459      3    1     5    17  \n",
       "SCONJ      0     0    316    0    15     0  \n",
       "NUM        4     2      0  359     0     0  \n",
       "ADP        0    10     59    0  1952     0  \n",
       "AUX        0   103      0    0     0  1486  "
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common error is classifying Nouns as Proper Nouns. I wonder what the hardest tokens were? Maybe UNK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unk_idx = np.argwhere(inputs == token_index['unk']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374 unknown tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"{} unknown tokens\".format(len(unk_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54545454545454541"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions[unk_idx], labels[unk_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54% accuracy on the 374 unkown tokens. Much worse than for the global group, but still not that bad! \n",
    "Also, it's definitely not enough to pull the accuracy down to 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I have tried many similar RNN models for POS-tagging, and ended up with 90% validation accuracy on the UD EWT corpus. \n",
    "This is worse compared to my previous best model, but a big step up from by previous attempts with RNN.\n",
    "\n",
    "I experimented with some preprocessing to reduce the number of out of vocabulary words.\n",
    "\n",
    "**What I have learned**\n",
    "* Handling padding in a more effective way during training and evaluation.\n",
    "    * Using sample_weights\n",
    "    * Using mask_zero in the Embedding layer\n",
    "    * Building variable size batches\n",
    "* Using pre trained word embeddings as part of my Keras model, instead of using it as pre-processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
