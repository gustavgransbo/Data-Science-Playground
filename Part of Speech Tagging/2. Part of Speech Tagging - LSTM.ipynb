{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In `1. Part of Speech Tagging - First Atempt` I used a simple DNN to do Part of Speech (POS) tagging based on word embeddings created with Google's word2vec pretrained on their news data set, and some extra features.\n",
    "\n",
    "I this notebook I will atempt a new approach, I will use a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells.\n",
    "\n",
    "Inspiration for this notebook was drawn from [Wang et al., 2015](https://arxiv.org/pdf/1510.06168.pdf).\n",
    "Some noteable differences:\n",
    "* I am not using Bidirectional layers, meaning that words can only influence the words that come after it in the sentence, not those before. \n",
    "* I am not using word embeddings trained by a LSTM network, which could potentially improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "I will be using the same training data for my tagger as in `1. Part of Speech Tagging - First Atempt`:\n",
    "[Universal Dependencies - English Web Treebank](http://universaldependencies.org/treebanks/en_ewt/index.html), a CoNLL-U formart corpus with 254 830 words and 16 622 sentences in english *taken from various web media including weblogs, newsgroups, emails, reviews, and Yahoo! answers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "First lets load the training data and convert it to a python dictionary and a pandas data frame.\n",
    "I use the [conllu](https://github.com/EmilStenstrom/conllu) python package to parse the CoNLL-U files to dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-train.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-dev.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    dev_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-test.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    test_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dict = conllu.parse(train_text)\n",
    "dev_dict = conllu.parse(dev_text)\n",
    "test_dict = conllu.parse(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count sentences and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 12543 sentences and 204607 tokens\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "n_train_sentences = len(train_dict)\n",
    "n_train_tokens = reduce(lambda x, y: x + len(y), train_dict, 0)\n",
    "\n",
    "print(\"The training set contains {} sentences and {} tokens\".format(n_train_sentences, n_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[token['form'] for token in sentence] for sentence in train_dict]\n",
    "train_labels = [[token['upostag'] for token in sentence] for sentence in train_dict]\n",
    "\n",
    "dev_sentences = [[token['form'] for token in sentence] for sentence in dev_dict]\n",
    "dev_labels = [[token['upostag'] for token in sentence] for sentence in dev_dict]\n",
    "\n",
    "test_sentences = [[token['form'] for token in sentence] for sentence in test_dict]\n",
    "test_labels = [[token['upostag'] for token in sentence] for sentence in test_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = list(set(reduce(lambda x, y: x + y, train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = dict(zip(pos_tags, np.arange(len(pos_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = {}\n",
    "for pos, i in pos_idx.items():\n",
    "    pos_encoding[pos] = np.zeros(len(pos_tags))\n",
    "    pos_encoding[pos][i]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Wang et al. showed that a bidirectional LSTM network could achieve state of the art performance without using any morphological features, they only used these features:\n",
    "* Word embedding of the word (cast to lower case)\n",
    "* Suffix of length two, one-hot encoded\n",
    "* Wether the word is all caps, lower case, or has an initial capital letter. One-hot encoded.\n",
    "\n",
    "I will opt to only use word embeddings as features initially. Also, as I will be using Googles pre-trained word2vec model that did not cast words to lower case I see no point in doing this conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Lets start by converting our tokens into word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_w2v = KeyedVectors.load_word2vec_format('word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04110769,  0.03943264, -0.0495911 ,  0.08454352, -0.09290703,\n",
       "        0.08739691, -0.0877254 , -0.07737273, -0.0381479 , -0.04204748])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(-.1, .1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    try:\n",
    "        return news_w2v[word]\n",
    "    except KeyError:\n",
    "        # As per Wang et al. I initialise unkown words with a uniform distribution ranging from -.1 to .1\n",
    "        np.random.seed(0)\n",
    "        return np.random.uniform(-.1, .1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = encode_word(train_sentences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_vec(X):\n",
    "    return np.array([np.array([encode_word(word) for word in sentence]) for sentence in X])\n",
    "def get_label_vec(y):\n",
    "    return np.array([np.array([pos_encoding[tok] for tok in sentence]) for sentence in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_feature_vec(train_sentences)\n",
    "y = get_label_vec(train_labels)\n",
    "\n",
    "X_val = get_feature_vec(dev_sentences)\n",
    "y_val = get_label_vec(dev_labels)\n",
    "\n",
    "X_test = get_feature_vec(test_sentences)\n",
    "y_test = get_label_vec(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels, n_features = len(pos_tags), 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(None, n_features)))\n",
    "    model.add(LSTM(units=100, return_sequences=True))\n",
    "    model.add(Dropout(rate=.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=100, return_sequences=True))\n",
    "    model.add(Dropout(rate=.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=n_labels, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "class GenerateData(Sequence):\n",
    "    def __init__(self, x_set, y_set):\n",
    "        self.x, self.y = x_set, y_set\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx]\n",
    "        batch_y = self.y[idx]\n",
    "        return batch_x.reshape(1, batch_x.shape[0], batch_x.shape[1]), batch_y.reshape(1, batch_y.shape[0], batch_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratePaddedData(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        max_len = max(map(lambda x: x.shape[0], batch_x))\n",
    "        batch_x = np.array(list(map(lambda x: self.pad_input(x, max_len - len(x)), batch_x)))\n",
    "        batch_y = np.array(list(map(lambda x: self.pad_labels(x, max_len - len(x)), batch_y)))\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "        #return batch_x.reshape(1, batch_x.shape[0], batch_x.shape[1]), batch_y.reshape(1, batch_y.shape[0], batch_x.shape[1])\n",
    "    \n",
    "    def pad_input(self, seq, pads):\n",
    "        # Pad with UNK tokens\n",
    "        return np.append(seq, np.zeros(pads * 300).reshape(pads, 300), axis=0)\n",
    "    def pad_labels(self, seq, pads):\n",
    "        return np.append(seq, np.zeros(17*pads).reshape(pads, 17), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 [==============================] - 160s 814ms/step - loss: 0.3259 - acc: 0.3041 - val_loss: 0.6716 - val_acc: 0.7974\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 177s 903ms/step - loss: 0.1689 - acc: 0.3419 - val_loss: 0.6040 - val_acc: 0.8120\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d83e3420b8>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit_generator(generator = GeneratePaddedData(X, y, batch_size=64), epochs=2, validation_data=GenerateData(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12543/12543 [==============================] - 937s 75ms/step - loss: 0.7781 - acc: 0.7694 - val_loss: 0.7932 - val_acc: 0.7896\n",
      "Wall time: 15min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d84ac82e48>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit_generator(generator = GenerateData(X, y), epochs=1, validation_data=GenerateData(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model One Summary\n",
    "So far I have constructed a RNN with LSTM cells and trained it on batches of padded data.\n",
    "Padding the data significantly sped up training compared to traing on batches with single sentences, but probably has negative effects on accuracy. \n",
    "\n",
    "I did not train for many epochs due to long training times, so it's hard to tell if I would get better results with some patience. But it really feels like I need to find a better solution for padding.\n",
    "\n",
    "I will try to mitigate the effects of padding by restircting my model to train on short sentences, no longer than 20 tokens.\n",
    "I realise this is a significant restirction to my model, but it avoids the problem of short sentences being padded to be 100 tokens long. It feels like a good idea to restrict my problem in order to get more experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create short padded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_short_instances(sentences, labels, max_len=20):\n",
    "    \n",
    "    # Find the indexes of all short sentences\n",
    "    short_sentences_mask = np.array([len(sentence) <= max_len for sentence in sentences])\n",
    "    short_sentences_idx = np.where(short_sentences_mask)\n",
    "    \n",
    "    short_sentences = np.array(sentences)[short_sentences_idx]\n",
    "    short_labels = np.array(labels)[short_sentences_idx]\n",
    "    \n",
    "    return get_feature_vec(short_sentences), get_label_vec(short_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_short, y_short = get_short_instances(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val_short, y_val_short = get_short_instances(dev_sentences, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8791,)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1630,)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_short.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm down to 8791 training sentences and 1630 validation sentences. This time lets store the padded data to avoid having to re-pad the sentences every epoch. \n",
    "Also, lets pad them all to be exactly 20 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_data(X, y, n_features, output_dim, max_seq_len=20):\n",
    "    X_padded = np.zeros((len(X), max_seq_len, n_features), dtype='float32')\n",
    "    y_padded = np.zeros((len(y), max_seq_len, output_dim), dtype='float32')\n",
    "    \n",
    "    for i, (instance, labels) in enumerate(zip(X, y)):\n",
    "        X_padded[i,:len(instance)] = instance\n",
    "        y_padded[i, :len(labels)] = labels\n",
    "    return X_padded, y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short_padded, y_short_padded = pad_data(X_short, y_short, n_features, n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8791, 20, 300)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model\n",
    "I will try to not pad the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8791/8791 [==============================] - 43s 5ms/step - loss: 0.5442 - acc: 0.3976\n",
      "Epoch 2/2\n",
      "8791/8791 [==============================] - 42s 5ms/step - loss: 0.2661 - acc: 0.4539\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d84ff5b208>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model2.fit(X_short_padded, y_short_padded, batch_size=64, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I significantly reduced training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62488046909476758, 0.80963499824419338]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate_generator(GenerateData(X_val_short, y_val_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, accuracy on the valiation set is 80% after 2 epochs, which is a similar result to my previous attempt with padding.\n",
    "Lets train the model a little longer and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8791/8791 [==============================] - 42s 5ms/step - loss: 0.2246 - acc: 0.4555\n",
      "Epoch 2/20\n",
      "8791/8791 [==============================] - 51s 6ms/step - loss: 0.1989 - acc: 0.4601\n",
      "Epoch 3/20\n",
      "8791/8791 [==============================] - 53s 6ms/step - loss: 0.1790 - acc: 0.4643\n",
      "Epoch 4/20\n",
      "8791/8791 [==============================] - 54s 6ms/step - loss: 0.1624 - acc: 0.4690\n",
      "Epoch 5/20\n",
      "8791/8791 [==============================] - 67s 8ms/step - loss: 0.1481 - acc: 0.4731\n",
      "Epoch 6/20\n",
      "8791/8791 [==============================] - 51s 6ms/step - loss: 0.1357 - acc: 0.4717\n",
      "Epoch 7/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.1247 - acc: 0.4733\n",
      "Epoch 8/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.1139 - acc: 0.4762\n",
      "Epoch 9/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.1036 - acc: 0.4790\n",
      "Epoch 10/20\n",
      "8791/8791 [==============================] - 51s 6ms/step - loss: 0.0961 - acc: 0.4817\n",
      "Epoch 11/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.0887 - acc: 0.4817\n",
      "Epoch 12/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.0819 - acc: 0.4850\n",
      "Epoch 13/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.0768 - acc: 0.4885\n",
      "Epoch 14/20\n",
      "8791/8791 [==============================] - 53s 6ms/step - loss: 0.0724 - acc: 0.4875\n",
      "Epoch 15/20\n",
      "8791/8791 [==============================] - 51s 6ms/step - loss: 0.0680 - acc: 0.4904\n",
      "Epoch 16/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.0650 - acc: 0.4911\n",
      "Epoch 17/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.0616 - acc: 0.4926\n",
      "Epoch 18/20\n",
      "8791/8791 [==============================] - 52s 6ms/step - loss: 0.0587 - acc: 0.4931\n",
      "Epoch 19/20\n",
      "8791/8791 [==============================] - 53s 6ms/step - loss: 0.0568 - acc: 0.4924\n",
      "Epoch 20/20\n",
      "8791/8791 [==============================] - 53s 6ms/step - loss: 0.0537 - acc: 0.4948\n",
      "Wall time: 17min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d852af2128>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model2.fit(X_short_padded, y_short_padded, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.94173224774462994, 0.81284289407126742]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate_generator(GenerateData(X_val_short, y_val_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97171295310731165, 0.81377732010556392]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate_generator(GenerateData(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12118941652556567, 0.95545017976576985]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate_generator(GenerateData(X_short, y_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so no improvement in accuracy from training. Thats a bummer.\n",
    "On the other hand, it's nice to see that I get the same accuracy on the full validation set as I do on the short sentences.\n",
    "\n",
    "Anyway, lets have a look at the predictions of the model just to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sentence in X_val:\n",
    "    predictions.append(model2.predict_classes(sentence.reshape(1, sentence.shape[0], sentence.shape[1]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[pos_tags[i] for i in sentence] for sentence in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eastern</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baqubah</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>guerrillas</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>detonated</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>car</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bomb</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>outside</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>a</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>police</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>station</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>killing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>several</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>people</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Token Prediction Target\n",
       "0           In        ADP    ADP\n",
       "1          the        DET    DET\n",
       "2      eastern        ADJ    ADJ\n",
       "3         city       NOUN   NOUN\n",
       "4           of      CCONJ    ADP\n",
       "5      Baqubah      PROPN  PROPN\n",
       "6            ,      PUNCT  PUNCT\n",
       "7   guerrillas       NOUN   NOUN\n",
       "8    detonated       VERB   VERB\n",
       "9            a      PUNCT    DET\n",
       "10         car       NOUN   NOUN\n",
       "11        bomb       NOUN   NOUN\n",
       "12     outside        ADV    ADP\n",
       "13           a      PUNCT    DET\n",
       "14      police       NOUN   NOUN\n",
       "15     station       NOUN   NOUN\n",
       "16           ,      PUNCT  PUNCT\n",
       "17     killing       NOUN   VERB\n",
       "18     several        ADJ    ADJ\n",
       "19      people       NOUN   NOUN\n",
       "20           .      PUNCT  PUNCT"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = list(zip(dev_sentences[10], predictions[10], dev_labels[10])), columns = ['Token', 'Prediction', 'Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like what you would expect, a pretty decent tagger making mistakes in about 20% of cases.\n",
    "\n",
    "Lets try just using one LSTM layer, but with more nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(BatchNormalization(input_shape=(None, n_features)))\n",
    "model3.add(LSTM(units=512, return_sequences=True))\n",
    "model3.add(Dropout(rate=.1))\n",
    "model3.add(Dense(units=n_labels, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8791/8791 [==============================] - 190s 22ms/step - loss: 0.3647 - acc: 0.4032\n",
      "Epoch 2/2\n",
      "8791/8791 [==============================] - 200s 23ms/step - loss: 0.2059 - acc: 0.4429\n",
      "Wall time: 6min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d85369aa90>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model3.fit(X_short_padded, y_short_padded, batch_size=64, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.56462470099781326, 0.82756802132981688]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(GenerateData(X_val_short, y_val_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55866566814410568, 0.82720946210560264]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(GenerateData(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35940897561025592, 0.88475357780696828]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(GenerateData(X_short, y_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar validation accuracy as previous models, longer trainingtime than the previous one. Let evaulate again after two more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8791/8791 [==============================] - 195s 22ms/step - loss: 0.1584 - acc: 0.4560\n",
      "Epoch 2/2\n",
      "8791/8791 [==============================] - 188s 21ms/step - loss: 0.1154 - acc: 0.4703\n",
      "Wall time: 6min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d84d9a1940>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model3.fit(X_short_padded, y_short_padded, batch_size=64, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.58038230885717357, 0.82080563112021565]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(GenerateData(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2106831872414589, 0.93159210102717604]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate_generator(GenerateData(X_short, y_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy did not increase with two more epochs. \n",
    "Right now I am at a loss on how to calibrate my network to generalize better.\n",
    "I think I would like to learn more about RNNs by trying out some more projects, and then get back to doing POS tagging with LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook I have trained RNNs on variable length inputs by using padding and single instance batches.\n",
    "\n",
    "I did not achieve any good results on the validation set, 82% accuracy being my best result. \n",
    "\n",
    "Training RNNs takes a lot more time compared to regular DNNs, this resulted in me not allowing my networks that many epochs to train. However, none of the networks showed any signs of improving validation accuracy with more training.\n",
    "\n",
    "Right now I feel like the main hurdle for me in improving my model is that I lack experience using RNNs.\n",
    "I would like to gain some more experience, perhaps by mimicking other people models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
