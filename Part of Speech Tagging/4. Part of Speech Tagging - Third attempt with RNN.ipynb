{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In `3. Part of Speech Tagging - Second attempt with RNN` I did Part of Speech (POS) tagging with several RNN's. My most successful was inspired by [Wang et al., 2015](https://arxiv.org/pdf/1510.06168.pdf), who used a model originally from [Graves, 2002](https://www.cs.toronto.edu/~graves/preprint.pdf):\n",
    "Two LSTM layers with 93 hidden units, one doing a forward pass of the input and the other a backward pass. \n",
    "\n",
    "I encoded all words using word embeddings trained by [Glove](https://nlp.stanford.edu/projects/glove/). Glove is Stanfords embedding model.\n",
    "\n",
    "In this notebook I will use a similar approach, but this time I will implement my model in PyTorch. Hopefully increased control will help me to learn an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "I will be using the same training data for my tagger as in all my previous notebooks:\n",
    "[Universal Dependencies - English Web Treebank](http://universaldependencies.org/treebanks/en_ewt/index.html), a CoNLL-U formart corpus with 254 830 words and 16 622 sentences in english *taken from various web media including weblogs, newsgroups, emails, reviews, and Yahoo! answers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "First lets load the training data and convert it to a python dictionary.\n",
    "I use the [conllu](https://github.com/EmilStenstrom/conllu) python package to parse the CoNLL-U files to dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import conllu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-train.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-dev.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    dev_text = f.read()\n",
    "    \n",
    "directory = 'UD/UD_English-EWT'\n",
    "with open('{}/en_ewt-ud-test.conllu'.format(directory), 'r', encoding='utf-8') as f:\n",
    "    test_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dict = conllu.parse(train_text)\n",
    "dev_dict = conllu.parse(dev_text)\n",
    "test_dict = conllu.parse(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count sentences and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 12543 sentences and 204607 tokens\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "n_train_sentences = len(train_dict)\n",
    "n_train_tokens = reduce(lambda x, y: x + len(y), train_dict, 0)\n",
    "\n",
    "print(\"The training set contains {} sentences and {} tokens\".format(n_train_sentences, n_train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences = [[token['form'] for token in sentence] for sentence in train_dict]\n",
    "train_labels = [[token['upostag'] for token in sentence] for sentence in train_dict]\n",
    "\n",
    "dev_sentences = [[token['form'] for token in sentence] for sentence in dev_dict]\n",
    "dev_labels = [[token['upostag'] for token in sentence] for sentence in dev_dict]\n",
    "\n",
    "test_sentences = [[token['form'] for token in sentence] for sentence in test_dict]\n",
    "test_labels = [[token['upostag'] for token in sentence] for sentence in test_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags = list(set(reduce(lambda x, y: x + y, train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labels = len(pos_tags)\n",
    "n_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "[Wang et al.](https://arxiv.org/pdf/1510.06168.pdf) showed that a bidirectional LSTM network could achieve state of the art performance without using any morphological features, they only used these features:\n",
    "* Word embedding of the word (cast to lower case). Embeddings trained by the same architecture, but on another task.\n",
    "* Suffix of length two, one-hot encoded\n",
    "* Wether the word is all caps, lower case, or has an initial capital letter. One-hot encoded.\n",
    "\n",
    "Other papers, like [Xiao et al.](https://arxiv.org/abs/1809.01997), complement word embeddings with character embeddings. \n",
    "It would be interesting to experiment with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "I will opt to use the 100 dimensional Glove 6B data. My vocabulary will be exactly the words inside the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_path = 'glove/glove.6B/glove.6B.100d.txt'\n",
    "embeddings = {}\n",
    "token_index = {}\n",
    "index_token = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tok, *vec = line.split()\n",
    "        embeddings[tok] = np.array(vec, dtype='float32')\n",
    "        # Reserve index 0 for padding\n",
    "        token_index[tok] = i + 1\n",
    "        index_token[i+1] = tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know from my last notebook that my data has many words that are OOV for Glove. I adressed this by pre-processing my data. In this notebook I will condense my preprocessing to a single method, check the other one for more rationale on why I apply each step.\n",
    "\n",
    "I will apply pre-processing to all three data sets: train, dev and test. As the preprocessing is not trained on the data this could be done on new data in production as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def allign_to_vocab(vocab, sentences):\n",
    "    \n",
    "    # Find all OOV tokens\n",
    "    oov = find_oov(vocab, sentences)\n",
    "    print(\"%d OOV tokens before processing\" % len(oov))\n",
    "    \n",
    "    # Convert to lower case\n",
    "    sentences = [[token.lower() for token in sentence] for sentence in sentences]\n",
    "    \n",
    "    oov = find_oov(vocab, sentences)\n",
    "    print(\"%d OOV tokens after converting to lower case\" % len(oov))\n",
    "    \n",
    "    # Replace URL's with 'url'\n",
    "    sentences = [[convert_url(token) for token in sentence] for sentence in sentences]\n",
    "    \n",
    "    oov = find_oov(vocab, sentences)\n",
    "    print(\"%d OOV tokens after converting urls\" % len(oov))\n",
    "    \n",
    "    # Build spelling correction dictionary\n",
    "    # Search for word in vocabulary words within 1 Levensthein Damerau distance\n",
    "    new_spelling = dict([(word, find_one_neighbour(word, embeddings)) for word in oov])\n",
    "    sentences = [[new_spelling[token] if token in new_spelling else token for token in sentence] for sentence in sentences]\n",
    "    \n",
    "    oov = find_oov(vocab, sentences)\n",
    "    print(\"%d OOV tokens after spelling correction\" % len(oov))\n",
    "    \n",
    "    # Replace OOV words with 'unk'\n",
    "    # See https://stackoverflow.com/questions/49239941/what-is-unk-in-glove-6b-50d-txt\n",
    "    sentences = [['unk' if token in oov else token for token in sentence] for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def find_oov(vocab, sentences):\n",
    "    oov = []\n",
    "    for sentence in sentences:\n",
    "        for tok in sentence:\n",
    "            if tok not in vocab:\n",
    "                oov.append(tok)\n",
    "    return oov\n",
    "     \n",
    "def convert_url(token):\n",
    "    # Match words starting with www., http:// or https://\n",
    "    if re.match(r'^(?:https{0,1}\\:\\/\\/.*|www\\.*)', token):\n",
    "        return \"url\"\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "# Checks for vocabulary words within 1 Damerau Levenstein distance and returns the first match\n",
    "# Logic inspired by http://norvig.com/spell-correct.html\n",
    "def find_one_neighbour(word, vocab):\n",
    "    \n",
    "    ascii_vocab = [str(chr(i)) for i in range(32, 127)]\n",
    "    \n",
    "    # Tuples with all possible splits of word\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # All words generated by deleting one character\n",
    "    for L, R in splits:\n",
    "        candidate = L + R[1:] if R else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by swapping two characters in word\n",
    "    for L, R in splits:\n",
    "        candidate = L + R[1:] if R else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by swapping two characters in word\n",
    "    for L, R in splits:    \n",
    "        candidate = L + R[1] + R[0] + R[2:] if len(R) > 1 else None\n",
    "        if candidate in vocab:\n",
    "            return candidate\n",
    "    # All words generated by inserting a character in word\n",
    "    for L, R in splits:\n",
    "        for c in ascii_vocab:    \n",
    "            candidate = L + c + R \n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "    # All words generated by replacing a character in word\n",
    "    for L, R, in splits:\n",
    "        for c in ascii_vocab:    \n",
    "            candidate = L + c +R[1:] if R else None\n",
    "            if candidate in vocab:\n",
    "                return candidate\n",
    "        \n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "31192 OOV tokens before processing\n",
      "2442 OOV tokens after converting to lower case\n",
      "2310 OOV tokens after converting urls\n",
      "1066 OOV tokens after spelling correction\n",
      "\n",
      "Dev set:\n",
      "4362 OOV tokens before processing\n",
      "463 OOV tokens after converting to lower case\n",
      "424 OOV tokens after converting urls\n",
      "215 OOV tokens after spelling correction\n",
      "\n",
      "Test set:\n",
      "4589 OOV tokens before processing\n",
      "522 OOV tokens after converting to lower case\n",
      "483 OOV tokens after converting urls\n",
      "267 OOV tokens after spelling correction\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Training set:\")\n",
    "train_sentences = allign_to_vocab(embeddings, train_sentences)\n",
    "\n",
    "print(\"\\nDev set:\")\n",
    "dev_sentences = allign_to_vocab(embeddings, dev_sentences)\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "test_sentences = allign_to_vocab(embeddings, test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data\n",
    "I will encode the targets and the tokens as integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "First build a map from pos tag to an index. Reserve 0 for Padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tag_index = {}\n",
    "for i, pos in enumerate(pos_tags):\n",
    "    pos_tag_index[pos] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [torch.from_numpy(np.asarray([pos_tag_index[pos] for pos in sentence])).long()  for sentence in train_labels]\n",
    "Y_dev = [torch.from_numpy(np.asarray([pos_tag_index[pos] for pos in sentence])).long()  for sentence in dev_labels]\n",
    "Y_test = [torch.from_numpy(np.asarray([pos_tag_index[pos] for pos in sentence])).long()  for sentence in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "I already built the mapping from token to index. I also replaced all OOV tokens with `unk`, so all words will be in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [torch.from_numpy(np.asarray([token_index[tok] for tok in sentence])).long() for sentence in train_sentences]\n",
    "X_dev = [torch.from_numpy(np.asarray([token_index[tok]  for tok in sentence])).long()  for sentence in dev_sentences]\n",
    "X_test = [torch.from_numpy(np.asarray([token_index[tok] for tok in sentence])).long()  for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "Create a frozen embedding layer from the Glove data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 100\n",
    "embedding_matrix = torch.from_numpy(np.array(list(embeddings.values())))\n",
    "n_words = len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a frozen embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embeddings(weights, frozen=True):\n",
    "    \n",
    "    embedding_layer = nn.Embedding(*weights.shape, _weight=embedding_matrix)\n",
    "    embedding_layer.weight.requires_grad = frozen\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = create_embeddings(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLSTM 1\n",
    "Let's implement the BLSTM introduced by Graves, with our Glove embeddings as the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BLSTM1(nn.Module):\n",
    "    def __init__(self, lstm_dim, n_classes, embedding_weights):\n",
    "        super(BLSTM1, self).__init__()\n",
    "        \n",
    "        # Variables\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.vocab_size, self.embedding_dim = embedding_weights.shape\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = create_embeddings(embedding_weights)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, lstm_dim, batch_first=True, bidirectional=True)\n",
    "        self.output = nn.Linear(self.lstm_dim * 2, self.n_classes + 1)\n",
    "        \n",
    "    def forward(self, padded_batch, sequence_lengths):\n",
    "        \n",
    "        # Embeddings\n",
    "        embedded = self.embedding(padded_batch)#.unsqueeze(0)\n",
    "        \n",
    "         # Pack Padded Batch\n",
    "        total_length = padded_batch.size(1)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, batch_first=True, lengths=sequence_lengths)\n",
    "        \n",
    "        # LSTM\n",
    "        out, _ = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Reverse Packing\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=total_length)\n",
    "        \n",
    "        # Fully Connected\n",
    "        out = F.log_softmax(self.output(out), 2)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blstm1 = BLSTM1(96, n_labels, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(blstm1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PaddedGenerator():\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.x, self.y, self.sequence_lengths = [], [], []\n",
    "        # Pad Data so all batches have sequences of equal length\n",
    "        for idx in range(int(np.ceil(len(x_set) / float(self.batch_size)))):\n",
    "            batch_x = x_set[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_y = y_set[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            \n",
    "            # The input is sorted so the first sequence is the longest\n",
    "            max_len = len(batch_x[0])\n",
    "            batch_sequence_lengths = torch.Tensor(list(map(len, batch_x))).int()\n",
    "            batch_x = nn.utils.rnn.pad_sequence(batch_x, batch_first=True)\n",
    "            batch_y = nn.utils.rnn.pad_sequence(batch_y, batch_first=True)\n",
    "            \n",
    "            self.x.append(batch_x)\n",
    "            self.y.append(batch_y)\n",
    "            self.sequence_lengths.append(batch_sequence_lengths)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.sequence_lengths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function inspired by https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "def train_model(model, train_generator, optimizer, criterion, dev_generator=None, epochs = 2):\n",
    "    \n",
    "    # Only enter the validation state if there is a validation_loader\n",
    "    phases = ['train']\n",
    "    data_dict = {'train' : train_generator} \n",
    "    if dev_generator:\n",
    "        phases.append('val')\n",
    "        data_dict['val'] = dev_generator\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in phases:\n",
    "            \n",
    "            data = data_dict[phase]\n",
    "            \n",
    "            # Only update model weights based on the training data\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            words_processed = 0\n",
    "            \n",
    "            for seq, labels, sequence_lengths in tqdm(data, total = len(data)):\n",
    "                \n",
    "                \n",
    "                #labels = torch.autograd.Variable(labels).type(torch.LongTensor)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Only track history during training\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(seq, sequence_lengths)\n",
    "                    loss = criterion(outputs.view(-1, n_labels + 1), labels.view(-1))\n",
    "                    predictions = torch.argmax(outputs, dim=2)\n",
    "                    \n",
    "                    # Only perform backpropagation during training\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # Save statistics\n",
    "                running_loss += loss.item() * torch.sum(sequence_lengths).item()\n",
    "                # Only count correct classifications when the label is not padding\n",
    "                running_corrects += torch.sum((predictions == labels.data) * (labels.data != 0))\n",
    "                words_processed += torch.sum(sequence_lengths)\n",
    "                \n",
    "                #return predictions, labels, words_processed\n",
    "                \n",
    "            epoch_loss = running_loss / words_processed.item()\n",
    "            epoch_acc = running_corrects.item() / words_processed.item()\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sorted(X_train, reverse=True, key=len)\n",
    "Y_train = sorted(Y_train, reverse=True, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dev = sorted(X_dev, reverse=True, key=len)\n",
    "Y_dev = sorted(Y_dev, reverse=True, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = sorted(X_test, reverse=True, key=len)\n",
    "Y_test = sorted(Y_test, reverse=True, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = PaddedGenerator(X_train, Y_train, batch_size=126)\n",
    "dev_padded = PaddedGenerator(X_dev, Y_dev, batch_size=126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.9682 Acc: 0.4418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.2596 Acc: 0.3310\n",
      "Epoch 2/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7649 Acc: 0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5118 Acc: 0.8401\n"
     ]
    }
   ],
   "source": [
    "train_model(blstm1, train_padded, optimizer, criterion, dev_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3798 Acc: 0.8869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4255 Acc: 0.8665\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:28<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2907 Acc: 0.9137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3846 Acc: 0.8813\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2355 Acc: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3750 Acc: 0.8860\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:08<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1992 Acc: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3761 Acc: 0.8892\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:09<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1728 Acc: 0.9489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3810 Acc: 0.8915\n"
     ]
    }
   ],
   "source": [
    "train_model(blstm1, train_padded, optimizer, criterion, dev_padded, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1524 Acc: 0.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3856 Acc: 0.8940\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1358 Acc: 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3891 Acc: 0.8957\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1219 Acc: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3921 Acc: 0.8978\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1101 Acc: 0.9680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3951 Acc: 0.8990\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0998 Acc: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3992 Acc: 0.8999\n"
     ]
    }
   ],
   "source": [
    "train_model(blstm1, train_padded, optimizer, criterion, dev_padded, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(blstm1.state_dict(), 'pytorch_models/blstm1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results as my last BLSTM.\n",
    "\n",
    "It would be interesting to try using a GRU instead of LSTM, apply some regularization like Dropout, and perhaps experiment with character level features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
