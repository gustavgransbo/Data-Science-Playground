{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook I will atempt to build a simple machine translator from English to Swedish based on Francois Chollet's article [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). \n",
    "\n",
    "My hope is that mimicking someoneelses work with RNNs will help me grow my understanding of both RNNs and how to model them using Keras.\n",
    "\n",
    "## Disclaimer\n",
    "I will draw lots of inspiration from Chollet's [code](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py), some parts will be straight up copy pasting. \n",
    "My goal with this notebook is simply to get some more experience working with RNNs in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "I will be using data from the same source as Chollet, http://www.manythings.org/anki/. I'm using the 17303 sentence long swe-eng data set, that contains english sentences and their swedish translations. The french data set used by Chollet is much larger, but he limited his training set to 10 000 sentences and used 20% of it for validation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/swe-eng/swe.txt'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences, target_sentences = [], []\n",
    "for line in lines:\n",
    "    try:\n",
    "        input_text, target_text, *_ = line.split('\\t')\n",
    "    except ValueError:\n",
    "        print(line)\n",
    "        \n",
    "    # Chollet uses tab as start of sentence and line feed as end of sentence characters.\n",
    "    # The start of sentence character will be used to seed new sentences and the end\n",
    "    # of sentence character will be used to terminate sentences.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_sentences.append(input_text)\n",
    "    target_sentences.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate sentence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_lens = np.array([len(sentence) for sentence in input_sentences])\n",
    "target_seq_lens = np.array([len(sentence) for sentence in target_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(input_seq_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(input_seq_lens, target_seq_lens)), columns = ['input lengths', 'target lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt = df.melt(value_name='Sentence Length', var_name='Data Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a7929aee80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAIVCAYAAADoGxMrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7929ae5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 9))\n",
    "sns.boxplot(x='Data Set', y='Sentence Length', data=df_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input lengths</th>\n",
       "      <th>target lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17304.000000</td>\n",
       "      <td>17304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.809408</td>\n",
       "      <td>28.172330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.960056</td>\n",
       "      <td>11.241518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>155.000000</td>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       input lengths  target lengths\n",
       "count   17304.000000    17304.000000\n",
       "mean       24.809408       28.172330\n",
       "std        10.960056       11.241518\n",
       "min         4.000000        5.000000\n",
       "25%        18.000000       21.000000\n",
       "50%        21.000000       26.000000\n",
       "75%        29.000000       33.000000\n",
       "max       155.000000      142.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be padding all data to be of equal sentence length, to reduce the amount of padding it is convinient to not include the longest sentences.\n",
    "Lets not keep any sentences longer than 50 characters.\n",
    "The length doesn't have to be the same for both languages, but it is convinient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " input_idx = np.where(input_seq_lens <= max_seq_len)\n",
    " target_idx = np.where(target_seq_lens <= max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16741 input sentences with 50 or fewer characters\n",
      "16522 target sentences with 50 or fewer characters\n"
     ]
    }
   ],
   "source": [
    "print(\"{} input sentences with {} or fewer characters\".format(len(input_idx[0]), max_seq_len))\n",
    "print(\"{} target sentences with {} or fewer characters\".format(len(target_idx[0]), max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_idx = np.intersect1d(input_idx, target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16404 input sentence pairs with 50 or fewer characters in both languages\n"
     ]
    }
   ],
   "source": [
    "print(\"{} input sentence pairs with {} or fewer characters in both languages\".format(len(keep_idx), max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = np.array(input_sentences)[keep_idx]\n",
    "target_sentences = np.array(target_sentences)[keep_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters = set()\n",
    "target_characters = set()\n",
    "for input_text, target_text in zip(input_sentences, target_sentences):\n",
    "    for char in input_text:\n",
    "        input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        target_characters.add(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "input_vocab_size = len(input_characters)\n",
    "target_vocab_size = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 75\n",
      "Target vocab size: 79\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: {}\".format(input_vocab_size))\n",
    "print(\"Target vocab size: {}\".format(target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build padded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create completely empty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_sentences), max_seq_len, input_vocab_size),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_sentences), max_seq_len, target_vocab_size),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_sentences), max_seq_len, target_vocab_size),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16404, 50, 75)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16404, 50, 79)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16404, 50, 79)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill in all the values that we have available, leaving the rest as padding. Encode all characters using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_sentences, target_sentences)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into a training and a validation set\n",
    "I will use 8 000 sentances as training set and 2000 as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainig_size, validation_size = 8000, 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(len(input_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = shuffle_idx[:trainig_size], shuffle_idx[trainig_size:trainig_size+validation_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input_train, encoder_input_val = encoder_input_data[train_idx], encoder_input_data[val_idx]\n",
    "\n",
    "decoder_input_train, decoder_input_val = decoder_input_data[train_idx], decoder_input_data[val_idx]\n",
    "\n",
    "decoder_target_train, decoder_target_val = decoder_target_data[train_idx], decoder_target_data[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chollet has built one model for training and the resused its layers in another model used for tranlsation.\n",
    "I will copy Chollet's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "batch_size = 64\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define the encoder to have one hidden LSTM layer and output its state\n",
    "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# Only save the states of the encoder, we don't care about it's output\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder, it will use the state of the encoder as its initial state\n",
    "decoder_inputs = Input(shape=(None, target_vocab_size))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 177s 22ms/step - loss: 1.4626 - val_loss: 1.3017\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 172s 22ms/step - loss: 1.1768 - val_loss: 1.0931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7de8b8780>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=([encoder_input_val, decoder_input_val], decoder_target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challot ran his model for 100 epochs before evaluating it. This would take close to 5 hours on my laptop CPU...\n",
    "I'd like to check if GRUs speed up training as they are simpler and have been shown to have similar performance as LSTM cells [Klaus Greff et al.](https://arxiv.org/pdf/1503.04069.pdf). Maybe I could use them instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "# Define the encoder to have one hidden GRU layer and output it's state\n",
    "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "_, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# Define the decoder, it will use the state of the encoder as its initial state\n",
    "decoder_inputs = Input(shape=(None, target_vocab_size))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs,\n",
    "                                     initial_state=state_h)\n",
    "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 1.4249 - val_loss: 1.1447\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 156s 20ms/step - loss: 1.0517 - val_loss: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7d742ca90>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model2.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=([encoder_input_val, decoder_input_val], decoder_target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time per epoch is a little bit lower, and also we reach a little bit lower validation loss. Lets bet on the GRU and allow it to train for 100 epochs.\n",
    "\n",
    "But first, lets see what kind of translations we get after two epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_outputs, state_h = decoder_gru(\n",
    "    decoder_inputs, initial_state=decoder_state_input_h)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_state_input_h],\n",
    "    [decoder_outputs] + [state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell is straight up copy pasted from https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "# With only small modifications to fit my GRU model and my global variable names\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, target_vocab_size))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + [states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, target_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentences_val = input_sentences[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I just left Tom.\n",
      "Decoded sentence: Jag är inte det här inte här inte här inte här inte\n",
      "-\n",
      "Input sentence: Can we make it?\n",
      "Decoded sentence: Det är inte det här inte här inte här inte här inte\n",
      "-\n",
      "Input sentence: She was right in the middle of cutting cucumbers.\n",
      "Decoded sentence: Han är inte det här inte här inte här inte här inte\n",
      "-\n",
      "Input sentence: I've got a few more.\n",
      "Decoded sentence: Jag är inte det här inte här inte här inte här inte\n",
      "-\n",
      "Input sentence: Did they buy it?\n",
      "Decoded sentence: Han är inte det här inte här inte här inte här inte\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_val[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_val[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar translations for each sentence, but with some interesting details:\n",
    "* The translated sentences begin with `Jag` when  the input sentence starts with `I`!\n",
    "* The translated sentences are real swedish words!\n",
    "\n",
    "Lets give the model another 98 epochs to train, for a total of 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/98\n",
      "8000/8000 [==============================] - 170s 21ms/step - loss: 0.9496 - val_loss: 0.9266\n",
      "Epoch 2/98\n",
      "8000/8000 [==============================] - 160s 20ms/step - loss: 0.8983 - val_loss: 0.8820\n",
      "Epoch 3/98\n",
      "8000/8000 [==============================] - 159s 20ms/step - loss: 0.8584 - val_loss: 0.8472\n",
      "Epoch 4/98\n",
      "8000/8000 [==============================] - 157s 20ms/step - loss: 0.8245 - val_loss: 0.8251\n",
      "Epoch 5/98\n",
      "8000/8000 [==============================] - 162s 20ms/step - loss: 0.7955 - val_loss: 0.7913\n",
      "Epoch 6/98\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.7700 - val_loss: 0.7732\n",
      "Epoch 7/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.7462 - val_loss: 0.7534\n",
      "Epoch 8/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.7257 - val_loss: 0.7338\n",
      "Epoch 9/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.7061 - val_loss: 0.7245\n",
      "Epoch 10/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.6887 - val_loss: 0.7055\n",
      "Epoch 11/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.6719 - val_loss: 0.6940\n",
      "Epoch 12/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.6564 - val_loss: 0.6826\n",
      "Epoch 13/98\n",
      "8000/8000 [==============================] - 158s 20ms/step - loss: 0.6423 - val_loss: 0.6783\n",
      "Epoch 14/98\n",
      "8000/8000 [==============================] - 28500s 4s/step - loss: 0.6288 - val_loss: 0.6658\n",
      "Epoch 15/98\n",
      "8000/8000 [==============================] - 137s 17ms/step - loss: 0.6177 - val_loss: 0.6595\n",
      "Epoch 16/98\n",
      "8000/8000 [==============================] - 129s 16ms/step - loss: 0.6032 - val_loss: 0.6553\n",
      "Epoch 17/98\n",
      "8000/8000 [==============================] - 133s 17ms/step - loss: 0.5914 - val_loss: 0.6484\n",
      "Epoch 18/98\n",
      "8000/8000 [==============================] - 143s 18ms/step - loss: 0.5794 - val_loss: 0.6450\n",
      "Epoch 19/98\n",
      "8000/8000 [==============================] - 147s 18ms/step - loss: 0.5687 - val_loss: 0.6410\n",
      "Epoch 20/98\n",
      "8000/8000 [==============================] - 146s 18ms/step - loss: 0.5578 - val_loss: 0.6372\n",
      "Epoch 21/98\n",
      "8000/8000 [==============================] - 149s 19ms/step - loss: 0.5474 - val_loss: 0.6345\n",
      "Epoch 22/98\n",
      "8000/8000 [==============================] - 148s 19ms/step - loss: 0.5371 - val_loss: 0.6310\n",
      "Epoch 23/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.5272 - val_loss: 0.6310\n",
      "Epoch 24/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.5171 - val_loss: 0.6296\n",
      "Epoch 25/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.5074 - val_loss: 0.6278\n",
      "Epoch 26/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.4982 - val_loss: 0.6291\n",
      "Epoch 27/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.4895 - val_loss: 0.6298\n",
      "Epoch 28/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.4805 - val_loss: 0.6289\n",
      "Epoch 29/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.4714 - val_loss: 0.6316\n",
      "Epoch 30/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.4624 - val_loss: 0.6367\n",
      "Epoch 31/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.4535 - val_loss: 0.6375\n",
      "Epoch 32/98\n",
      "8000/8000 [==============================] - 149s 19ms/step - loss: 0.4454 - val_loss: 0.6404\n",
      "Epoch 33/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.4371 - val_loss: 0.6446\n",
      "Epoch 34/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.4290 - val_loss: 0.6469\n",
      "Epoch 35/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.4209 - val_loss: 0.6548\n",
      "Epoch 36/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.4130 - val_loss: 0.6539\n",
      "Epoch 37/98\n",
      "8000/8000 [==============================] - 158s 20ms/step - loss: 0.4057 - val_loss: 0.6601\n",
      "Epoch 38/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.3981 - val_loss: 0.6660\n",
      "Epoch 39/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.3912 - val_loss: 0.6716\n",
      "Epoch 40/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.3842 - val_loss: 0.6758\n",
      "Epoch 41/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.3767 - val_loss: 0.6830\n",
      "Epoch 42/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.3709 - val_loss: 0.6841\n",
      "Epoch 43/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.3637 - val_loss: 0.6946\n",
      "Epoch 44/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.3582 - val_loss: 0.6972\n",
      "Epoch 45/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.3519 - val_loss: 0.7043\n",
      "Epoch 46/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.3467 - val_loss: 0.7113\n",
      "Epoch 47/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.3406 - val_loss: 0.7168\n",
      "Epoch 48/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.3354 - val_loss: 0.7213\n",
      "Epoch 49/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.3306 - val_loss: 0.7229\n",
      "Epoch 50/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.3254 - val_loss: 0.7287\n",
      "Epoch 51/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.3209 - val_loss: 0.7401\n",
      "Epoch 52/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.3159 - val_loss: 0.7398\n",
      "Epoch 53/98\n",
      "8000/8000 [==============================] - 158s 20ms/step - loss: 0.3119 - val_loss: 0.7511\n",
      "Epoch 54/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.3073 - val_loss: 0.7530\n",
      "Epoch 55/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.3030 - val_loss: 0.7589\n",
      "Epoch 56/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.2994 - val_loss: 0.7652\n",
      "Epoch 57/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2946 - val_loss: 0.7654\n",
      "Epoch 58/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2910 - val_loss: 0.7737\n",
      "Epoch 59/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2872 - val_loss: 0.7780\n",
      "Epoch 60/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.2834 - val_loss: 0.7852\n",
      "Epoch 61/98\n",
      "8000/8000 [==============================] - 158s 20ms/step - loss: 0.2802 - val_loss: 0.7909\n",
      "Epoch 62/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2767 - val_loss: 0.7944\n",
      "Epoch 63/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2733 - val_loss: 0.7991\n",
      "Epoch 64/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2702 - val_loss: 0.8010\n",
      "Epoch 65/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2668 - val_loss: 0.8074\n",
      "Epoch 66/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2639 - val_loss: 0.8150\n",
      "Epoch 67/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.2612 - val_loss: 0.8209\n",
      "Epoch 68/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.2578 - val_loss: 0.8208\n",
      "Epoch 69/98\n",
      "8000/8000 [==============================] - 156s 19ms/step - loss: 0.2547 - val_loss: 0.8303\n",
      "Epoch 70/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2522 - val_loss: 0.8269\n",
      "Epoch 71/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2496 - val_loss: 0.8375\n",
      "Epoch 72/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.2465 - val_loss: 0.8368\n",
      "Epoch 73/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2441 - val_loss: 0.8534\n",
      "Epoch 74/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2419 - val_loss: 0.8515\n",
      "Epoch 75/98\n",
      "8000/8000 [==============================] - 154s 19ms/step - loss: 0.2398 - val_loss: 0.8626\n",
      "Epoch 76/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.2372 - val_loss: 0.8613\n",
      "Epoch 77/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.2350 - val_loss: 0.8643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2326 - val_loss: 0.8671\n",
      "Epoch 79/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2294 - val_loss: 0.8718\n",
      "Epoch 80/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2277 - val_loss: 0.8743\n",
      "Epoch 81/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.2257 - val_loss: 0.8822\n",
      "Epoch 82/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2233 - val_loss: 0.8888\n",
      "Epoch 83/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2214 - val_loss: 0.8916\n",
      "Epoch 84/98\n",
      "8000/8000 [==============================] - 155s 19ms/step - loss: 0.2191 - val_loss: 0.8929\n",
      "Epoch 85/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2169 - val_loss: 0.9020\n",
      "Epoch 86/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2154 - val_loss: 0.8995\n",
      "Epoch 87/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2135 - val_loss: 0.9024\n",
      "Epoch 88/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2108 - val_loss: 0.9016\n",
      "Epoch 89/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2095 - val_loss: 0.9148\n",
      "Epoch 90/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2079 - val_loss: 0.9127\n",
      "Epoch 91/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2053 - val_loss: 0.9198\n",
      "Epoch 92/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.2039 - val_loss: 0.9237\n",
      "Epoch 93/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.2019 - val_loss: 0.9285\n",
      "Epoch 94/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.2003 - val_loss: 0.9298\n",
      "Epoch 95/98\n",
      "8000/8000 [==============================] - 152s 19ms/step - loss: 0.1981 - val_loss: 0.9348\n",
      "Epoch 96/98\n",
      "8000/8000 [==============================] - 150s 19ms/step - loss: 0.1982 - val_loss: 0.9406\n",
      "Epoch 97/98\n",
      "8000/8000 [==============================] - 151s 19ms/step - loss: 0.1966 - val_loss: 0.9462\n",
      "Epoch 98/98\n",
      "8000/8000 [==============================] - 153s 19ms/step - loss: 0.1942 - val_loss: 0.9473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7d8db9278>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=98,\n",
    "          validation_data=([encoder_input_val, decoder_input_val], decoder_target_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer gru_28 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'gru_27/while/Exit_2:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model2.save('s2s_200epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy actually starts going up again after just 25 epochs, so maybe I should have stopped the training there.\n",
    "\n",
    "Anyway lets translate some sentences from the validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I just left Tom.\n",
      "Decoded sentence: Jag vill bara ha stanna.\n",
      "\n",
      "-\n",
      "Input sentence: Can we make it?\n",
      "Decoded sentence: Kan vi gå nu?\n",
      "\n",
      "-\n",
      "Input sentence: She was right in the middle of cutting cucumbers.\n",
      "Decoded sentence: Hon skadades i en bilolycka.\n",
      "\n",
      "-\n",
      "Input sentence: I've got a few more.\n",
      "Decoded sentence: Jag har kulant rätt.\n",
      "\n",
      "-\n",
      "Input sentence: Did they buy it?\n",
      "Decoded sentence: Skadades någon?\n",
      "\n",
      "-\n",
      "Input sentence: She is quiet.\n",
      "Decoded sentence: Hon är ute efter ett skad.\n",
      "\n",
      "-\n",
      "Input sentence: Are you listening?\n",
      "Decoded sentence: Är du trött?\n",
      "\n",
      "-\n",
      "Input sentence: Mary really is a very cute girl.\n",
      "Decoded sentence: Mary är väldigt tillbaka här.\n",
      "\n",
      "-\n",
      "Input sentence: Follow the car.\n",
      "Decoded sentence: Tala om de har gå.\n",
      "\n",
      "-\n",
      "Input sentence: You're always careful.\n",
      "Decoded sentence: Du är så gatt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_val[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_val[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not correct translations, but they are similar to the inout sentences.\n",
    "For example, the sentence \"Can we make it?\" is translated to \"Kan vi gå nu?\", which means \"Can we leave now?\". \n",
    "So the first part is correct, and the model also understood it was a question and ended it with a questionmark!\n",
    "\n",
    "Some words are not even real Swedish though.\n",
    "\n",
    "Lets check some sentences from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentences_train = input_sentences[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Have I missed much?\n",
      "Decoded sentence: Har du röla tro oss?\n",
      "\n",
      "-\n",
      "Input sentence: Tom isn't rude.\n",
      "Decoded sentence: Tom är inte den där staden.\n",
      "\n",
      "-\n",
      "Input sentence: I have a headache.\n",
      "Decoded sentence: Jag har en fråga.\n",
      "\n",
      "-\n",
      "Input sentence: He is a gentleman.\n",
      "Decoded sentence: Han är en fort som musik stannade.\n",
      "\n",
      "-\n",
      "Input sentence: We were at the park.\n",
      "Decoded sentence: Vi var väldigt nära varandra.\n",
      "\n",
      "-\n",
      "Input sentence: Where in Turkey do you live?\n",
      "Decoded sentence: Var är de hur?\n",
      "\n",
      "-\n",
      "Input sentence: How did Tom do it?\n",
      "Decoded sentence: Hur kom du dit?\n",
      "\n",
      "-\n",
      "Input sentence: How can we prove it?\n",
      "Decoded sentence: Hur kan jag lå att det här korma?\n",
      "\n",
      "-\n",
      "Input sentence: I have to go.\n",
      "Decoded sentence: Jag måste sätta mig.\n",
      "\n",
      "-\n",
      "Input sentence: Tom's birthday's coming up.\n",
      "Decoded sentence: Toms muser är färdig.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_train[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not much better at the training sentences..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to feed the decoder it's own output as input during training, so it could learn the same way as it trains. Unfortunately this seems to be challenging to implement in Keras. See [this](https://github.com/keras-team/keras/issues/4068) Github issue for example.\n",
    "\n",
    "I also think I ought to have some way of giving the padded labels zero weight during training so that the model does not try to adjust based on how it performs past the end of sentence symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook I pretty much copied Chollet's work in [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html), with only some slight modifications:\n",
    "* I used a GRU layers instead of LSTM layers\n",
    "* I trained on English -> Swedish translations instead of English -> French\n",
    "* I limited training sentence length to 50 for both input and output\n",
    "\n",
    "Except for switching the LSTUs to GRUs I used the exact same model architecture, same amount of epochs and same size of training data.\n",
    "I even restricted my training data to not include sentences longer than 50, which should make it easier to train on.\n",
    "\n",
    "It seem like my model would not perform better by training longer, as validation loss reached it's minimum after just 25 epochs. \n",
    "Some possible improvements could be:\n",
    "* Increase model complexity:\n",
    "    * More layers\n",
    "    * More Neurons\n",
    "* Use LSTUs instead of GRUs\n",
    "* Regularize the model\n",
    "    * Dropout layers\n",
    "    * Batch normalization\n",
    "    * Perhaps some L1 or L2 regularization\n",
    "    \n",
    "## What I learned\n",
    "* I got to work with a seq2seq model, using some fancy new Keras syntax!\n",
    "* I got to use an inference model to make predictions post training.\n",
    "* Training a RNN really takes a lot of time, so there is little room for me to experiment with similar models.\n",
    "\n",
    "## New Questions\n",
    "* How does zero padding training data affect my loss fuction during training?\n",
    "* How are RNNs affected by extra hidden layers? What about wider layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
