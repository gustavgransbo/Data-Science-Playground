{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook I follow up on `1. Machine Translation - Character Level Model` by building a seq2seq model based onf word embeddings instead of character level embeddings.\n",
    "My previous notebooks was heavily inspired by Francois Chollet's article [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) and accompanying [code](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py). This notebook will be similar, but will probably diverge more.\n",
    "\n",
    "## Approach:\n",
    "Instead of one-hot encoding characters and terating them as separate units in the input and output sequences, I will be encoding full word tokens using an embedding layer.\n",
    "\n",
    "It would be nice to use pre-trained word embeddings, which are widely available for the English language, but not as common for Swedish. One approach would be to train Swedish embeddings on an auxilary task, but lets just use the standard Keras Embeddings layer this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "I will be using data from the same source as Chollet, http://www.manythings.org/anki/. I'm using the 17303 sentence long swe-eng data set, that contains english sentences and their swedish translations. The french data set used by Chollet is much larger, but he limited his training set to 10 000 sentences and used 20% of it for validation during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data/swe-eng/swe.txt'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all sentences. I won't add the begining of sentence symbol `<BOS>` until after tokenization to avoid it being split into mutliple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences, target_sentences = [], []\n",
    "for line in lines:\n",
    "    try:\n",
    "        input_text, target_text, *_ = line.split('\\t')\n",
    "    except ValueError:\n",
    "        print(line)\n",
    "        \n",
    "    input_sentences.append(input_text)\n",
    "    target_sentences.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and encode the data\n",
    "I will use the NLTK tokenizer to split sentences into tokens, and then process these tokens with Keras tokenizer which maps tokens to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK splits sentences into lists, but Keras tokenier expects them to be strings with a delimeter character.\n",
    "I will convert the lists back to strings with space as delimeter.\n",
    "\n",
    "Also, I will be adding a beggining of sentence token `<BOS>` to all target sequences, which will be used to seed the predictions during inference.\n",
    "I will also add the end of sentence token `<EOS>` token to the targets.\n",
    "\n",
    "I do not need these tokens for the inputs, as I will simply read the full input sentence during inference. The only stop condition I have will be based on the output. \n",
    "\n",
    "I will end up with four special tokens:\n",
    "* `<BOS>` and `<EOS>` are added explicitly \n",
    "* `<PAD>` is added implicitly during padding\n",
    "* `<UNK>`, the unkown token, will be inserted during inference for unrecognized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tokenized = [\" \".join(word_tokenize(sentence)) for sentence in input_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run !'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK even comes with a pre-trained swedish tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenized = [\"<BOS> \" + \" \".join(word_tokenize(sentence, language='swedish')) + \" <EOS>\" for sentence in target_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS> Spring ! <EOS>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to keep `!?.,` as punctuations, so I won't be filtering those out.\n",
    "I think one of the nicest features of my character level model was that it understood where to put punctuation.\n",
    "I will not be keeping the case of characters, sure, it's nice if sentences start with a capital letter, but this can easily be handled by a heuristic during inference.\n",
    "\n",
    "Also, I will keep the `<>` characters in the targets, as I use them in `<BOS>` and `<EOS>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\]^_`{|}~ ')\n",
    "input_tokenizer.fit_on_texts(input_tokenized)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_tokenizer = Tokenizer(filters='\"#$%&()*+-/:;=@[\\]^_`{|}~ ')\n",
    "target_tokenizer.fit_on_texts(target_tokenized)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = len(input_tokenizer.word_index)\n",
    "target_vocab_size = len(target_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a reverse lookup table from integer to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_input_word_index = dict(\n",
    "    (i, word) for word, i in input_tokenizer.word_index.items())\n",
    "reverse_target_word_index = dict(\n",
    "    (i, word) for word, i in target_tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run !'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join((map(lambda x: reverse_input_word_index[x], input_sequences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> spring ! <eos>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join((map(lambda x: reverse_target_word_index[x], target_sequences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_seq_len = max([len(sent) for sent in input_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_seq_len = max([len(sent) for sent in target_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input sequence length: 36\n",
      "Max target sequence length: 34\n"
     ]
    }
   ],
   "source": [
    "print(\"Max input sequence length: {}\" .format(max_input_seq_len))\n",
    "print(\"Max target sequence length: {}\" .format(max_target_seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest sentence is 36 tokens long. Last time I limited sentences to be 50 characters long to reduce the impact of padding. This time I will not limit them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_seq_len, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_target_seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift the decoder targets by one timestep and reshape to fit each target inside an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_sequences = np.zeros((target_sequences.shape[0], target_sequences.shape[1]))\n",
    "decoder_target_sequences[:,:-1] = target_sequences[:,1:]\n",
    "decoder_target_sequences = decoder_target_sequences.reshape(decoder_target_sequences.shape[0], \n",
    "                                                            decoder_target_sequences.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17304, 34, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into a training and a validation set\n",
    "I will use 8 000 sentances as training set and 2000 as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "trainig_size, validation_size = 8000, 2000\n",
    "\n",
    "shuffle_idx = np.random.permutation(len(input_sentences))\n",
    "\n",
    "train_idx, val_idx = shuffle_idx[:trainig_size], shuffle_idx[trainig_size:trainig_size+validation_size]\n",
    "\n",
    "input_sequences_train, input_sequences_val = input_sequences[train_idx], input_sequences[val_idx]\n",
    "\n",
    "target_sequences_train, target_sequences_val = target_sequences[train_idx], target_sequences[val_idx]\n",
    "\n",
    "decoder_target_sequences_train, decoder_target_sequences_val = decoder_target_sequences[train_idx], decoder_target_sequences[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_train = np.array(input_sentences)[train_idx]\n",
    "input_sentences_val = np.array(input_sentences)[val_idx]\n",
    "\n",
    "target_sentences_train = np.array(target_sentences)[train_idx]\n",
    "target_sentences_val = np.array(target_sentences)[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in my previous notebook I will opt for using GRUs instead of LSTM, mainly because I like the idea of their simpler architecture and because I would like to compare my results with my previous attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "batch_size = 64\n",
    "latent_dim = 256\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Embedding, Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I name layers that I won't reference in the future `x`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(input_vocab_size+1, embedding_dim)(encoder_inputs)\n",
    "x, state_h = GRU(latent_dim, return_state=True)(x)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(target_vocab_size+1, embedding_dim)(decoder_inputs)\n",
    "x = GRU(latent_dim, return_sequences=True)(x, initial_state=state_h)\n",
    "decoder_outputs = Dense(target_vocab_size+1, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(input_vocab_size+1, embedding_dim)(encoder_inputs)\n",
    "x, state_h = GRU(latent_dim, return_state=True)(x)\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embeddings = Embedding(target_vocab_size+1, embedding_dim)(decoder_inputs)\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "x, _ = decoder_gru(decoder_embeddings, initial_state=state_h)\n",
    "decoder_dense = Dense(target_vocab_size+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My targets are sequences of integers, so I use the `sparse_categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use sample_weight to ignore the padding when calculating the loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_train = (decoder_target_sequences_train != 0).reshape(decoder_target_sequences_train.shape[0], decoder_target_sequences_train.shape[1])\n",
    "\n",
    "sample_weights_train = sample_weights_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 892s 111ms/step - loss: 5.0035 - val_loss: 10.9708\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 993s 124ms/step - loss: 4.3377 - val_loss: 11.4374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d70239d7b8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_sequences_train, target_sequences_train], decoder_target_sequences_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=([input_sequences_val, target_sequences_val], decoder_target_sequences_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it translates some of the training sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict([input_sequences_train[:10], target_sequences_train[:10]])\n",
    "pred_val = model.predict([input_sequences_val[:10], target_sequences_val[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_output_seq(output_seq):\n",
    "    return \" \".join([reverse_target_word_index[sampled_word_index] if sampled_word_index > 0 else \"\" for sampled_word_index in np.argmax(output_seq, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions by the training model (Fed correct decoder input at each step)\n",
      "Input Sentence: Did you enjoy that?\n",
      "Target Sentence: Njöt du av det där?\n",
      "Predicted Sentence: tom är inte tom ? ? <eos> . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: He is kind.\n",
      "Target Sentence: Han är snäll.\n",
      "Predicted Sentence: jag är inte . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: I know this is hard.\n",
      "Target Sentence: Jag vet att det är svårt.\n",
      "Predicted Sentence: jag har att jag är . . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: We can deal with it.\n",
      "Target Sentence: Vi kan ta itu med det.\n",
      "Predicted Sentence: tom har inte det ? ? ? <eos> . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: I must finish my homework before dinner.\n",
      "Target Sentence: Jag måste göra klart läxan innan middagen.\n",
      "Predicted Sentence: jag har inte att ? . . . <eos> . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions by the training model (Fed correct decoder input at each step)\")\n",
    "for i, pred in enumerate(pred_train[:5]):\n",
    "    print(\"Input Sentence: \" + input_sentences_train[i])\n",
    "    print(\"Target Sentence: \" + target_sentences_train[i])\n",
    "    print(\"Predicted Sentence: \" + decode_output_seq(pred))\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so 30 minutes of training and it cannot handle even the training sentences, even when being guided with the correct decoder input at each step. Let's see how it performs on the validation sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions by the training model (Fed correct decoder input at each step)\n",
      "\n",
      "Input Sentence: I don't think this armchair is comfortable.\n",
      "Target Sentence: Jag tycker inte att den här fåtöljen är bekväm.\n",
      "Predicted Sentence: jag är att att jag . . . . . <eos> . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: I'm cold. May I close the window?\n",
      "Target Sentence: Jag fryser. Kan jag stänga fönstret?\n",
      "Predicted Sentence: jag är inte <eos> att jag . . <eos> . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: A child is missing.\n",
      "Target Sentence: Ett barn är försvunnet.\n",
      "Predicted Sentence: jag är . . . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: How was the reunion?\n",
      "Target Sentence: Hur var återträffen?\n",
      "Predicted Sentence: tom du du ? <eos> ? . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n",
      "Input Sentence: Greek is not an easy language.\n",
      "Target Sentence: Grekiska är inget lätt språk.\n",
      "Predicted Sentence: tom är inte . . . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions by the training model (Fed correct decoder input at each step)\\n\")\n",
    "for i, pred in enumerate(pred_val[:5]):\n",
    "    print(\"Input Sentence: \" + input_sentences_val[i])\n",
    "    print(\"Target Sentence: \" + target_sentences_val[i])\n",
    "    print(\"Predicted Sentence: \" + decode_output_seq(pred))\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions seem to follow to patterns, they either start with `tom` or `jag`. The `jag` predictions are actually accurate for the first word! \n",
    "Anyway, the performance is very poor.\n",
    "\n",
    "It also predicts words after the `<eos>` token, this will not happen during inference though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_outputs, state_h = decoder_gru(\n",
    "    decoder_embeddings, initial_state=decoder_state_input_h)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_state_input_h],\n",
    "    [decoder_outputs] + [state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is straight up copy pasted from https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "# With only small modifications to fit my GRU model and my global variable names\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate target sequence with just the <bos> token.\n",
    "    target_seq = np.array(target_tokenizer.word_index['<bos>']).reshape(1,1)\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + [states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_word_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index[sampled_word_index] if sampled_word_index > 0 else \"\"\n",
    "        decoded_sentence.append(sampled_word)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<eos>' or\n",
    "           len(decoded_sentence) > max_target_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.array(target_tokenizer.word_index[sampled_word] if sampled_word else 0).reshape(1,1)\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "\n",
    "    return \" \".join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check what the inference model predicts for some of the sentences in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Did you enjoy that?\n",
      "Decoded sentence: tom är inte att jag har inte . <eos>\n",
      "-\n",
      "Input sentence: He is kind.\n",
      "Decoded sentence: jag har inte att jag har inte . <eos>\n",
      "-\n",
      "Input sentence: I know this is hard.\n",
      "Decoded sentence: jag har inte en dag . <eos>\n",
      "-\n",
      "Input sentence: We can deal with it.\n",
      "Decoded sentence: tom är inte att jag har inte . <eos>\n",
      "-\n",
      "Input sentence: I must finish my homework before dinner.\n",
      "Decoded sentence: jag har inte att jag har inte . <eos>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_sequences_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_train[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My character level model performed much better after two epochs, but let's give this model the chance to train a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "8000/8000 [==============================] - 934s 117ms/step - loss: 4.0301 - val_loss: 9.9404\n",
      "Epoch 2/4\n",
      "8000/8000 [==============================] - 1070s 134ms/step - loss: 3.7998 - val_loss: 12.5358\n",
      "Epoch 3/4\n",
      "8000/8000 [==============================] - 1026s 128ms/step - loss: 3.6160 - val_loss: 11.9686\n",
      "Epoch 4/4\n",
      "8000/8000 [==============================] - 1010s 126ms/step - loss: 3.4727 - val_loss: 11.0466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d702874a90>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_sequences_train, target_sequences_train], decoder_target_sequences_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=([input_sequences_val, target_sequences_val], decoder_target_sequences_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Did you enjoy that?\n",
      "Decoded sentence: kan du få det här ? <eos>\n",
      "-\n",
      "Input sentence: He is kind.\n",
      "Decoded sentence: han är en bra . <eos>\n",
      "-\n",
      "Input sentence: I know this is hard.\n",
      "Decoded sentence: jag är en vän . <eos>\n",
      "-\n",
      "Input sentence: We can deal with it.\n",
      "Decoded sentence: vi har en vän . <eos>\n",
      "-\n",
      "Input sentence: I must finish my homework before dinner.\n",
      "Decoded sentence: jag har inte varit en dag . <eos>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_sequences_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_train[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language has improved! I'll give the model another couple of epochs to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer gru_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'gru_9/while/Exit_2:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('keras_models/s2s_word_6epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/24\n",
      "8000/8000 [==============================] - 814s 102ms/step - loss: 3.3530 - val_loss: 10.2241\n",
      "Epoch 2/24\n",
      "8000/8000 [==============================] - 954s 119ms/step - loss: 3.2423 - val_loss: 10.8753\n",
      "Epoch 3/24\n",
      "8000/8000 [==============================] - 951s 119ms/step - loss: 3.1385 - val_loss: 12.1172\n",
      "Epoch 4/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 3.0457 - val_loss: 12.1912\n",
      "Epoch 5/24\n",
      "8000/8000 [==============================] - 953s 119ms/step - loss: 2.9580 - val_loss: 12.4362\n",
      "Epoch 6/24\n",
      "8000/8000 [==============================] - 954s 119ms/step - loss: 2.8708 - val_loss: 12.4146\n",
      "Epoch 7/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 2.7901 - val_loss: 12.6742\n",
      "Epoch 8/24\n",
      "8000/8000 [==============================] - 955s 119ms/step - loss: 2.7109 - val_loss: 12.4474\n",
      "Epoch 9/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 2.6374 - val_loss: 12.6369\n",
      "Epoch 10/24\n",
      "8000/8000 [==============================] - 953s 119ms/step - loss: 2.5639 - val_loss: 12.5921\n",
      "Epoch 11/24\n",
      "8000/8000 [==============================] - 954s 119ms/step - loss: 2.4927 - val_loss: 12.8667\n",
      "Epoch 12/24\n",
      "8000/8000 [==============================] - 951s 119ms/step - loss: 2.4247 - val_loss: 12.9642\n",
      "Epoch 13/24\n",
      "8000/8000 [==============================] - 958s 120ms/step - loss: 2.3589 - val_loss: 13.0691\n",
      "Epoch 14/24\n",
      "8000/8000 [==============================] - 956s 120ms/step - loss: 2.2931 - val_loss: 12.9422\n",
      "Epoch 15/24\n",
      "8000/8000 [==============================] - 955s 119ms/step - loss: 2.2332 - val_loss: 13.1200\n",
      "Epoch 16/24\n",
      "8000/8000 [==============================] - 954s 119ms/step - loss: 2.1718 - val_loss: 13.1013\n",
      "Epoch 17/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 2.1129 - val_loss: 13.1301\n",
      "Epoch 18/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 2.0581 - val_loss: 13.1500\n",
      "Epoch 19/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 2.0029 - val_loss: 13.1346\n",
      "Epoch 20/24\n",
      "8000/8000 [==============================] - 954s 119ms/step - loss: 1.9452 - val_loss: 13.1566\n",
      "Epoch 21/24\n",
      "8000/8000 [==============================] - 953s 119ms/step - loss: 1.8949 - val_loss: 13.1482\n",
      "Epoch 22/24\n",
      "8000/8000 [==============================] - 952s 119ms/step - loss: 1.8428 - val_loss: 13.1836\n",
      "Epoch 23/24\n",
      "8000/8000 [==============================] - 951s 119ms/step - loss: 1.7972 - val_loss: 13.1755\n",
      "Epoch 24/24\n",
      "8000/8000 [==============================] - 987s 123ms/step - loss: 1.7460 - val_loss: 13.2114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d702874908>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_sequences_train, target_sequences_train], decoder_target_sequences_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=24,\n",
    "          sample_weight=sample_weights_train,\n",
    "          validation_data=([input_sequences_val, target_sequences_val], decoder_target_sequences_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 8 hours of training in total!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer gru_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'gru_9/while/Exit_2:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('keras_models/s2s_word_30epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously I should have used sample weights for the validation data as well, let's see what the actual validation loss was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_val = (decoder_target_sequences_val != 0).reshape(decoder_target_sequences_val.shape[0], decoder_target_sequences_val.shape[1])\n",
    "\n",
    "sample_weights_val = sample_weights_val.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 70s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.283867162704468"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([input_sequences_val, target_sequences_val], decoder_target_sequences_val, sample_weight=sample_weights_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So loss is 3, not 13!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Did you enjoy that?\n",
      "Decoded sentence: tycker du om det här ? <eos>\n",
      "-\n",
      "Input sentence: He is kind.\n",
      "Decoded sentence: han är din vän . <eos>\n",
      "-\n",
      "Input sentence: I know this is hard.\n",
      "Decoded sentence: jag vet att det är svårt . <eos>\n",
      "-\n",
      "Input sentence: We can deal with it.\n",
      "Decoded sentence: vi kan inte göra det . <eos>\n",
      "-\n",
      "Input sentence: I must finish my homework before dinner.\n",
      "Decoded sentence: jag måste göra mig nästa gång i morgon . <eos>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_sequences_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_train[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, almost correct translations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I don't think this armchair is comfortable.\n",
      "Decoded sentence: jag vet inte vad som är för så att jag älskar dig . <eos>\n",
      "-\n",
      "Input sentence: I'm cold. May I close the window?\n",
      "Decoded sentence: jag är på det som jag är borta på att dansa . <eos>\n",
      "-\n",
      "Input sentence: A child is missing.\n",
      "Decoded sentence: de kommer att ha reda . <eos>\n",
      "-\n",
      "Input sentence: How was the reunion?\n",
      "Decoded sentence: hur ska vi hjälpa ? <eos>\n",
      "-\n",
      "Input sentence: Greek is not an easy language.\n",
      "Decoded sentence: det är mycket att vi ska göra . <eos>\n",
      "-\n",
      "Input sentence: Be friendly.\n",
      "Decoded sentence: var inte . <eos>\n",
      "-\n",
      "Input sentence: Tom is lying.\n",
      "Decoded sentence: tom har rätt . <eos>\n",
      "-\n",
      "Input sentence: I must be there.\n",
      "Decoded sentence: jag måste göra det . <eos>\n",
      "-\n",
      "Input sentence: I feel so pretty.\n",
      "Decoded sentence: jag har en barn . <eos>\n",
      "-\n",
      "Input sentence: I attempted to swim across the river.\n",
      "Decoded sentence: jag gick i en vid vid mina saker . <eos>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = input_sequences_val[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_sentences_val[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the validation data is much worse... \n",
    "The start words are typically correct, like `I` -> `Jag`, `Be` -> `Var`, but I think the language is worse than what the character level model produced. \n",
    "The sentences have less flow to them, and sound less like real Swedish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the word embeddings\n",
    "I trained my model to translate English sentences to Swedish sentences.\n",
    "In the process it has produced two word embedding layers. \n",
    "\n",
    "Let's explore the properties of these layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embedding_input = Input(shape=(None,))\n",
    "target_embeddings_model = Model(decoder_inputs, decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word(word_index):\n",
    "    return target_embeddings_model.predict(np.array(word_index).reshape(1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for word, i in target_tokenizer.word_index.items():\n",
    "    embeddings[word] = encode_word(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7112"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes word emmbedings place similar words close to eachother in the vector space. If that is the case similar words can be found by checking the words euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_distance(w1, w2):\n",
    "    embedding1 = embeddings[w1]\n",
    "    embedding2 = embeddings[w2]\n",
    "    \n",
    "    return np.linalg.norm(embedding1- embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, k=5, embedded=False):\n",
    "    if embedded:\n",
    "        embedding=word\n",
    "    else:\n",
    "        embedding = embeddings[word]\n",
    "    # Calculate the distance to all other words\n",
    "    distances = np.array(list(map(lambda x: np.linalg.norm(embedding- x), embeddings.values())))\n",
    "    \n",
    "    # Find the k shortest distances, might not be sorted\n",
    "    idx = np.argpartition(distances, k)[:k]\n",
    "    \n",
    "    # Sort the k shortest distances\n",
    "    idx = idx[np.argsort(distances[idx])]\n",
    "    \n",
    "    # Return the words with the k shortest distances, together with the distance\n",
    "    closest_embeddings = np.array(list(embeddings.keys()))[idx]\n",
    "    return list(zip(closest_embeddings, distances[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.570869"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_distance('tom', 'jag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jag', 0.0),\n",
       " ('hon', 1.1784053),\n",
       " ('du', 1.2056358),\n",
       " ('man', 1.2721634),\n",
       " ('ni', 1.2950184)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('jag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, all the closests words are actually pronouns!\n",
    "Pronouns are very common in the training data, most sentences actually start with some kind of pronoun, so I would think this word group might be the easiest to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('köpa', 0.0),\n",
       " ('svalde', 0.88832903),\n",
       " ('släpp', 0.89586461),\n",
       " ('tände', 0.90464681),\n",
       " ('bevisade', 0.91694701),\n",
       " ('hålla', 0.93043923),\n",
       " ('skadade', 0.93186134),\n",
       " ('blonda', 0.93238038),\n",
       " ('for', 0.94291615),\n",
       " ('kallade', 0.94447839)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('köpa', k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so all these wors are adjectives, though not in the same tense as `köpa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('katt', 0.0),\n",
       " ('lov', 0.6620847),\n",
       " ('öron', 0.69108564),\n",
       " ('lek', 0.69138545),\n",
       " ('order', 0.69297498)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('katt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All are nouns, but I wouldn't say they carry very similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0792097"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_distance('katt', 'hund')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my embeddings think `Katt`, which means `Cat`, is much closer to the word `Öron` which means `Ears` than to `Hund` which is `Dog`.\n",
    "\n",
    "I think my training set is way to small to learn the similarities of these nouns, they simply don't appear in similar contexts that often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can do operations on the word embeddings. Unfortunately I dont have a very big vocabulary, so I cant try classics like `King` - `Man` + `Woman`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flygresa', 0.27931079),\n",
       " ('diskar', 0.48389259),\n",
       " ('jordnötter', 0.49537137),\n",
       " ('räkningen', 0.49831137),\n",
       " ('teckenspråk', 0.4996382),\n",
       " ('frivilligt', 0.5030669),\n",
       " ('cookie', 0.50501686),\n",
       " ('vilar', 0.50531614),\n",
       " ('glänser', 0.50846142),\n",
       " ('hushållssysslorna', 0.51011735)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(embeddings['flygresa'] - embeddings['flyg'], k=10, embedded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did't find any good operations. Maybe there are some, but as with the other words I think my training set is way to small to build meaningfull embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I trained a RNN encoder decoder model to translate Englis to Swedish based on word tokens. \n",
    "In the process I trained two word embedding layers!\n",
    "\n",
    "I did not achieve great results with this model, as I still think the translations are very poor. I think this largely depends on the size of my training data, but unfortunately training time is so long that I cannot really scale up train data size or the model complexity.\n",
    "\n",
    "I experimented with the embeddings created for the Swedish words and found that the closest words to a pronoun were pronouns, same story for adjectives and nouns. However, I did not think the closest words to any of the ones I tried were very similar other than having the same part of speech tag. Anyway, I think this really shows the potential of word embeddings in part of speech tagging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
