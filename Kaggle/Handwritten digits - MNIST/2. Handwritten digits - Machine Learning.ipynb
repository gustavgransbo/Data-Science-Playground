{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I aim to test out some different machinelearning techinques on the MNIST data set. I will not spend much time on feature engineering, but rather on exploring scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting point\n",
    "I have already tried out some basic feature engineering in another notebook. I noticed binarizing the ranges of all features from [0, 255] to [0, 1] had a positive effect when classifying with a Random Forest. I have also read about some tricks that are usefull when working on the MNIST data set that I will try to make use of:\n",
    "* The data set can be extended by shifting digits in different directions while preserving their shape\n",
    "* Nearest Neighbours is supposed to be more effective than Random Forest. As my knowledge is very limited on Random FOrest I cannot tell why, but I know Nearest Neighbours can handle non-linear systems, which I strongly suspect is necessary when classifying the MNIST set. I would like to try out Nearest Neighbours and finetune hyper parameters using scikit-learns Grid Search.\n",
    "* Similar to the first point, I am considering extending the data by rotating digits by small degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "digits = train_df.iloc[:, 1:].values\n",
    "labels = train_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN on Binarized features\n",
    "Let's see what results we get with the `KNeighborsClassifier` straight out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DigitBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cutoff = 1):\n",
    "        self.cutoff = cutoff\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y= None):\n",
    "        return np.apply_along_axis(self.binarize_digit_inner, 1, X)\n",
    "    def binarize_digit_inner(self, digit):\n",
    "        black = np.full(28*28, 1)\n",
    "        white = np.full(28*28, 0)\n",
    "        mask = digit >= self.cutoff\n",
    "        return np.where(mask, black, white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = DigitBinarizer().transform(digits)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My hopes of using GridSearch to find the optimal number of neighbours were stumped by the impossibly slow performance of my laptop on the large data set. The complexity of performing KNN classification using brute force distance meassurments is `O(N*D*Q)` where `N` (40 000) is the amount of samples in the training data and `D` the amount of features (784), and `Q` the number of queries (28 000). Which by the competitive programming rule of thumb stating we get about 10^8 operations/second tells me the worst case would run in about 2 hours and 30 minutes... I'm glad I seem to land far from the worst case, but classification is just way to slow for me to want to tinker with hyperparameters, even with the help of GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm=\"brute\", n_neighbors=3)\n",
    "knn.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv').values\n",
    "test = DigitBinarizer().transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing some experimenting I noticed my laptop grinds to a halt when making predictions on the whole test set, so I do it in batches of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = [knn.predict(test[i*1000:(i+1)*1000]) for i in range(len(test) // 1000)]\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = np.array(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(list(zip(np.arange(1, 28001), predictions_flat)), columns = ['ImageID', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df.set_index('ImageID').to_csv('submission2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission of Kaggle got an accuracy of 96.3 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended training set\n",
    "Let's see what accuracy we get if we extend the training set with images shifted 1 px in all directions. This makes the set 5x larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "class DigitShifter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]):\n",
    "        self.directions = directions\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y= None):\n",
    "        X_shifts = [np.apply_along_axis(lambda x: self.shift_digit(x, direction[0], direction[1]), 1, X) for direction in self.directions]\n",
    "        return np.concatenate([X] + X_shifts)\n",
    "    def shift_digit(self, digit, delta_x=0, delta_y=0):\n",
    "        digit = digit.reshape(28, 28)\n",
    "        return shift(digit, (delta_x, delta_y)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('binarize' , DigitBinarizer()),\n",
    "    ('shifter' , DigitShifter())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pipeline.transform(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.concatenate([labels for i in range(len(X)//len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = knn.predict(test[:1000])\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n",
      "Wall time: 7min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = [knn.predict(test[i*1000:(i+1)*1000]) for i in range(len(test) // 1000)]\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_flat = np.array(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(list(zip(np.arange(1, 28001), predictions_flat)), columns = ['ImageID', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df.set_index('ImageID').to_csv('submission3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission got an accuracy of 97.0%. Improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using KNN on the extended training set I improved my prediction accuracy to 97%, compared to my previous RandomForest baseline of 94.5%.\n",
    "\n",
    "I had high hopes for KNN, and wanted to try experimenting with scikit-learn to tune hyperparameters and try out some more extensions of the training set. However, the long prediction times make this very time consuming. I would rather focus on feature extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
