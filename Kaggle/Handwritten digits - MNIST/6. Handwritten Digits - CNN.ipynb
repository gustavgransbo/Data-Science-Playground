{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting point\n",
    "In my previous notebooks I have experimented with extending the MNIST data set by shifting the digits, dimensionality reduction using PCA and classification using a simple KNN, some ensemble methods and a dense deep neural network.\n",
    "\n",
    "My best result on the Kaggle test set has so far been 98.48% accuracy using a dense deep neural network.\n",
    "\n",
    "In this notebook I will tackle MNIST using a Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "All pixels take values in the range [0, 255], I will normalise them to [0, 1].\n",
    "I might opt to extend the data set by shifting the digits like I did in previous notebooks, but first I want to get a feeling for what the training time is like on the standard training set.\n",
    "\n",
    "Also, like in `4. Handwritten Digits - Ensemble Learning` I will opt to set aside a validation set of 8 000 images.\n",
    "\n",
    "I will not be doing any PCA. The reason is that a Convolutional layer actually cares about the feature order. The first layer will be  focusing on small square areas of the image. If I displace all feautes with PCA local paterns in the image will disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('train.csv')\n",
    "digits = data_df.iloc[:, 1:].values\n",
    "labels = data_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "digits_test = test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_one_hot = OneHotEncoder().fit_transform(labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dimensions = (28, 28, 1)\n",
    "\n",
    "digits = digits.reshape(-1, *img_dimensions)\n",
    "digits_test = digits_test.reshape(-1, *img_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits_scaled = digits / 255\n",
    "digits_test_scaled = digits_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_val, y, y_val = train_test_split(digits_scaled, labels_one_hot, test_size = 8000, stratify = labels_one_hot.toarray(), random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up my first model\n",
    "My first model will be heavily inspired by the Keras teams example CNN for MNIST. [Github code](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, BatchNormalization, Conv2D, MaxPooling2D, Dense, Flatten, Input\n",
    "from keras.models import Model\n",
    "\n",
    "model_input = Input(shape=img_dimensions)\n",
    "x = Conv2D(32, kernel_size=(4,4), strides = 2, activation='relu')(model_input)\n",
    "x = Conv2D(64, kernel_size=(2,2), strides = 1, activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(rate=.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(rate=.5)(x)\n",
    "model_output = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model, use cross entropy as loss function and stochastic gradient descent for backpropagation. Accuracy should be used to measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just do a super small training run to start of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 8000 samples\n",
      "Epoch 1/3\n",
      "34000/34000 [==============================] - 38s 1ms/step - loss: 0.4695 - acc: 0.8511 - val_loss: 0.1061 - val_acc: 0.9679\n",
      "Epoch 2/3\n",
      "34000/34000 [==============================] - 43s 1ms/step - loss: 0.1374 - acc: 0.9575 - val_loss: 0.0646 - val_acc: 0.9801\n",
      "Epoch 3/3\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.1002 - acc: 0.9692 - val_loss: 0.0553 - val_acc: 0.9835\n",
      "Wall time: 2min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x149a2efe320>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(X, y, validation_data=[X_val, y_val], epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000/34000 [==============================] - 13s 389us/step\n",
      "8000/8000 [==============================] - 4s 549us/step\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics_train = model.evaluate(X, y, batch_size=64)\n",
    "loss_and_metrics_val = model.evaluate(X_val, y_val, batch_size=64)\n",
    "\n",
    "print(\"{} accuracy on training set\".format(loss_and_metrics_train[1]))\n",
    "print(\"{} accuracy on validation set\".format(loss_and_metrics_val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 8000 samples\n",
      "Epoch 1/3\n",
      "34000/34000 [==============================] - 44s 1ms/step - loss: 0.0832 - acc: 0.9742 - val_loss: 0.0447 - val_acc: 0.9865\n",
      "Epoch 2/3\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.0700 - acc: 0.9785 - val_loss: 0.0435 - val_acc: 0.9858\n",
      "Epoch 3/3\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.0602 - acc: 0.9813 - val_loss: 0.0406 - val_acc: 0.9876\n",
      "Wall time: 2min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1499673eeb8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(X, y, validation_data=[X_val, y_val], epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000/34000 [==============================] - 12s 344us/step\n",
      "8000/8000 [==============================] - 4s 494us/step\n",
      "0.9931764705882353 accuracy on training set\n",
      "0.987625 accuracy on validation set\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics_train = model.evaluate(X, y, batch_size=64)\n",
    "loss_and_metrics_val = model.evaluate(X_val, y_val, batch_size=64)\n",
    "\n",
    "print(\"{} accuracy on training set\".format(loss_and_metrics_train[1]))\n",
    "print(\"{} accuracy on validation set\".format(loss_and_metrics_val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 8000 samples\n",
      "Epoch 1/6\n",
      "34000/34000 [==============================] - 47s 1ms/step - loss: 0.0557 - acc: 0.9824 - val_loss: 0.0412 - val_acc: 0.9870\n",
      "Epoch 2/6\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.0510 - acc: 0.9841 - val_loss: 0.0320 - val_acc: 0.9900\n",
      "Epoch 3/6\n",
      "34000/34000 [==============================] - 45s 1ms/step - loss: 0.0494 - acc: 0.9847 - val_loss: 0.0364 - val_acc: 0.9887\n",
      "Epoch 4/6\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.0425 - acc: 0.9867 - val_loss: 0.0376 - val_acc: 0.9877\n",
      "Epoch 5/6\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.0417 - acc: 0.9873 - val_loss: 0.0376 - val_acc: 0.9886\n",
      "Epoch 6/6\n",
      "34000/34000 [==============================] - 46s 1ms/step - loss: 0.0376 - acc: 0.9878 - val_loss: 0.0340 - val_acc: 0.9900\n",
      "Wall time: 4min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x149940a89e8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(X, y, validation_data=[X_val, y_val], epochs=6, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000/34000 [==============================] - 11s 311us/step\n",
      "8000/8000 [==============================] - 2s 257us/step\n",
      "0.9967941176470588 accuracy on training set\n",
      "0.99 accuracy on validation set\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics_train = model.evaluate(X, y, batch_size=64)\n",
    "loss_and_metrics_val = model.evaluate(X_val, y_val, batch_size=64)\n",
    "\n",
    "print(\"{} accuracy on training set\".format(loss_and_metrics_train[1]))\n",
    "print(\"{} accuracy on validation set\".format(loss_and_metrics_val[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 12 training epochs the model sits at 99.7% training set accuracy and 99.0% dev set accuracy. This beats my previous models by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(digits_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(list(zip(np.arange(1, 28001), predictions)), columns = ['ImageID', 'Label'])\n",
    "submission_df.set_index('ImageID').to_csv('Submissions/submission11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.7% submission accuracy. Cool!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
