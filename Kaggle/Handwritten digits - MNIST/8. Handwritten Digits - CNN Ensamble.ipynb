{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I recently read an interesting [article](https://mlwave.com/kaggle-ensembling-guide/) on different ensamble methods commonly used in Kaggle competitions, and thought it would be interesting to apply to a dataset that I have previosuly acieved a high accuracy on.\n",
    "\n",
    "Of course, the best way to achieve high accuracy on MNIST is probably to use a very deep CNN, but here I will try to create an ensamble of different neural networks, and train a stacked generalizer on them using blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('train.csv')\n",
    "X = data_df.iloc[:, 1:].values\n",
    "y = data_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "X_test = test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dimensions = (1, 28, 28)\n",
    "\n",
    "X = X.reshape(-1, *img_dimensions)\n",
    "X_test = X_test.reshape(-1, *img_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(y.size)\n",
    "X = X[idx]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "stratified_kfold = StratifiedKFold(n_folds, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(stratified_kfold.split(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).long()\n",
    "\n",
    "# Test\n",
    "X_test = torch.from_numpy(X_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(p=.25)\n",
    "        self.dropout2 = nn.Dropout(p=.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.avg_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function inspired by https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "def train_model(model, train_loader, optimizer, criterion, validation_loader = None, epochs = 2):\n",
    "    \n",
    "    # Only enter the validation state if there is a validation_loader\n",
    "    phases = ['train']\n",
    "    data_set_loaders = {'train' : train_loader, 'val' : validation_loader} \n",
    "    if validation_loader:\n",
    "        phases.append('val')\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in phases:\n",
    "            \n",
    "            data_set_loader = data_set_loaders[phase]\n",
    "            \n",
    "            # Only update model weights based on the training data\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(data_set_loader):\n",
    "                #inputs, labels = batch\n",
    "                \n",
    "                #labels = torch.autograd.Variable(labels).type(torch.LongTensor)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Only track history during training\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    predictions = torch.argmax(outputs, dim=1)\n",
    "                    \n",
    "                    # Only perform backpropagation during training\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # Save statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(data_set_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_set_loader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_proba(model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        probas = []\n",
    "        for batch, *_ in test_loader:\n",
    "            probas.append(model(batch))\n",
    "\n",
    "        #return probas\n",
    "        return torch.cat(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MLP(100), CNN1()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_blend_train = np.zeros((X.shape[0], len(models), 10))\n",
    "dataset_blend_test = np.zeros((X_test.shape[0], len(models), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "test_dataset_mlp = torch.utils.data.TensorDataset(X_test.reshape(-1, 28*28))\n",
    "test_dataset_cnn = torch.utils.data.TensorDataset(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Blending approach inspired by https://github.com/emanuele/kaggle_pbr/blob/master/blend.py\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    dataset_blend_test_j = np.zeros((X_test.shape[0], len(folds), 10))\n",
    "    for i, (train, val) in enumerate(folds):\n",
    "        model.apply(weight_reset)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        print(\"Fold\", i)\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_val = X[val]\n",
    "        y_val = y[val]\n",
    "        \n",
    "        if j > 0:\n",
    "            # CNN\n",
    "            train_dataset = torch.utils.data.TensorDataset(X[train], y[train])\n",
    "            val_dataset = torch.utils.data.TensorDataset(X[val], y[val])\n",
    "            test_dataset = test_dataset_cnn\n",
    "        else:\n",
    "            # MLP\n",
    "            train_dataset = torch.utils.data.TensorDataset(X[train].reshape(-1, 28*28), y[train])\n",
    "            val_dataset = torch.utils.data.TensorDataset(X[val].reshape(-1, 28*28), y[val])\n",
    "            test_dataset = test_dataset_mlp\n",
    "            \n",
    "            \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\n",
    "        \n",
    "        train_model(model, train_loader, optimizer, criterion, epochs = 4)\n",
    "        y_val = predict_proba(model, val_loader)\n",
    "        dataset_blend_train[val, j] = y_val\n",
    "        dataset_blend_test_j[:, i] = predict_proba(model, test_loader)\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Gustav\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "generalizer = LogisticRegression()\n",
    "generalizer.fit(dataset_blend_train.reshape(-1, 20), y)\n",
    "y_train = generalizer.predict(dataset_blend_train.reshape(-1, 20))\n",
    "y_test = generalizer.predict(dataset_blend_test.reshape(-1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833571428571428"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 2, 10)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_blend_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_preds = dataset_blend_train[:,0,:].argmax(1)\n",
    "cnn_preds = dataset_blend_train[:,1,:].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9572857142857143, 0.9843571428571428)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(mlp_preds, y), accuracy_score(cnn_preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_preds = dataset_blend_test[:,0,:].argmax(1)\n",
    "cnn_preds = dataset_blend_test[:,1,:].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(list(zip(np.arange(1, 28001), cnn_preds)), columns = ['ImageID', 'Label'])\n",
    "submission_df.set_index('ImageID').to_csv('Submissions/submission_nb8_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(list(zip(np.arange(1, 28001), y_test)), columns = ['ImageID', 'Label'])\n",
    "submission_df.set_index('ImageID').to_csv('Submissions/submission_nb8_generalizer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set accuracy is 0.98528 for the CNN, and 0.98342 for the generalizer. It's likely that the generalizer does not benefit from the predictions of the MLP, which only scored 95.7% accuracy on the training set. \n",
    "\n",
    "I should let both models train for more epochs, 4 are pretty few, and probably should also add more models. An ensamble of just two models seems a little bit small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I implemented a stacked ensamble with blending of two models, a CNN and an MLP. I did not allow them to train for very long, which resulted in the MLP performing pretty poorly. As a result, the generalizer did not perform better than the CNN did on its own. I should re-run the experiment with more epochs, and should probably also include more models. However, if I am interested in experimenting with ensamble learning, it would probably be better to tackle a problem which is solved well by simpler models, so that I can avoid the long training times of the neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
